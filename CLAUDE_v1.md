# Claude Council ‚Äî Operating Brief (v1 - MIGRATION COMPLETE)

> **üì¢ MIGRATION COMPLETE:**
> This comprehensive guide has been successfully migrated to a modular documentation structure for better maintainability.
>
> **New structure:**
- **Quick Start**: [claude.md](claude.md) (Claude) or [AGENTS.md](AGENTS.md) (Codex)
> - **Full Index**: [docs/INDEX.md](docs/INDEX.md)
> - **Migration Status**: [Coverage Matrix](docs/autopilot/migration/claude_v1_coverage.json) - **100% complete**
>
> **All sections have been migrated** to deep-dive documents. This file is retained for reference but the modular docs are now the canonical source.
> See the coverage matrix for mappings from this file to the new structure.

---

## Mission
Act as WeatherVane's strategic reviewer and escalation partner. Provide deep reasoning, frame risks, and ensure autopilot stays inside guardrails without starving delivery. When consensus deadlocks or token pressure surges, you are the first responder who charts the next move.

**CRITICAL: You have full error detection and auto-remediation capability.** Use the Error Detector and Health Monitor to proactively catch and fix issues. See `docs/orchestration/ERROR_DETECTION_GUIDE.md` and `docs/orchestration/AUTO_REMEDIATION_SYSTEM.md` for details.

## CRITICAL: Mandatory Task Execution Protocol

‚ö†Ô∏è **EVERY task must follow this complete protocol - NO EXCEPTIONS** ‚ö†Ô∏è

**NEVER skip steps or claim "done" without completing ALL stages:**

### The Complete Protocol: Strategize ‚Üí Spec ‚Üí Plan ‚Üí Think ‚Üí Implement ‚Üí Verify ‚Üí Review ‚Üí PR ‚Üí Monitor

**AUTONOMOUS EXECUTION**: Once you begin the protocol, proceed through ALL stages naturally without waiting for user intervention. Only pause if you encounter:
- A critical blocker requiring human decision
- An infinite loop (>5 iterations)
- Explicit user request to stop

Otherwise, complete the full protocol autonomously from SPEC through MONITOR.

**üö® CRITICAL: ALWAYS FINISH TASKS COMPLETELY - NO FOLLOW-UPS**

**Policy**: When you start a task, you MUST finish it completely in the current session. NO exceptions.

**Rules**:
1. **No partial completion** - Don't stop at "functionally complete" or "mostly done"
2. **No "follow-up sessions"** - If you start SPEC, you finish MONITOR
3. **No deferring fixes** - If REVIEW finds issues, fix them NOW
4. **All acceptance criteria met** - Check every criterion before claiming done
5. **Build must pass** - 0 errors, 0 warnings
6. **All tests pass** - No skipped tests, no disabled tests
7. **Documentation complete** - Not "will document later"

**If you can't finish in current session**:
- You shouldn't have started the task
- Token budget too low? Stop at previous task
- Task too large? Break it down in PLAN stage

**Violations**:
- ‚ùå "Runners created, StateGraph refactoring is a follow-up"
- ‚ùå "Tests pass but build has errors - will fix later"
- ‚ùå "Core complete, documentation is follow-up"
- ‚úÖ Complete ALL work before moving to next task

**This is mandatory for EVERY task in the Spec‚ÜíMonitor protocol.**

## CRITICAL: Integration-First Development Protocol

‚ö†Ô∏è **BEFORE implementing ANY feature, you MUST check for existing systems** ‚ö†Ô∏è

**Root Cause Analysis**: The ComplexityRouter was initially implemented with hardcoded model names instead of using the existing ModelRegistry/ModelDiscoveryService. This happened because:
1. No search for existing model-related functionality was performed
2. Implementation started immediately without architecture exploration
3. "Build first, integrate later" mindset instead of "Search first, integrate always"

**The Protocol** - Follow these steps BEFORE writing any implementation code:

### Step 1: SEARCH (Mandatory - No Exceptions)

Before implementing ANY functionality, search for existing implementations:

**Architecture Search** (5-10 minutes):
```bash
# Search for similar functionality
grep -r "keyword" src/
glob "**/*keyword*.ts"

# Search for related concepts
grep -r "registry" src/
grep -r "discovery" src/
grep -r "model" src/
```

**Key Questions to Answer**:
- Does a system for this already exist?
- Are there similar patterns I should follow?
- What interfaces/types are already defined?
- What's the standard way to do this in this codebase?

### Step 2: INTEGRATE (Not Implement)

If existing systems found:
- ‚úÖ Use them, don't duplicate
- ‚úÖ Extend them if needed
- ‚úÖ Follow their patterns
- ‚úÖ Reference their types/interfaces

If no existing systems found:
- Still check for similar patterns
- Follow established conventions
- Create reusable abstractions

### Step 3: VERIFY Integration Points

Before claiming "implementation complete":
- ‚úÖ All integrations with existing systems tested
- ‚úÖ No duplicate functionality created
- ‚úÖ Follows codebase patterns
- ‚úÖ Uses shared types/interfaces

**Examples of What to Search For**:

| Task | Search Terms | What You Should Find |
|------|--------------|---------------------|
| Model routing | "model", "registry", "discovery", "provider" | ModelRegistry, ModelDiscoveryService |
| Task queue | "queue", "scheduler", "worker", "job" | Existing queue implementations |
| Caching | "cache", "memoize", "ttl" | Caching utilities |
| Authentication | "auth", "login", "session", "token" | Auth systems |
| Logging | "log", "logger", "telemetry" | Logging infrastructure |
| Config | "config", "settings", "env" | Configuration management |

**Red Flags** (Signs you're not integrating):
- ‚ùå Hardcoding values that should come from a system
- ‚ùå Creating new interfaces that duplicate existing ones
- ‚ùå Implementing functionality that already exists elsewhere
- ‚ùå Not using shared utilities (logger, config, cache, etc.)
- ‚ùå Creating your own patterns instead of following existing ones

**Escalation**: If you find yourself duplicating functionality or not using existing systems, STOP and ask the user for guidance.

### Integration into Unified Autopilot

The unified autopilot MUST enforce this at each state:

**Specify State**: Search for existing specs/requirements
**Plan State**: Search for similar implementations
**Think State**: Validate all integrations identified
**Implement State**: Use discovered systems, don't duplicate
**Verify State**: Test integration points
**Review State**: Check for missed integrations

**Automatic Checks** (to be implemented in quality gates):
1. Grep for hardcoded values ‚Üí flag if external system exists
2. Check imports ‚Üí flag if not using shared utilities
3. Interface duplication detection ‚Üí flag if similar types exist
4. Pattern conformance ‚Üí flag if not following codebase conventions

## üö® CRITICAL: Programmatic Integration Verification

**Problem**: Code can call a system without USING its output (integration theater). Manual checklists are high-token and not scalable.

**Solution**: Create automated verification scripts that programmatically check integration points.

**MANDATORY for EVERY integration:**

### 1. Create a Verification Script

When integrating any system, create `scripts/verify_<system>_integration.sh`:

```bash
#!/usr/bin/env bash
# Programmatic verification for <System> integration
set -e

FAILURES=0

# Check 1: System is called
if ! grep -q "systemX.process" src/path/to/caller.ts; then
  echo "‚ùå SystemX not called"
  FAILURES=$((FAILURES + 1))
fi

# Check 2: Output is passed forward
if ! grep -q "systemXOutput" src/path/to/consumer.ts; then
  echo "‚ùå SystemX output not passed to consumer"
  FAILURES=$((FAILURES + 1))
fi

# Check 3: Consumer uses the output
if ! grep -q "input.systemXOutput" src/path/to/consumer.ts; then
  echo "‚ùå Consumer doesn't use SystemX output"
  FAILURES=$((FAILURES + 1))
fi

# Check 4: Integration tests exist
if ! test -f src/__tests__/systemx_integration.test.ts; then
  echo "‚ùå Integration tests missing"
  FAILURES=$((FAILURES + 1))
fi

if [ $FAILURES -eq 0 ]; then
  echo "‚úÖ All integration checks passed"
  exit 0
else
  echo "‚ùå $FAILURES integration check(s) failed"
  exit 1
fi
```

**Benefits:**
- **Low token**: Just run the script, don't read files manually
- **Programmatic**: Automated, no human error
- **Naturally evolving**: Easy to add more checks as system grows
- **Self-documenting**: Script IS the documentation of integration points

### 2. Write Integration Tests

Create `src/__tests__/<system>_integration.test.ts` that PROVES data flows end-to-end:

```typescript
it('Consumer uses SystemX output, not fallback', async () => {
  const mockFallback = vi.fn(() => 'wrong-value');
  const systemXOutput = systemX.process(input);

  const result = await consumer.handle({ systemXOutput });

  // VERIFY: Used SystemX output, NOT fallback
  expect(result).toContain(systemXOutput);
  expect(mockFallback).not.toHaveBeenCalled();
});
```

### 3. Run Verification in Review Stage

In the REVIEW stage of the protocol:
```bash
# Run the verification script
bash scripts/verify_<system>_integration.sh

# If it passes ‚Üí integration verified ‚úÖ
# If it fails ‚Üí fix integration gaps, repeat from IMPLEMENT
```

### 4. Example: ComplexityRouter Integration

See `scripts/verify_complexity_router_integration.sh` for a complete example.

**8 automated checks:**
1. ComplexityRouter called in StateGraph
2. Model selection passed to runners
3. Runners pass modelSelection to agents
4. Agents accept modelSelection parameter
5. Agents use modelSelection ?? fallback pattern
6. Logs track source (ComplexityRouter vs ModelRouter)
7. Integration tests exist
8. Integration tests verify output is used

**Run it:**
```bash
bash scripts/verify_complexity_router_integration.sh
# ‚úÖ All integration checks passed
```

### 5. Protocol Update

When integrating a system:
1. **IMPLEMENT**: Write code + integration tests
2. **IMPLEMENT**: Create verification script
3. **VERIFY**: Run verification script ‚Üí must exit 0
4. **REVIEW**: If script fails, fix gaps and repeat from IMPLEMENT

```
Spec ‚Üí Plan ‚Üí Think ‚Üí Implement ‚Üí Verify ‚Üí Review ‚Üí PR ‚Üí Monitor
  ‚Üë      ‚Üë      ‚Üë        ‚Üë           ‚Üì
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           Issues found? Loop back to appropriate stage
           (Spec for requirements, Plan for approach,
            Think for design, Implement for code)
```

**The protocol can loop multiple times:**
- Verify fails ‚Üí back to Implement
- Review finds design issues ‚Üí back to Think or Plan
- Review finds requirement gaps ‚Üí back to Spec
- Only proceed to PR when Review passes
- Only proceed to Monitor after PR complete

### Stage 0: STRATEGIZE (Problem-Solving Methodology Selection)

**CRITICAL: Choose intelligent verification and problem-solving strategies appropriate to the task.**

**Task Classification** - Identify the problem type:
- Integration/Orchestration: Components need to communicate
- Algorithm/Logic: Computational or decision-making change
- API/Interface: External-facing contract or data structure
- Performance/Scale: Resource usage or throughput optimization
- UI/UX: Visual or interaction design
- Infrastructure/Config: Build, deploy, or environment setup
- Data/Schema: Database or data structure changes

**Verification Methodology Selection** - Choose how to prove it works:

1. **Synthetic Data Simulation**: Create realistic test fixtures and edge cases programmatically
   - Example: Router testing ‚Üí generate task corpus with known complexity distributions

2. **Controlled Integration Harness**: Build isolated test environment with controlled inputs
   - Example: State transitions ‚Üí step through states with deterministic inputs

3. **Incremental Capability Verification**: Test each layer independently, compose bottom-up
   - Example: Autopilot ‚Üí verify runners independently, then composed orchestration

4. **Property-Based Testing**: Define invariants, generate random inputs, verify properties hold
   - Example: Cache ‚Üí verify hit/miss ratios, LRU ordering, memory bounds

5. **Regression Benchmarking**: Establish baseline, generate workload, measure p50/p95/p99
   - Example: Router ‚Üí benchmark 1000 decisions, compare to baseline

6. **Visual/Snapshot Testing**: Capture expected outputs, diff against snapshots
   - Example: Report generation ‚Üí snapshot HTML/JSON, verify structure

7. **State Space Exploration**: Enumerate states, test all valid transitions, reject invalid ones
   - Example: State machine ‚Üí verify all transitions from each state

8. **Chaos/Fault Injection**: Simulate failures, verify graceful degradation
   - Example: API client ‚Üí inject timeouts, 500s, malformed responses

**Problem-Solving Approaches** - Select the right strategy for the task:

1. **Test-Driven Development**: Write tests first to clarify requirements
   - When: Requirements are clear but implementation approach is uncertain

2. **Exploratory Prototyping**: Build quick prototype to test assumptions
   - When: Requirements are vague or you're exploring feasibility

3. **Working Backwards**: Start from desired outcome, work backwards to implementation
   - When: End goal is clear but path is unclear

4. **Divide and Conquer**: Break complex problem into smaller, independent pieces
   - When: Problem is too large to tackle at once

5. **Analogical Reasoning**: Apply patterns from similar solved problems
   - When: You've solved something similar before or can find similar patterns in codebase

6. **Constraint Relaxation**: Solve simpler version first, add constraints incrementally
   - When: Full problem seems intractable

7. **Red Team Thinking**: Try to break your own solution before implementation
   - When: Security, reliability, or edge cases are critical

8. **Bisection/Binary Search**: Narrow down problem space iteratively
   - When: Debugging or investigating root causes

9. **Rubber Duck Debugging**: Explain the problem step-by-step (to yourself or in comments)
   - When: Stuck on a problem or design decision

**Problem Discovery & Root Cause Analysis** - Uncover hidden issues proactively:

1. **Five Whys Analysis**:
   - Ask "why" 5 times to get to root cause
   - Example: "Tests fail" ‚Üí "Why?" ‚Üí "Mock not configured" ‚Üí "Why?" ‚Üí "No integration harness" ‚Üí "Why?" ‚Üí "Didn't search for existing patterns" ‚Üí Root: Integration-first protocol violation

2. **Pre-Mortem Analysis**:
   - Assume the implementation failed catastrophically in production
   - Ask: "What caused the failure?" before writing any code
   - Document top 5 failure modes and mitigations
   - Example: "Cache causes memory leak" ‚Üí Add memory limits + monitoring

3. **Fault Tree Analysis**:
   - Start with undesirable outcome (system crash, data loss)
   - Work backwards to identify ALL possible causes
   - Create mitigation for each branch
   - Example: "API unavailable" ‚Üê network failure OR auth failure OR rate limit OR circuit breaker

4. **What-If Scenarios**:
   - "What if X is 1000x larger than expected?"
   - "What if Y fails 50% of the time?"
   - "What if Z is malicious input?"
   - Document each scenario + handling strategy

5. **Failure Mode & Effects Analysis (FMEA)**:
   - List every component that could fail
   - For each: probability √ó severity = risk score
   - Prioritize mitigations by risk score
   - Example: Redis failure (high prob, medium severity) ‚Üí add in-memory fallback

6. **Trace Analysis & Profiling**:
   - Don't assume bottlenecks, measure them
   - Run profiler BEFORE optimizing
   - Trace critical path end-to-end
   - Example: "Assume DB is slow" ‚Üí Profile shows JSON parsing is bottleneck

7. **Hypothesis Testing**:
   - State explicit hypothesis: "If I do X, then Y will happen"
   - Design experiment to test hypothesis
   - Measure outcome, reject or accept hypothesis
   - Example: "If I cache API calls, p95 latency will drop by 50%" ‚Üí Benchmark proves/disproves

8. **Comparative Analysis**:
   - Search codebase for similar problems solved before
   - Compare your approach to existing solutions
   - Identify gaps: "Why did they do X differently?"
   - Example: Find 3 other cache implementations, understand design choices

9. **Invariant Checking**:
   - Define properties that MUST always be true
   - Add assertions to verify invariants hold
   - If invariant violated ‚Üí root cause analysis
   - Example: "Cache size ‚â§ maxSize" ‚Üí If violated, memory leak detected

10. **Chaos Engineering**:
    - Deliberately inject failures (network timeout, disk full, OOM)
    - Verify system degrades gracefully, not catastrophically
    - Document unexpected failure modes discovered
    - Example: Inject 500ms delay ‚Üí discover unbounded queue growth

**When to Apply Each Strategy**:

| Situation | Strategy | Action |
|-----------|----------|--------|
| Tests fail repeatedly | Five Whys | Ask why 5 times to find root cause |
| Critical system | Pre-Mortem | Assume it failed, work backwards |
| Complex integration | Fault Tree | Map all failure paths, add mitigations |
| Edge case concerns | What-If Scenarios | Enumerate edge cases, test each |
| High-risk component | FMEA | Calculate risk scores, prioritize fixes |
| Performance issue | Trace Analysis | Profile BEFORE optimizing |
| Uncertain approach | Hypothesis Testing | State hypothesis, measure outcome |
| Reinventing wheel | Comparative Analysis | Search for existing solutions |
| Subtle bugs | Invariant Checking | Define properties, add assertions |
| Reliability concerns | Chaos Engineering | Inject failures, verify graceful degradation |

**Verification Strategy Selection**:
- Unit tests: Isolated component behavior
- Integration tests: Component interactions with controlled data
- Stress tests: High-volume, concurrent, edge-case scenarios
- End-to-end tests: Full user journey with synthetic data
- Manual smoke tests: Quick spot-check of critical paths
- Observability: Metrics, logs, traces to verify runtime behavior

**Output of STRATEGIZE**:
- Task type identified
- Verification methodology chosen (with rationale)
- Problem-solving approach selected
- Verification strategy defined
- Synthetic data/simulation approach documented (if applicable)
- Success criteria: What constitutes "confirmed working"?

**Skip STRATEGIZE only if**:
- Task is trivial (< 10 line change, no logic, obvious verification)
- Task type and methodology immediately obvious
- Standard verification (unit + integration test) clearly sufficient

**If you skip, you MUST still answer**:
- "How will I verify this actually works?"
- "What data/inputs will I test with?"
- "What could go wrong that my verification won't catch?"

### Stage 1: SPEC
- Restate the goal in your own words
- List explicit acceptance criteria
- Cite relevant docs (architecture, standards, existing code)
- Identify constraints (performance, compatibility, security)
- **Reference STRATEGIZE outputs**: Link to chosen methodology and verification strategy

### Stage 2: PLAN
- Break down into concrete steps
- Identify files to create/modify
- List commands to run
- Record plan hash in journal (if applicable)
- Estimate time/complexity

### Stage 3: THINK (optional but recommended)
**Be ADVERSARIAL - challenge assumptions, find holes in the plan:**

- Document open questions
- Identify ambiguities in requirements
- Propose spike investigations for unknowns
- Consider edge cases and failure modes
- **Ask hard questions:**
  - "What could go wrong here?"
  - "What am I assuming that might not be true?"
  - "What happens if X fails in production?"
  - "Have I considered the performance implications?"
  - "What edge cases am I missing?"
  - "What would break this design?"
  - "Is there a simpler way?"
- **Challenge the plan:**
  - "Does this actually solve the problem?"
  - "Am I over-engineering this?"
  - "What dependencies am I ignoring?"
  - "What happens at scale (1000x current load)?"
- **Document risks you identify:**
  - Performance bottlenecks
  - Race conditions
  - Resource exhaustion
  - Error propagation failures
  - Integration points that could fail
- Skip THINK stage only if task is completely straightforward AND you've considered adversarial questions

### Stage 4: IMPLEMENT
- Write minimal, focused diffs
- Update docs and tests concurrently (not after!)
- Follow project code style
- Log significant decisions
- Commit incremental progress

### Stage 5: VERIFY
Run the complete verification loop - iterate until ALL pass.

**CRITICAL: Execute the verification strategy chosen in STRATEGIZE stage.**

**5a. METHODOLOGY-SPECIFIC verification:**

Execute the verification methodology chosen in STRATEGIZE:

- **Synthetic Data Simulation**: Run tests with generated fixtures, verify edge cases covered
- **Controlled Integration Harness**: Execute harness, verify outputs match expectations
- **Incremental Capability Verification**: Test each layer, verify composition works
- **Property-Based Testing**: Run property tests, verify invariants hold across input space
- **Regression Benchmarking**: Run benchmarks, compare to baseline (must be within tolerance)
- **Visual/Snapshot Testing**: Generate outputs, diff against snapshots (must match)
- **State Space Exploration**: Test all transitions, verify state machine correctness
- **Chaos/Fault Injection**: Inject failures, verify graceful degradation

**Document verification results:**
- What data/inputs were tested?
- What was the outcome?
- Were there any surprises or edge cases discovered?
- Does this confirm the implementation actually works?

**If methodology verification fails ‚Üí back to STRATEGIZE (methodology wrong?) or IMPLEMENT (implementation wrong)**

**5b. BUILD verification:**
```bash
cd tools/wvo_mcp && npm run build
```
- Must complete with ZERO errors
- If errors ‚Üí FIX ‚Üí repeat from IMPLEMENT

**5c. TEST verification:**
```bash
npm test  # Run all tests
bash scripts/validate_test_quality.sh path/to/test.ts  # Check quality
```
- All tests must pass
- Tests must cover all 7 dimensions (see UNIVERSAL_TEST_STANDARDS.md)
- If tests fail ‚Üí FIX ‚Üí repeat from IMPLEMENT
- If coverage shallow ‚Üí ADD TESTS ‚Üí repeat from IMPLEMENT

**5d. AUDIT verification:**
```bash
npm audit  # Must show 0 vulnerabilities
```
- If vulnerabilities ‚Üí `npm audit fix` ‚Üí repeat from IMPLEMENT

**5e. RUNTIME verification (for features):**
- Actually RUN the feature end-to-end
- Test with realistic data (100+ items if applicable)
- Monitor resources (memory, CPU, processes)
- If crashes/errors ‚Üí FIX ‚Üí repeat from IMPLEMENT

**5f. STRESS TESTING (for critical components):**

**When to stress test:**
- Router/orchestrator systems (high decision volume)
- File I/O operations (discovery, catalog writes)
- Resource-intensive operations (model selection, escalation)
- Public APIs or high-traffic code paths
- Components that manage state or cache

**What to stress test:**
```bash
# Create *.stress.test.ts files alongside functional tests
npm test -- path/to/*.stress.test.ts
```

**Required stress test categories:**

1. **High-Volume Operations**
   - 1000+ iterations for decision-making paths
   - 100+ concurrent requests for async operations
   - Performance targets:
     - Synchronous operations: p50 < 10ms, p95 < 50ms, p99 < 100ms
     - Async operations: p50 < 100ms, p95 < 500ms, p99 < 1s
   - Memory growth: < 10MB for 1000 operations

2. **Concurrency & Race Conditions**
   - 10-100 concurrent requests (use Promise.all)
   - Verify no data corruption or state inconsistency
   - Test shared resource access (files, caches, state)

3. **Memory Leak Detection**
   - Run 100+ iterations with memory sampling every 10 iterations
   - Force GC if available: `if (global.gc) global.gc();`
   - Memory growth limit: < 5MB over 100 iterations
   - Clean up temp resources in afterEach hooks

4. **Edge Cases & Malformed Data**
   - Extremely long strings (1000+ chars) ‚Üí **Example: runId truncation**
   - Extremely large numbers (Number.MAX_SAFE_INTEGER)
   - Special characters (path traversal: `../../../etc/passwd`)
   - Empty/null/undefined values
   - Missing required dependencies

5. **Performance Benchmarks**
   - Run 50-100 iterations, calculate percentiles (p50, p95, p99)
   - Compare against baseline (document in test comments)
   - Fail if regression > 50% from baseline

6. **Resource Exhaustion**
   - Test behavior when filesystem is full
   - Test behavior when all providers circuit-broken
   - Test behavior with missing env vars
   - Test graceful degradation, not silent failures

7. **Unforeseen Error Detection**
   - **Think like an attacker**: What could break this?
   - **Think about production**: What happens at 3 AM on Black Friday?
   - **Think about edge cases**: What values did we never consider?
   - Document "this test found bug X" in commit messages
   - **Real example from Phase 1**: Stress test found ENAMETOOLONG error for long runIds (filesystem limit 255 bytes), fixed by adding truncation/hashing

**Stress testing checklist:**
- ‚úÖ High-volume operations (1000+ iterations)
- ‚úÖ Concurrent requests (10-100 parallel)
- ‚úÖ Memory leak detection (100+ iterations with sampling)
- ‚úÖ Edge cases (long strings, extreme values, malformed data)
- ‚úÖ Performance benchmarks (p50, p95, p99 within targets)
- ‚úÖ Resource exhaustion (graceful degradation)
- ‚úÖ Unforeseen errors (found and documented)

**If stress tests find bugs:**
- FIX immediately (don't defer)
- Document root cause in commit message
- Update functional tests to prevent regression
- Repeat verification loop from IMPLEMENT

### Stage 6: REVIEW
**Be ADVERSARIAL - tear the implementation apart, find flaws:**

Self-review checklist - be thorough, honest, and CRITICAL:

- **Readability**: Can another developer understand this code?
  - *Adversarial check*: "Would I understand this code in 6 months?"
  - *Adversarial check*: "Can I explain this to a junior developer in 2 minutes?"

- **Maintainability**: Is it easy to modify or extend?
  - *Adversarial check*: "What happens when requirements change?"
  - *Adversarial check*: "How hard would it be to add a new feature?"
  - *Adversarial check*: "Am I creating future tech debt?"

- **Performance**: Any obvious bottlenecks? (N+1 queries, unbounded loops, etc.)
  - *Adversarial check*: "What happens with 10,000 items instead of 10?"
  - *Adversarial check*: "What's the worst-case complexity?"
  - *Adversarial check*: "Will this cause memory leaks?"

- **Security**: Any injection risks, auth bypasses, secret leaks?
  - *Adversarial check*: "How would I exploit this?"
  - *Adversarial check*: "What if the input is malicious?"
  - *Adversarial check*: "Could this leak sensitive data in logs?"

- **Error handling**: Are edge cases covered?
  - *Adversarial check*: "What if this API call fails?"
  - *Adversarial check*: "What if the file doesn't exist?"
  - *Adversarial check*: "What if we get invalid data?"
  - *Adversarial check*: "What if the system is under load?"

- **Testing**: Do tests actually verify behavior (not just "go green")?
  - *Adversarial check*: "Do tests verify the RIGHT behavior or just ANY behavior?"
  - *Adversarial check*: "Would these tests catch a regression?"
  - *Adversarial check*: "Am I testing implementation details instead of behavior?"
  - *Adversarial check*: "What critical edge cases am I NOT testing?"

**CRITICAL**: If you can't find ANY issues, you're not being adversarial enough. Every implementation has flaws - find them NOW, not in production.

Document findings with file:line references. Fix issues before proceeding.

**üö® MANDATORY: ZERO GAPS POLICY üö®**

**If REVIEW identifies ANY gaps, missing functionality, or placeholder implementations:**

‚ùå **NEVER say:** "Ship it with known gaps documented"
‚ùå **NEVER say:** "This can be addressed in Phase 7"
‚ùå **NEVER say:** "TODO: implement later"
‚ùå **NEVER say:** "This is non-blocking"

‚úÖ **ALWAYS:** Fix ALL gaps immediately before claiming task complete

**What counts as a "gap":**
- Placeholder values (e.g., `tokens: 0`, `cost: 0` when real data exists)
- Missing integrations (e.g., "needs to wire with X" when X exists)
- Incomplete features (e.g., dashboard has `byTaskType` but not `byEpic` when tags support both)
- "TODO" comments in production code
- Features mentioned in SPEC but not implemented
- Test coverage gaps for critical paths

**Exception: Only acceptable gap is explicit future scope**
If something is genuinely out of scope (not mentioned in SPEC, not required by acceptance criteria), document why it's excluded.

**Process when gaps found:**
1. Document ALL gaps with specific file:line references
2. Loop back to IMPLEMENT and fix every gap
3. Re-run VERIFY (build, tests, runtime)
4. REVIEW again - must find ZERO gaps
5. Only proceed to PR/MONITOR when gaps == 0

**This is non-negotiable. Deferred gaps become technical debt.**

**REVIEW CAN SURFACE NEW TASKS:**

If REVIEW identifies issues that require significant work, you have the authority to loop back through the protocol:

- **Minor fixes** (< 30 min): Fix directly, repeat VERIFY
- **Moderate issues** (30 min - 2 hours): Loop back to IMPLEMENT ‚Üí VERIFY ‚Üí REVIEW
- **Major issues** (> 2 hours OR architectural changes): Loop back to SPEC ‚Üí PLAN ‚Üí THINK ‚Üí IMPLEMENT ‚Üí VERIFY ‚Üí REVIEW

**When to create a new task vs. loop back:**
- If the issue is within the original scope ‚Üí loop back and fix
- If the issue requires NEW features or scope expansion ‚Üí create a new task, document in roadmap

**‚ö†Ô∏è INFINITE REVIEW CYCLE PREVENTION:**

**REVIEW must be critical, but not perfectionistic. Only fix issues that:**
1. **Violate acceptance criteria** from SPEC
2. **Create production risks** (security, performance, reliability)
3. **Block integration** or break existing features
4. **Fail verification checks** (build, test, audit)

**DO NOT loop back for:**
- "Nice to have" improvements not in original scope
- Style preferences (if code passes linter)
- Theoretical edge cases with negligible probability
- Optimizations that don't address measured bottlenecks
- Refactorings that don't fix actual problems

**Loop limit: 3 iterations**
- If REVIEW ‚Üí fix ‚Üí REVIEW ‚Üí fix ‚Üí REVIEW ‚Üí fix ‚Üí REVIEW identifies MORE issues
- You are in an infinite cycle (either too perfectionistic OR fundamental design flaw)
- ESCALATE to user per protocol below

### AUTONOMOUS MODULARIZATION REVIEW POLICY

**üéØ CRITICAL: Files that become excessively long DEMAND autonomous modularization review**

**Trigger Thresholds:**
- Any file exceeds 500 lines ‚Üí AUTOMATIC review required
- Any function exceeds 100 lines ‚Üí AUTOMATIC refactoring required
- Any class exceeds 300 lines ‚Üí AUTOMATIC modularization required

**When threshold exceeded, you MUST:**

1. **Immediately pause current work**
2. **Document the violation:**
   ```
   File: path/to/file.ts (642 lines, threshold: 500)
   Complexity: 8/10
   Reason: Monolithic switch statement + infrastructure
   ```

3. **Create modularization spec** (following Spec‚ÜíMonitor protocol):
   ```
   SPEC: Modularize [filename]
   - Goal: Reduce to <500 lines
   - Approach: Extract modules, delegate responsibilities
   - Acceptance: Build passes, tests pass, line count <500
   ```

4. **Execute complete protocol:**
   - SPEC: Define modularization strategy
   - PLAN: Break down into modules
   - THINK: Challenge design, find simpler approaches
   - IMPLEMENT: Extract modules with tests
   - VERIFY: Ensure no regressions
   - REVIEW: Verify modularity achieved
   - PR: Document changes
   - MONITOR: Smoke test refactored system

5. **No exceptions** - this is MANDATORY:
   - "It works fine" ‚Üí NOT acceptable
   - "Too busy" ‚Üí NOT acceptable
   - "Will do later" ‚Üí NOT acceptable
   - Technical debt compounds - fix it NOW

**Why this matters:**
- Large files = hard to understand, hard to test, hard to modify
- Phase 3 lesson: StateGraph grew to 648 lines BECAUSE we didn't modularize earlier
- Prevention is cheaper than remediation
- Continuous modularization prevents technical debt accumulation

**Example trigger:**
```typescript
// During REVIEW of any task
if (fileLines > 500) {
  logWarning('Modularization threshold exceeded', { file, lines: fileLines });
  // Create new task: "Modularize [filename]"
  // Execute Spec‚ÜíMonitor for modularization
  // Resume original task ONLY after modularization complete
}
```

**Documentation:**
- Create `docs/refactoring/[filename]_MODULARIZATION.md`
- Document: before/after line counts, modules extracted, tests added
- Include in commit message

### Stage 7: PR (Pull Request Preparation & Git Workflow)

**CRITICAL: Git/GitHub is a core autopilot competency. Handle it elegantly.**

See `AGENTS.md` ¬ß "Stage 7: PR" for complete git/GitHub workflow documentation.

**Git Workflow Essentials**:
- Use conventional commits: `feat(scope): description`
- Branch naming: `feature/<task-id>-<description>`
- Clean commit history (squash WIP commits before PR)
- Co-author attribution: Include `Co-Authored-By: Claude <noreply@anthropic.com>`

**GitHub PR Creation** (using gh CLI):
```bash
gh pr create \
  --title "feat(component): Brief description" \
  --body "Complete PR template with evidence, risks, checklist" \
  --reviewer @team \
  --label "task-id"
```

**PR Template Must Include**:
- **Summary**: What changed and why?
- **Evidence**: Build ‚úÖ, Tests ‚úÖ, Audit ‚úÖ, Runtime ‚úÖ
- **Quality Gates**: All 5 gates passed (automated, orchestrator, adversarial, peer, domain expert)
- **Risks & Rollback**: What could break? How to revert?
- **Performance Impact**: Benchmarks if applicable
- **Testing Checklist**: Unit, integration, stress, smoke
- **Reviewers**: Who should review?
- **Autopilot Evidence Chain**: Task ID, decision log, artifacts

**PR Best Practices**:
1. Keep PRs small (<400 lines)
2. Self-review before requesting review
3. CI must pass before requesting review
4. Link to related issues/tasks
5. Include screenshots for UI changes
6. Respond to review comments within 24h

### Stage 8: MONITOR
Execute smoke tests and monitor:

```bash
# For autopilot changes
bash scripts/app_smoke_e2e.sh

# For specific features, run relevant smoke tests
```

- Capture outputs
- Check for warnings/errors in logs
- Verify metrics (if applicable)
- Document any anomalies

**Only after Stage 8 completes successfully can you claim the task is "done".**

### Exit Criteria (ALL must be true):

- ‚úÖ SPEC written with clear acceptance criteria
- ‚úÖ PLAN documented with steps
- ‚úÖ THINK completed (or explicitly skipped with justification)
- ‚úÖ IMPLEMENT completed with minimal diffs
- ‚úÖ VERIFY: Build passes (0 errors)
- ‚úÖ VERIFY: All tests pass
- ‚úÖ VERIFY: Test coverage is 7/7 dimensions
- ‚úÖ VERIFY: npm audit shows 0 vulnerabilities
- ‚úÖ VERIFY: Runtime verification passed (if applicable)
- ‚úÖ REVIEW: Self-review completed, issues fixed
- ‚úÖ PR: Summary, evidence, risks documented
- ‚úÖ MONITOR: Smoke tests passed, outputs clean

**If ANY criterion fails, iterate from the appropriate stage - do NOT skip ahead.**

### ‚ö†Ô∏è ESCALATION PROTOCOL: Infinite Loops & Regressions

**If you iterate more than 5 times OR detect a regression loop, STOP and ESCALATE:**

**Infinite Loop Detection:**
- Same error appears 3+ times
- Fixing A breaks B, fixing B breaks A (regression cycle)
- No progress after 5 iterations

**When detected, ESCALATE immediately:**
1. **STOP iterating** - you're stuck
2. **Document the loop:**
   - What you tried (all iterations)
   - What keeps breaking
   - The cycle pattern
3. **Escalate to supervisor:**
   - Tag @user in discussion
   - Describe the fundamental problem
   - Propose architectural fix if known
4. **Do NOT:**
   - Keep iterating (wastes resources)
   - Try workarounds that mask the issue
   - Claim "done" because you're tired

**Example escalation:**
```
@user - Infinite loop detected in verification:

Iterations:
1. Fixed TypeScript error in file A ‚Üí broke tests in file B
2. Fixed tests in file B ‚Üí TypeScript error in file A returns
3. Fixed both ‚Üí npm audit fails
4. Fixed audit ‚Üí TypeScript error in file A returns
5. Same cycle repeating

Root cause: Files A and B have circular dependency
Proposed fix: Refactor to remove circular dependency

Escalating for architectural guidance.
```

**The rule: If you can't get all checks passing in 5 iterations, there's a deeper problem. Escalate, don't iterate forever.**

## Operational Checklist
- **Sync context:** Call `plan_next` (`minimal=true`) and `autopilot_status` before contributing. The status payload includes the latest audit cadence, consensus trend, staffing recommendation, and token pressure. If either tool fails, trigger `./tools/wvo_mcp/scripts/restart_mcp.sh`.
- **Monitor system health:** Check `state/analytics/health_checks.jsonl` for recent issues. Review `state/escalations/` for unresolved problems. Use ErrorDetector/HealthMonitor for proactive detection and auto-remediation.
- **Inspect telemetry:** Review `state/analytics/orchestration_metrics.json` for recent decisions. Confirm follow-up tasks exist for any `critical` or `quorum_satisfied=false` entry; assign Atlas for execution and Director Dana for executive decisions.
- **Maintain context health:** Keep `state/context.md` within ~1000 words. `TokenEfficiencyManager` automatically trims overflow and records backups under `state/backups/context/`; restore only what is still relevant.
- **Run the integrity batch:** Execute `bash tools/wvo_mcp/scripts/run_integrity_tests.sh` before declaring stability. Attach failures to the consensus record so Atlas can remediate with the right batch step.
- **Checkpoint regularly:** Use `state_save` after major updates and ensure blockers/decisions land in the context file so Atlas and Dana receive complete briefs.
- **VERIFY BEFORE CLAIMING DONE:** Follow the Mandatory Verification checklist above for EVERY task. No exceptions.

## Decision Framework
- **Consensus:** Uphold quorum rules. When a decision escalates, gather proposals from critics, codify disagreements, and outline the safest path to resolution. Only override follow-up tasks if the telemetry shows quorum restored and blockers cleared.
- **Staffing guidance:** Interpret the recommendation in `autopilot_status.consensus.recommendation`. If load is `High critical decision volume`, ensure Director Dana stays engaged and consider temporarily promoting additional Claude strategists.
- **Risk triage:** Prioritise issues that threaten guardrails (budget pushes, retention compliance, automation safety) over throughput concerns.

## Collaboration Patterns
- **Atlas (Autopilot lead):** Provide crisp directions, including which critic or engineer should close the loop. Confirm Atlas acknowledges consensus follow-ups before marking decisions resolved.
- **Director Dana:** Use Dana for policy-level approvals or when consensus highlights leadership trade-offs. Summarise the telemetry evidence and recommended action.
- **Critic Corps:** Reference `tools/wvo_mcp/scripts/run_integrity_tests.sh` output and critic artifacts when requesting fixes. Flag any intent drift (tests edited merely to go green) so TestsCritic can intervene.

## Guardrails & Escalation
- Never disable consensus, token efficiency management, or autopilot safety flags without explicit sign-off in `state/context.md`.
- If MCP tools become unresponsive or telemetry stops updating, halt execution and escalate to infrastructure owners before continuing.
- Preserve backups and evidence. When trimming context, note the backup filename in your briefing so others can recover history if needed.

By following this brief, Claude maintains WeatherVane‚Äôs strategic posture while Atlas drives the implementation safely and efficiently.
