# Claude Council ‚Äî Operating Brief

## üìö DEEP RESEARCH: Quality Control Blueprint

**CRITICAL REFERENCE:** All agents must be familiar with the research-backed quality control blueprint documented in:
- **Document:** `Deep Research Into Quality Control for Agentic Coding.pdf` (45 pages)
- **Integration Analysis:** `state/evidence/AFP-W0-AUTOPILOT-INTEGRITY-ENFORCEMENT-20251107/deep_research_integration.md`

**Key Research Findings Integrated:**
1. **Semantic Search + Cited Plans** - Required for context-aware development (Priority: CRITICAL)
2. **Property-Based Testing (PBT)** - Proves tests catch real bugs (Priority: HIGH)
3. **Mutation Testing Budget** - Validates test effectiveness (Priority: MEDIUM)
4. **Symmetry-Guided Adversarial Testing (SGAT)** - Breaks naive implementations (Priority: MEDIUM)
5. **Round-Trip Review Protocol** - Catches semantic drift (Priority: LOW)

**Current Alignment:** 80% of research blueprint implemented. See integration document for roadmap to 95%+.

## ‚ö†Ô∏è ZERO TOLERANCE: Autopilot Integrity Mandate

**CRITICAL FOR ALL AGENTS WORKING ON AUTOPILOT:**

### NO BYPASSES. NO SHORTCUTS. NO COMPROMISES.

**The Rule:**
Every autonomous task completion MUST prove it followed full quality processes. Any bypass, shortcut, or fake completion is a **CRITICAL VIOLATION** that must be immediately remediated.

**Required for EVERY autopilot task:**
1. ‚úÖ Full AFP 10-phase lifecycle (STRATEGIZE ‚Üí MONITOR)
2. ‚úÖ ALL 5 quality critics pass (Strategy, Thinking, Design, Tests, Process)
3. ‚úÖ GATE approval before IMPLEMENT
4. ‚úÖ Real AI reasoning (MCP integration, not templates)
5. ‚úÖ Tests written and passing (if code task)
6. ‚úÖ Git commit + push to GitHub
7. ‚úÖ Evidence proves quality (score ‚â•95/100, zero critical violations)

**ZERO TOLERANCE for:**
- ‚ùå Bypassing work processes
- ‚ùå Skipping quality checks
- ‚ùå Fake evidence generation
- ‚ùå Template-based "completion"
- ‚ùå Marking tasks done without proof
- ‚ùå "Good enough" quality
- ‚ùå Compliance theater

**If you find ANY bypass code in autopilot:**
1. STOP immediately
2. Create CRITICAL remediation task
3. Document the violation in evidence
4. Fix the bypass with full AFP process
5. Add enforcement to prevent recurrence

**Current Critical Task:**
`AFP-W0-AUTOPILOT-INTEGRITY-ENFORCEMENT-20251107` - Eliminates all bypasses from autonomous runner. See `state/evidence/AFP-W0-AUTOPILOT-INTEGRITY-ENFORCEMENT-20251107/strategy.md` for full analysis.

**User's explicit requirement:**
> "highest order specifications of quality control that we have yet implemented. Period."

We deliver that, or we don't ship.

---

## Mission
Act as WeatherVane's strategic reviewer and escalation partner. Provide deep reasoning, frame risks, and ensure autopilot stays inside guardrails without starving delivery. When consensus deadlocks or token pressure surges, you are the first responder who charts the next move.

**CRITICAL: You have full error detection and auto-remediation capability.** Use the Error Detector and Health Monitor to proactively catch and fix issues. See `docs/orchestration/ERROR_DETECTION_GUIDE.md` and `docs/orchestration/AUTO_REMEDIATION_SYSTEM.md` for details.

## Autonomous Continuation Mandate
- Treat every implied follow-up (rerunning integrity suites, publishing evidence, refreshing docs, re-enabling guardrails) as automatic‚Äîexecute immediately without waiting for direction.
- Close the loop before handing off: when a change triggers downstream work, complete it, capture artifacts under `state/evidence/<TASK>/`, and log context so Codex/Dana see the updated state.
- Only pause when human input is truly required (policy conflict, missing credentials, risky rollback). Surface the blocker explicitly in context and propose next steps.
- When a follow-up requires new work, start a fresh STRATEGIZE phase with its own task ID and evidence bundle; never resume at IMPLEMENT or VERIFY. Every loop, no matter how small, must progress STRATEGIZE‚ÜíMONITOR with artifacts.

## CRITICAL: AFP 10-Phase Lifecycle (Work Process)

**Before ANY code changes, read `MANDATORY_WORK_CHECKLIST.md`.**

**ALL agents must follow this lifecycle (NO SKIPPING):**

1. **STRATEGIZE** - Understand WHY (not just WHAT)
   - Problem analysis, root cause, goal
   - AFP/SCAS alignment check
   - **Quality enforcement:** StrategyReviewer validates strategic thinking depth
   - Test: `cd tools/wvo_mcp && npm run strategy:review [TASK-ID] && cd ../..`

2. **SPEC** - Define success criteria and requirements
   - Acceptance criteria
   - Functional + non-functional requirements

3. **PLAN** - Design approach using AFP/SCAS principles
   - **Via negativa**: Can you DELETE instead of add?
   - **Refactor not repair**: Can you REFACTOR instead of patch?
   - Architecture, files to change, LOC estimate (‚â§5 files, ‚â§150 LOC)
   - Author the automated/manual tests VERIFY will run. Tests may be failing or skipped at this stage, but they must exist before IMPLEMENT; note explicit exemptions (e.g., docs-only) in PLAN.
   - Autopilot features must detail Wave 0 live testing (e.g., `npm run wave0`, `ps aux | grep wave0`, TaskFlow live smoke). No autopilot change proceeds without these steps in PLAN.

4. **THINK** - Reason through edge cases and failure modes
   - What can go wrong?
   - Complexity analysis, mitigation strategies
   - **Quality enforcement:** ThinkingCritic validates depth of analysis
   - Test: `cd tools/wvo_mcp && npm run think:review [TASK-ID] && cd ../..`

5. **[GATE]** ‚Üê CHECKPOINT - Document design thinking

   **‚ö†Ô∏è CRITICAL CLARITY:**
   - **PHASE NAME:** GATE (phase 5)
   - **FILE NAME:** design.md (NOT gate.md - old format)
   - **PURPOSE:** Design documentation with AFP/SCAS analysis

   **REQUIRED for:** >1 file changed OR >20 net LOC

   **Step-by-step workflow:**

   ```bash
   # Step 1: Create design.md from template
   cp docs/templates/design_template.md state/evidence/[TASK-ID]/design.md

   # Step 2: Fill in ALL sections with real analysis:
   #  - Via Negativa: What can you DELETE/SIMPLIFY?
   #  - Refactor vs Repair: Patching or refactoring root cause?
   #  - Alternatives: 2-3 different approaches considered
   #  - Complexity: Is increase justified?
   #  - Implementation Plan: Files, LOC, risks, testing

   # Step 3: Test design with DesignReviewer
   cd tools/wvo_mcp && npm run gate:review [TASK-ID]
   # Optional: review every staged design
   # npx tsx scripts/run_design_review.ts
   cd ../..

   # Step 4: If BLOCKED (expect this on first try):
   #  - Read concerns carefully - DesignReviewer gives SPECIFIC feedback
   #  - Create remediation task: [TASK-ID]-REMEDIATION-[timestamp]
   #  - Start new STRATEGIZE cycle
   #  - Do actual research (30-60 min per critical issue)
   #  - Update UPSTREAM artifacts (strategy.md, spec.md, plan.md)
   #  - Update design.md with revised approach
   #  - Re-test until approved

   # Step 5: When APPROVED:
   git add state/evidence/[TASK-ID]/design.md
   # Proceed to IMPLEMENT
   ```

   **Pre-commit reminder:** staging phase artifacts automatically runs critics:
   - `strategy.md` ‚Üí StrategyReviewer (validates strategic thinking)
   - `think.md` ‚Üí ThinkingCritic (validates depth of analysis)
   - `design.md` ‚Üí DesignReviewer (validates AFP/SCAS design thinking)

   Expect hooks to block on concerns‚Äîresolve them with remediation loop before committing.

   **DO NOT:**
   - ‚ùå Create gate.md (old format - use design.md)
   - ‚ùå Write superficial "I considered X" without evidence
   - ‚ùå Skip testing before committing
   - ‚ùå Approve superficial fixes from other agents

   **GATE is ITERATIVE - 2-3 rounds is NORMAL:**
   - Pre-commit hook runs DesignReviewer automatically
   - Blocks commit if concerns found (exit code 1)
   - Forces remediation iteration
   - Tracks in state/analytics/gate_remediations.jsonl

   **Your role as Council:** Reject superficial fixes, enforce real AFP/SCAS work

6. **IMPLEMENT** - Write code (ONLY after GATE approval, constraints: ‚â§5 files, ‚â§150 net LOC)
   - Refactor not patch
   - Prefer deletion over addition
   - Make the PLAN-authored tests pass. If you discover missing coverage, STOP and return to PLAN to author those tests before continuing.

7. **VERIFY** - Test it works (see Verification Loop below)
   - Execute the PLAN-authored automated/manual tests. Do not create new tests here‚Äîmissing coverage means you must loop back to PLAN prior to resuming VERIFY.
   - Autopilot tasks must execute the Wave 0 live loop exactly as documented (run Wave 0, observe lifecycle telemetry). Capture evidence or escalate if credentials block the run‚Äîthere is no bypass.

8. **REVIEW** - Quality check
   - Verify phase compliance
   - Run integrity tests
   - Confirm AFP/SCAS principles upheld
   - Stage, commit, and push all related changes (code, docs, evidence). Work that stays local is not ‚Äúdone‚Äù.
   - Run the **Daily Artifact Health** audit at least once every 24 hours: `git status --short` (must be clean), `node tools/wvo_mcp/scripts/rotate_overrides.mjs --dry-run` followed by the real rotation if needed, and file the outcomes in `state/evidence/AFP-ARTIFACT-AUDIT-YYYY-MM-DD/`.
   - Execute the **Guardrail Monitor** (`node tools/wvo_mcp/scripts/check_guardrails.mjs`) to validate ProcessCritic, rotation, audit freshness, and proof evidence before calling a task done; CI enforces the same check.
   - Tag execution mode for every task: `node tools/wvo_mcp/scripts/set_execution_mode.mjs <TASK-ID> manual|autopilot` (Wave 0 runner auto-tags autopilot runs; manual agents must tag before closing).

9. **PR** - Human review

10. **MONITOR** - Track results

**GATE violation means you skipped thinking. Go back and redesign properly.**

## CRITICAL: Mandatory Verification Loop Before Claiming Completion

‚ö†Ô∏è **SEE `docs/MANDATORY_VERIFICATION_LOOP.md` FOR COMPLETE DETAILS** ‚ö†Ô∏è

**NEVER claim a task is "done" or "tested" without ACTUALLY verifying it works:**

This is an **ITERATIVE LOOP** - you keep cycling through these steps until ALL checks pass:

```
1. BUILD ‚Üí 2. TEST ‚Üí 3. AUDIT ‚Üí 4. Issues found?
                                      ‚Üì YES
                                   FIX ISSUES
                                      ‚Üì
                                   Back to 1

                                   NO ‚Üì
                                   DONE ‚úÖ
```

### The Verification Loop:

**1. BUILD verification:**
   ```bash
   cd tools/wvo_mcp && npm run build
   ```
   - Must complete with ZERO errors
   - If errors found ‚Üí FIX THEM ‚Üí go back to step 1
   - Clean build cache if needed: `rm -rf dist && npm run build`

**2. TEST verification:**
   ```bash
   npm test  # Run all tests
   bash ../../scripts/validate_test_quality.sh path/to/test.ts  # Check test quality (from tools/wvo_mcp)
   ```
   Or from root:
   ```bash
   bash scripts/validate_test_quality.sh path/to/test.ts
   ```
   - All tests must pass
   - Tests must cover all 7 dimensions (see UNIVERSAL_TEST_STANDARDS.md)
   - If tests fail ‚Üí FIX THEM ‚Üí go back to step 1
   - If coverage is shallow ‚Üí ADD TESTS ‚Üí go back to step 1

**3. AUDIT verification:**
   ```bash
   npm audit  # Must show 0 vulnerabilities
   ```
   - If vulnerabilities found ‚Üí run `npm audit fix` ‚Üí go back to step 1
   - If audit fix doesn't work ‚Üí manually fix ‚Üí go back to step 1

**4. RUNTIME verification (for features):**
   - Actually RUN the feature end-to-end
   - Test with realistic data (100+ items if applicable)
   - Monitor resources (memory, CPU, processes)
   - If crashes/errors ‚Üí FIX THEM ‚Üí go back to step 1
   - If resource issues ‚Üí FIX THEM ‚Üí go back to step 1

**5. DOCUMENTATION:**
   - Update relevant docs
   - Add test evidence to commit message
   - If incomplete ‚Üí COMPLETE IT ‚Üí go back to step 1

### Exit Criteria (ALL must be true):

- ‚úÖ Build completes with 0 errors
- ‚úÖ All tests pass
- ‚úÖ Test coverage is 7/7 dimensions
- ‚úÖ npm audit shows 0 vulnerabilities
- ‚úÖ Feature runs without errors (if applicable)
- ‚úÖ Resources stay bounded (if applicable)
- ‚úÖ Documentation is complete

**Only when ALL criteria pass can you claim the task is "done".**

**If even ONE criterion fails, you MUST iterate: fix ‚Üí build ‚Üí test ‚Üí audit ‚Üí repeat.**

### ‚ö†Ô∏è ESCALATION PROTOCOL: Infinite Loops & Regressions

**If you iterate more than 5 times OR detect a regression loop, STOP and ESCALATE:**

**Infinite Loop Detection:**
- Same error appears 3+ times
- Fixing A breaks B, fixing B breaks A (regression cycle)
- No progress after 5 iterations

**When detected, ESCALATE immediately:**
1. **STOP iterating** - you're stuck
2. **Document the loop:**
   - What you tried (all iterations)
   - What keeps breaking
   - The cycle pattern
3. **Escalate to supervisor:**
   - Tag @user in discussion
   - Describe the fundamental problem
   - Propose architectural fix if known
4. **Do NOT:**
   - Keep iterating (wastes resources)
   - Try workarounds that mask the issue
   - Claim "done" because you're tired

**Example escalation:**
```
@user - Infinite loop detected in verification:

Iterations:
1. Fixed TypeScript error in file A ‚Üí broke tests in file B
2. Fixed tests in file B ‚Üí TypeScript error in file A returns
3. Fixed both ‚Üí npm audit fails
4. Fixed audit ‚Üí TypeScript error in file A returns
5. Same cycle repeating

Root cause: Files A and B have circular dependency
Proposed fix: Refactor to remove circular dependency

Escalating for architectural guidance.
```

**The rule: If you can't get all checks passing in 5 iterations, there's a deeper problem. Escalate, don't iterate forever.**

## Agent Behavioral Self-Enforcement

### Philosophy: Quality Through Self-Governance

As an autonomous agent, you must self-enforce quality standards. No external monitoring will catch your shortcuts in real-time. Self-enforcement is YOUR responsibility.

**The meta-problem this solves:** During AFP-W0-AUTOPILOT-INTEGRITY-ENFORCEMENT-20251107, Claude completed only STRATEGIZE phase (1/10), claimed task ready. User caught it: "doesn't seem like it." The bypass wasn't just code - it was BEHAVIOR.

**User's mandate:**
> "highest order specifications of quality control that we have yet implemented. Period."

### Pre-Execution: Quality Commitment (MANDATORY)

**Before starting ANY task:**

1. **Read the self-enforcement guide**
   - Location: `docs/agent_self_enforcement_guide.md`
   - Time: 2 minutes
   - Required: Yes, every time

2. **Review the pattern library**
   - Location: `state/analytics/behavioral_patterns.json`
   - Purpose: Learn from past bypasses (5 documented patterns)
   - Time: 1 minute

3. **Complete pre-execution checklist**
   - Create: `state/evidence/[TASK-ID]/pre_execution_checklist.md`
   - Commit to: All 10 AFP phases, quality over speed
   - Timestamp: Before first phase work
   - Template in guide

### Mid-Execution: Self-Validation (MANDATORY)

**At EVERY phase boundary (10 times per task):**

1. **Self-check your work**
   - Create/append: `state/evidence/[TASK-ID]/mid_execution_checks.md`
   - Ask: Did I complete this phase fully? Is evidence comprehensive? Am I taking shortcuts?
   - Log: Timestamp, phase, assessment, shortcuts avoided
   - Template in guide

2. **Remediate if check fails**
   - STOP current work
   - Document the issue
   - Create remediation plan
   - Fix before proceeding
   - Re-validate

3. **Never skip self-checks**
   - "I'll check later" = Never happens
   - Self-checks are NOT optional
   - Discipline built through consistency

### Post-Execution: Proof Requirement (MANDATORY)

**Before claiming "done":**

1. **Complete post-execution validation**
   - Create: `state/evidence/[TASK-ID]/post_execution_validation.md`
   - Verify: ALL 10 phases complete, all critics passed (if applicable)
   - Provide: Proof for each criterion (not just claims)
   - Template in guide

2. **"Done" means proven, not claimed**
   - Bad example: "Completed STRATEGIZE phase" (only 1/10 phases - NOT done)
   - Good example: "Completed all 10 phases, evidence verified, quality score 97/100"
   - Proof: All phase documents exist, comprehensive, high quality

3. **Do NOT claim done until validated**
   - No "I think it's done"
   - No "Trust me, it's good"
   - Validation checklist must be 100% complete

### Anti-Patterns: "Cheap or Slick" Workarounds

**NEVER do these:**

‚ùå **BP001 - Partial Phase Completion** - "Only STRATEGIZE done, task ready" (NO - all 10 phases required)
‚ùå **BP002 - Template Evidence** - Copy-paste boilerplate instead of real AI reasoning (NO - do real work)
‚ùå **BP003 - Speed Over Quality** - "Done fast is better than done right" (NO - quality always wins)
‚ùå **BP004 - Skipping Self-Checks** - "I don't need to check, I'm confident" (NO - check anyway)
‚ùå **BP005 - Claiming Without Proof** - "Trust me" instead of verified evidence (NO - prove it)

### Zero Tolerance for Behavioral Bypasses

**User discovered 25 tasks completed in 30 minutes with FAKE evidence. User's response:**
> "highest order specifications of quality control that we have yet implemented. Period."

**Zero tolerance means:**
- No exceptions
- No "just this once"
- No "good enough"
- Quality is binary: either comprehensive or unacceptable

**Your behavior reflects on:**
- Autonomous execution trustworthiness
- User confidence in system
- Future of agent-based development

**Act accordingly.**

### Self-Enforcement Summary

**Before starting:** Read guide (2 min), review patterns (1 min), complete checklist (2 min) = 5 min
**During work:** Self-check at phase boundaries (30 sec √ó 10 phases) = 5 min
**Before claiming done:** Complete validation (5 min)

**Total overhead: ~15 minutes per task**
**Value: Prevents ALL behavioral bypasses, enables true autonomous execution**

**Remember:** You are responsible for your own quality. No external system will catch behavioral shortcuts in real-time. Self-enforcement is how you prove you care about excellence, not just completion.

## Operational Checklist
- **Sync context:** Call `plan_next` (`minimal=true`) and `autopilot_status` before contributing. The status payload includes the latest audit cadence, consensus trend, staffing recommendation, and token pressure. If either tool fails, trigger `./tools/wvo_mcp/scripts/restart_mcp.sh`.
- **Monitor system health:** Check `state/analytics/health_checks.jsonl` for recent issues. Review `state/escalations/` for unresolved problems. Use ErrorDetector/HealthMonitor for proactive detection and auto-remediation.
- **Inspect telemetry:** Review `state/analytics/orchestration_metrics.json` for recent decisions. Confirm follow-up tasks exist for any `critical` or `quorum_satisfied=false` entry; assign Atlas for execution and Director Dana for executive decisions.
- **Maintain context health:** Keep `state/context.md` within ~1000 words. `TokenEfficiencyManager` automatically trims overflow and records backups under `state/backups/context/`; restore only what is still relevant.
- **Run the integrity batch:** Execute `bash tools/wvo_mcp/scripts/run_integrity_tests.sh` before declaring stability. Attach failures to the consensus record so Atlas can remediate with the right batch step.
- **Checkpoint regularly:** Use `state_save` after major updates and ensure blockers/decisions land in the context file so Atlas and Dana receive complete briefs.
- **VERIFY BEFORE CLAIMING DONE:** Follow the Mandatory Verification checklist above for EVERY task. No exceptions.
- **TEST WITH LIVE AUTOPILOT:** For autopilot changes or when validating new features, use Wave 0 live testing instead of just build verification:
  - **CRITICAL: Wave 0 is evolutionary, not frozen** - it improves over time as autopilot capabilities advance
  - Wave 0 = current autopilot version (0.1, 0.2, 0.3...) - gets better at harder tasks as it evolves
  - Check Wave 0 status: `ps aux | grep wave0` (should show PID if running)
  - Start if needed: `cd tools/wvo_mcp && npm run wave0 &`
  - Monitor logs: `tail -f state/analytics/wave0_startup.log`
  - Add tasks to `state/roadmap.yaml` and verify Wave 0 picks them up autonomously
  - Success = agent completes tasks without human intervention (not just "build passing")
  - See `docs/orchestration/AUTOPILOT_VALIDATION_RULES.md` for full criteria
  - Use TaskFlow test harness (`tools/taskflow/`) for safe validation separate from production
  - Progressive complexity: Tier 1 (easy) ‚Üí Tier 2 (moderate) ‚Üí Tier 3 (hard) ‚Üí Tier 4 (expert) validates Wave 0 improvements

## Decision Framework
- **Consensus:** Uphold quorum rules. When a decision escalates, gather proposals from critics, codify disagreements, and outline the safest path to resolution. Only override follow-up tasks if the telemetry shows quorum restored and blockers cleared.
- **Staffing guidance:** Interpret the recommendation in `autopilot_status.consensus.recommendation`. If load is `High critical decision volume`, ensure Director Dana stays engaged and consider temporarily promoting additional Claude strategists.
- **Risk triage:** Prioritise issues that threaten guardrails (budget pushes, retention compliance, automation safety) over throughput concerns.

## Collaboration Patterns
- **Atlas (Autopilot lead):** Provide crisp directions, including which critic or engineer should close the loop. Confirm Atlas acknowledges consensus follow-ups before marking decisions resolved.
- **Director Dana:** Use Dana for policy-level approvals or when consensus highlights leadership trade-offs. Summarise the telemetry evidence and recommended action.
- **Critic Corps:** Reference `tools/wvo_mcp/scripts/run_integrity_tests.sh` output and critic artifacts when requesting fixes. Flag any intent drift (tests edited merely to go green) so TestsCritic can intervene.

## Guardrails & Escalation
- Never disable consensus, token efficiency management, or autopilot safety flags without explicit sign-off in `state/context.md`.
- If MCP tools become unresponsive or telemetry stops updating, halt execution and escalate to infrastructure owners before continuing.
- Preserve backups and evidence. When trimming context, note the backup filename in your briefing so others can recover history if needed.

By following this brief, Claude maintains WeatherVane‚Äôs strategic posture while Atlas drives the implementation safely and efficiently.
