{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeatherVane Model Validation \u2014 Reproducible Runbook\n",
    "\n",
    "Use this notebook to execute the standardized validation pipeline on cross-validated MMM results and produce audit-ready artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checklist\n",
    "- Refresh or generate cross-validation results before running.\n",
    "- Execute the notebook top-to-bottom without skipping cells.\n",
    "- Attach generated artifacts when submitting validation evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "1. Python 3.11 environment with WeatherVane dependencies installed.\n",
    "2. Cross-validation metrics JSON (default: `state/analytics/mmm_training_results_cv.json`).\n",
    "3. Optional: set the environment variable `VALIDATION_RUN_ID` to control artifact folder naming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Enforce deterministic hash seeds for reproducibility\n",
    "os.environ.setdefault(\"PYTHONHASHSEED\", \"42\")\n",
    "\n",
    "# Seed numpy RNG used for any downstream sampling\n",
    "np.random.seed(42)\n",
    "\n",
    "cwd = Path.cwd().resolve()\n",
    "candidate_roots = [cwd, cwd.parent, cwd.parents[1] if len(cwd.parents) > 1 else cwd]\n",
    "PROJECT_ROOT: Path | None = None\n",
    "for candidate in candidate_roots:\n",
    "    if (candidate / \"apps\").is_dir() and (candidate / \"notebooks\").is_dir():\n",
    "        PROJECT_ROOT = candidate\n",
    "        break\n",
    "\n",
    "if PROJECT_ROOT is None:\n",
    "    raise RuntimeError(\"Unable to locate project root. Start Jupyter from the repo root or notebooks directory.\")\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apps.model.mmm_lightweight_weather import load_cv_results_from_json\n",
    "from apps.model.validate_model_performance import (\n",
    "    ValidationThresholds,\n",
    "    export_validation_report,\n",
    "    generate_validation_report,\n",
    "    validate_all_models,\n",
    ")\n",
    "\n",
    "DEFAULT_CV_PATH = PROJECT_ROOT / \"state/analytics/mmm_training_results_cv.json\"\n",
    "CV_RESULTS_PATH = Path(os.environ.get(\"CV_RESULTS_PATH\", str(DEFAULT_CV_PATH))).expanduser().resolve()\n",
    "RUN_ID = os.environ.get(\"VALIDATION_RUN_ID\") or datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "ARTIFACT_DIR = PROJECT_ROOT / \"artifacts/validation_runs\" / RUN_ID\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Using CV results: {CV_RESULTS_PATH}\")\n",
    "print(f\"Artifacts directory: {ARTIFACT_DIR}\")\n",
    "\n",
    "if not CV_RESULTS_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Cross-validation results were not found. \"\n",
    "        \"Generate them with `python scripts/train_mmm_synthetic_cv.py --n-folds 5` before proceeding.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = load_cv_results_from_json(CV_RESULTS_PATH)\n",
    "\n",
    "if not cv_results:\n",
    "    raise ValueError(\"No cross-validation results loaded; ensure the training pipeline produced data.\")\n",
    "\n",
    "print(f\"Loaded cross-validation metrics for {len(cv_results)} tenants.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = ValidationThresholds()\n",
    "validation_results = validate_all_models(cv_results, thresholds)\n",
    "report = generate_validation_report(validation_results, thresholds)\n",
    "\n",
    "report_path = ARTIFACT_DIR / \"validation_report.json\"\n",
    "export_validation_report(validation_results, report, report_path)\n",
    "\n",
    "print(f\"Validation report saved to {report_path}\")\n",
    "print(\n",
    "    f\"Pass rate: {report['validation_summary']['pass_rate']:.1%} \"\n",
    "    f\"({report['validation_summary']['passing_models']}/{report['validation_summary']['total_models']})\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for tenant_name, result in validation_results.items():\n",
    "    records.append(\n",
    "        {\n",
    "            \"tenant\": tenant_name,\n",
    "            \"mean_r2\": float(result.mean_r2),\n",
    "            \"std_r2\": float(result.std_r2),\n",
    "            \"mean_rmse\": float(result.mean_rmse),\n",
    "            \"passes_all_checks\": result.passes_all_checks,\n",
    "            \"failure_reasons\": \"; \".join(result.failure_reasons) if result.failure_reasons else \"\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "validation_df = (\n",
    "    pd.DataFrame.from_records(records)\n",
    "    .sort_values(by=\"mean_r2\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "validation_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failing = validation_df[~validation_df[\"passes_all_checks\"]]\n",
    "if not failing.empty:\n",
    "    print(\"Failing tenants:\")\n",
    "    display(failing[[\"tenant\", \"mean_r2\", \"failure_reasons\"]])\n",
    "else:\n",
    "    print(\"All tenants pass the validation thresholds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = report[\"validation_summary\"]\n",
    "thresholds_used = report[\"thresholds\"]\n",
    "\n",
    "exit_criteria_met = summary[\"pass_rate\"] >= 0.80\n",
    "status = \"\u2705\" if exit_criteria_met else \"\u26a0\ufe0f\"\n",
    "\n",
    "print(f\"{status} Pass rate {summary['pass_rate']:.1%} (target >= 80%).\")\n",
    "print(\n",
    "    f\"Passing {summary['passing_models']} / {summary['total_models']} models | \"\n",
    "    f\"Failing {summary['failing_models']} models.\"\n",
    ")\n",
    "\n",
    "summary_path = ARTIFACT_DIR / \"validation_summary.json\"\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump({\"summary\": summary, \"thresholds\": thresholds_used}, f, indent=2)\n",
    "\n",
    "print(f\"Summary written to {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Share `validation_report.json` and `validation_summary.json` under the artifacts folder.\n",
    "- Investigate failing tenants (if any) using the failure reasons above.\n",
    "- Update remediation plans or thresholds only with reviewer approval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
