{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather-Aware MMM: Comprehensive Model Validation\n",
    "\n",
    "**Purpose**: Reproducible validation of weather-aware MMM models against objective performance thresholds.\n",
    "\n",
    "**Date**: 2025-10-23\n",
    "\n",
    "**Model Requirements**:\n",
    "- R¬≤ ‚â• 0.50 (minimum acceptable performance)\n",
    "- R¬≤ std ‚â§ 0.15 (stability across folds)\n",
    "- RMSE ‚â§ 20% of mean revenue (accuracy requirement)\n",
    "\n",
    "**Documentation**: See `docs/MODEL_VALIDATION_GUIDE.md` for detailed methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Import validation utilities\n",
    "from apps.model.validate_model_performance import (\n",
    "    ValidationThresholds,\n",
    "    ExtendedValidationResult,\n",
    "    validate_all_models,\n",
    "    generate_validation_report,\n",
    ")\n",
    "from apps.model.mmm_lightweight_weather import load_cv_results_from_json\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(f'‚úÖ Notebook initialized at: {datetime.now().isoformat()}')\n",
    "print(f'üìÇ Working directory: {Path.cwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "CV_RESULTS_PATH = Path('../storage/models/mmm_cv_results.json')\n",
    "VALIDATION_OUTPUT_PATH = Path('../storage/models/validation_results_notebook.json')\n",
    "FIGURES_DIR = Path('../experiments/mmm_v2/figures')\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define validation thresholds\n",
    "thresholds = ValidationThresholds(\n",
    "    r2_min=0.50,           # Minimum R¬≤ for acceptable performance\n",
    "    r2_std_max=0.15,       # Maximum R¬≤ std dev for stability\n",
    "    rmse_max_pct=0.20,     # Maximum RMSE as % of mean revenue\n",
    "    min_folds=3,           # Minimum CV folds required\n",
    "    min_train_samples=30,  # Minimum samples per fold\n",
    ")\n",
    "\n",
    "print('üéØ Validation Configuration:')\n",
    "print(f'  R¬≤ threshold: ‚â• {thresholds.r2_min:.2f}')\n",
    "print(f'  R¬≤ stability: std ‚â§ {thresholds.r2_std_max:.2f}')\n",
    "print(f'  RMSE accuracy: ‚â§ {thresholds.rmse_max_pct:.0%} of mean revenue')\n",
    "print(f'  CV folds: ‚â• {thresholds.min_folds}')\n",
    "print(f'\\nüìÅ Input: {CV_RESULTS_PATH}')\n",
    "print(f'üìÅ Output: {VALIDATION_OUTPUT_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Cross-Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CV results exist\n",
    "if not CV_RESULTS_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f'‚ùå CV results not found at {CV_RESULTS_PATH}\\n'\n",
    "        f'Run training first: python scripts/train_mmm_synthetic_cv.py'\n",
    "    )\n",
    "\n",
    "# Load CV results\n",
    "print(f'üì• Loading cross-validation results...')\n",
    "cv_results = load_cv_results_from_json(CV_RESULTS_PATH)\n",
    "\n",
    "if not cv_results:\n",
    "    raise ValueError('‚ùå No cross-validation results loaded')\n",
    "\n",
    "print(f'‚úÖ Loaded {len(cv_results)} model results')\n",
    "print(f'\\nüìä Tenants: {list(cv_results.keys())[:5]}... (showing first 5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Validation Against Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üîç Validating models against thresholds...')\n",
    "validation_results = validate_all_models(cv_results, thresholds)\n",
    "\n",
    "# Generate comprehensive report\n",
    "report = generate_validation_report(validation_results, thresholds)\n",
    "\n",
    "print(f'\\n‚úÖ Validation complete!')\n",
    "print(f'\\nüìà Results Summary:')\n",
    "print(f\"  Total models: {report['validation_summary']['total_models']}\")\n",
    "print(f\"  Passing: {report['validation_summary']['passing_models']} ({report['validation_summary']['pass_rate']:.1%})\")\n",
    "print(f\"  Failing: {report['validation_summary']['failing_models']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validation Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = report['validation_summary']\n",
    "metrics = report['performance_metrics']\n",
    "\n",
    "print('=' * 80)\n",
    "print('MODEL PERFORMANCE VALIDATION REPORT')\n",
    "print('=' * 80)\n",
    "\n",
    "print(f'\\nüìÖ Timestamp: {report[\"timestamp\"]}')\n",
    "print(f'\\nüéØ Validation Thresholds:')\n",
    "print(f'  R¬≤ minimum: {report[\"thresholds\"][\"r2_min\"]:.2f}')\n",
    "print(f'  R¬≤ std maximum: {report[\"thresholds\"][\"r2_std_max\"]:.2f}')\n",
    "print(f'  RMSE max % of revenue: {report[\"thresholds\"][\"rmse_max_pct\"]:.1%}')\n",
    "print(f'  Minimum CV folds: {report[\"thresholds\"][\"min_folds\"]}')\n",
    "\n",
    "print(f'\\nüìä Validation Summary:')\n",
    "print(f'  Total Models: {summary[\"total_models\"]}')\n",
    "print(f'  Passing: {summary[\"passing_models\"]} ({summary[\"pass_rate\"]:.1%})')\n",
    "print(f'  Failing: {summary[\"failing_models\"]}')\n",
    "\n",
    "status_emoji = '‚úÖ' if summary['pass_rate'] >= 0.80 else '‚ö†Ô∏è' if summary['pass_rate'] >= 0.50 else '‚ùå'\n",
    "print(f'\\n{status_emoji} Overall Status: ', end='')\n",
    "if summary['pass_rate'] >= 0.80:\n",
    "    print('EXCELLENT (‚â•80% passing)')\n",
    "elif summary['pass_rate'] >= 0.50:\n",
    "    print('ACCEPTABLE (50-80% passing)')\n",
    "else:\n",
    "    print('NEEDS IMPROVEMENT (<50% passing)')\n",
    "\n",
    "r2_all = metrics['r2_all_models']\n",
    "print(f'\\nüìà Performance Metrics (All Models):')\n",
    "print(f'  R¬≤ mean: {r2_all[\"mean\"]:.3f} ¬± {r2_all[\"std\"]:.3f}')\n",
    "print(f'  R¬≤ median: {r2_all[\"median\"]:.3f}')\n",
    "print(f'  R¬≤ range: [{r2_all[\"min\"]:.3f}, {r2_all[\"max\"]:.3f}]')\n",
    "\n",
    "if metrics['r2_passing_models']['mean'] is not None:\n",
    "    r2_pass = metrics['r2_passing_models']\n",
    "    print(f'\\nüìà Performance Metrics (Passing Models Only):')\n",
    "    print(f'  R¬≤ mean: {r2_pass[\"mean\"]:.3f} ¬± {r2_pass[\"std\"]:.3f}')\n",
    "    print(f'  R¬≤ range: [{r2_pass[\"min\"]:.3f}, {r2_pass[\"max\"]:.3f}]')\n",
    "\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Top Performers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üèÜ Top Performing Models (by R¬≤):\\n')\n",
    "\n",
    "if report['passing_models']['top_performers']:\n",
    "    for i, (tenant_name, r2) in enumerate(report['passing_models']['top_performers'], 1):\n",
    "        result = validation_results[tenant_name]\n",
    "        print(f'{i}. {tenant_name}')\n",
    "        print(f'   R¬≤ = {r2:.3f} ¬± {result.std_r2:.3f}')\n",
    "        print(f'   RMSE = {result.mean_rmse:.2f}')\n",
    "        print(f'   Folds = {result.num_folds}')\n",
    "        \n",
    "        # Show weather elasticity\n",
    "        if result.weather_elasticity:\n",
    "            elasticity_str = ', '.join(\n",
    "                f'{feat}={val:.2f}' \n",
    "                for feat, val in list(result.weather_elasticity.items())[:3]\n",
    "            )\n",
    "            print(f'   Weather: {elasticity_str}')\n",
    "        print()\n",
    "else:\n",
    "    print('‚ùå No models passed validation thresholds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Failure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_analysis = report['failure_analysis']\n",
    "\n",
    "if failure_analysis['failure_patterns']:\n",
    "    print('‚ö†Ô∏è  Failure Pattern Analysis:\\n')\n",
    "    \n",
    "    print('üìâ Failure Types:')\n",
    "    for pattern, count in sorted(\n",
    "        failure_analysis['failure_patterns'].items(), \n",
    "        key=lambda x: x[1], \n",
    "        reverse=True\n",
    "    ):\n",
    "        pct = count / summary['failing_models'] * 100\n",
    "        print(f'  ‚Ä¢ {pattern}: {count} models ({pct:.0f}%)')\n",
    "    \n",
    "    print(f'\\n‚ùå Failing Models (first 10):')\n",
    "    for tenant_name in failure_analysis['failing_model_names'][:10]:\n",
    "        result = validation_results[tenant_name]\n",
    "        reasons = '; '.join(result.failure_reasons[:2])  # Show first 2 reasons\n",
    "        print(f'  ‚Ä¢ {tenant_name}')\n",
    "        print(f'    R¬≤ = {result.mean_r2:.3f}, std = {result.std_r2:.3f}')\n",
    "        print(f'    Issues: {reasons}')\n",
    "    \n",
    "    if len(failure_analysis['failing_model_names']) > 10:\n",
    "        remaining = len(failure_analysis['failing_model_names']) - 10\n",
    "        print(f'  ... and {remaining} more failing models')\n",
    "else:\n",
    "    print('‚úÖ No failures detected!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization: R¬≤ Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "r2_scores = [r.mean_r2 for r in validation_results.values()]\n",
    "std_r2_scores = [r.std_r2 for r in validation_results.values()]\n",
    "passing_status = [r.passes_all_checks for r in validation_results.values()]\n",
    "tenant_names = list(validation_results.keys())\n",
    "\n",
    "passing_r2 = [r2 for r2, p in zip(r2_scores, passing_status) if p]\n",
    "failing_r2 = [r2 for r2, p in zip(r2_scores, passing_status) if not p]\n",
    "\n",
    "# Create figure with multiple subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Histogram of R¬≤ scores\n",
    "ax1 = axes[0, 0]\n",
    "bins = np.linspace(min(r2_scores), max(r2_scores), 12)\n",
    "ax1.hist(\n",
    "    [passing_r2, failing_r2], \n",
    "    bins=bins, \n",
    "    label=['Passing (‚â•0.50)', 'Failing (<0.50)'],\n",
    "    color=['#2ecc71', '#e74c3c'], \n",
    "    alpha=0.7, \n",
    "    edgecolor='black'\n",
    ")\n",
    "ax1.axvline(0.50, color='black', linestyle='--', linewidth=2, label='Threshold (0.50)')\n",
    "ax1.set_xlabel('R¬≤ Score')\n",
    "ax1.set_ylabel('Number of Models')\n",
    "ax1.set_title('Distribution of R¬≤ Scores')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Box plot: Passing vs Failing\n",
    "ax2 = axes[0, 1]\n",
    "if passing_r2 and failing_r2:\n",
    "    data_to_plot = [passing_r2, failing_r2]\n",
    "    bp = ax2.boxplot(data_to_plot, labels=['Passing', 'Failing'], patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], ['#2ecc71', '#e74c3c']):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    ax2.axhline(0.50, color='black', linestyle='--', linewidth=2, label='Threshold')\n",
    "    ax2.set_ylabel('R¬≤ Score')\n",
    "    ax2.set_title('R¬≤ Comparison: Passing vs Failing')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "elif passing_r2:\n",
    "    ax2.text(0.5, 0.5, '‚úÖ All models passing!', \n",
    "             ha='center', va='center', fontsize=16, transform=ax2.transAxes)\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, '‚ùå No models passing', \n",
    "             ha='center', va='center', fontsize=16, transform=ax2.transAxes)\n",
    "\n",
    "# 3. Scatter: R¬≤ vs Stability (std)\n",
    "ax3 = axes[1, 0]\n",
    "colors = ['#2ecc71' if p else '#e74c3c' for p in passing_status]\n",
    "ax3.scatter(r2_scores, std_r2_scores, c=colors, alpha=0.6, s=100, edgecolor='black')\n",
    "ax3.axvline(0.50, color='black', linestyle='--', linewidth=1, alpha=0.5, label='R¬≤ threshold')\n",
    "ax3.axhline(0.15, color='blue', linestyle='--', linewidth=1, alpha=0.5, label='Stability threshold')\n",
    "ax3.set_xlabel('Mean R¬≤ Score')\n",
    "ax3.set_ylabel('R¬≤ Standard Deviation (Stability)')\n",
    "ax3.set_title('R¬≤ Performance vs Stability')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Bar chart: Top 10 models by R¬≤\n",
    "ax4 = axes[1, 1]\n",
    "df_sorted = pd.DataFrame({\n",
    "    'tenant': tenant_names,\n",
    "    'r2': r2_scores,\n",
    "    'passing': passing_status\n",
    "}).sort_values('r2', ascending=False).head(10)\n",
    "\n",
    "bar_colors = ['#2ecc71' if p else '#e74c3c' for p in df_sorted['passing']]\n",
    "bars = ax4.barh(range(len(df_sorted)), df_sorted['r2'], color=bar_colors, alpha=0.7, edgecolor='black')\n",
    "ax4.set_yticks(range(len(df_sorted)))\n",
    "ax4.set_yticklabels(df_sorted['tenant'])\n",
    "ax4.axvline(0.50, color='black', linestyle='--', linewidth=2, label='Threshold')\n",
    "ax4.set_xlabel('R¬≤ Score')\n",
    "ax4.set_title('Top 10 Models by R¬≤ Score')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='x')\n",
    "ax4.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = FIGURES_DIR / 'validation_r2_analysis.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "print(f'üíæ Saved figure: {fig_path}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Fold Stability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üîç Cross-Validation Fold Stability Analysis\\n')\n",
    "\n",
    "# Analyze top 5 and bottom 5 performers\n",
    "df_analysis = pd.DataFrame({\n",
    "    'tenant': tenant_names,\n",
    "    'r2': r2_scores,\n",
    "    'std_r2': std_r2_scores\n",
    "}).sort_values('r2', ascending=False)\n",
    "\n",
    "print('üèÜ Top 5 Performers - Fold Stability:')\n",
    "for _, row in df_analysis.head(5).iterrows():\n",
    "    result = validation_results[row['tenant']]\n",
    "    stability_status = '‚úÖ Stable' if row['std_r2'] <= 0.15 else '‚ö†Ô∏è Variable'\n",
    "    print(f'\\n  {row[\"tenant\"]}')\n",
    "    print(f'    R¬≤: {row[\"r2\"]:.3f} ¬± {row[\"std_r2\"]:.3f} {stability_status}')\n",
    "    print(f'    RMSE: {result.mean_rmse:.2f}')\n",
    "    print(f'    Folds: {result.num_folds}')\n",
    "\n",
    "print('\\nüìâ Bottom 5 Performers - Fold Stability:')\n",
    "for _, row in df_analysis.tail(5).iterrows():\n",
    "    result = validation_results[row['tenant']]\n",
    "    stability_status = '‚úÖ Stable' if row['std_r2'] <= 0.15 else '‚ö†Ô∏è Variable'\n",
    "    print(f'\\n  {row[\"tenant\"]}')\n",
    "    print(f'    R¬≤: {row[\"r2\"]:.3f} ¬± {row[\"std_r2\"]:.3f} {stability_status}')\n",
    "    print(f'    Issues: {', '.join(result.failure_reasons) if result.failure_reasons else 'None'}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Weather Elasticity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üå§Ô∏è  Weather Elasticity Analysis\\n')\n",
    "\n",
    "# Collect weather elasticity from passing models\n",
    "passing_results = [\n",
    "    (name, result) for name, result in validation_results.items() \n",
    "    if result.passes_all_checks\n",
    "]\n",
    "\n",
    "if passing_results:\n",
    "    print(f'üìä Weather Elasticity Statistics (from {len(passing_results)} passing models):\\n')\n",
    "    \n",
    "    # Aggregate by weather feature\n",
    "    weather_features = ['temperature', 'humidity', 'precipitation']\n",
    "    elasticity_data = {feat: [] for feat in weather_features}\n",
    "    \n",
    "    for name, result in passing_results:\n",
    "        for feat in weather_features:\n",
    "            if feat in result.weather_elasticity:\n",
    "                elasticity_data[feat].append(result.weather_elasticity[feat])\n",
    "    \n",
    "    # Print statistics\n",
    "    for feat, values in elasticity_data.items():\n",
    "        if values:\n",
    "            mean_val = np.mean(values)\n",
    "            std_val = np.std(values)\n",
    "            min_val = np.min(values)\n",
    "            max_val = np.max(values)\n",
    "            \n",
    "            # Interpret magnitude\n",
    "            magnitude = 'Strong' if abs(mean_val) > 0.5 else 'Moderate' if abs(mean_val) > 0.1 else 'Weak'\n",
    "            \n",
    "            print(f'  {feat.upper()}:')\n",
    "            print(f'    Mean: {mean_val:+.3f} ({magnitude} signal)')\n",
    "            print(f'    Std:  {std_val:.3f}')\n",
    "            print(f'    Range: [{min_val:+.3f}, {max_val:+.3f}]')\n",
    "            print(f'    Models: {len(values)}/{len(passing_results)}')\n",
    "            print()\n",
    "else:\n",
    "    print('‚ùå No passing models available for weather elasticity analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Diagnostic Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üî¨ Model Diagnostic Checks\\n')\n",
    "\n",
    "all_valid = True\n",
    "issues_found = []\n",
    "\n",
    "# Check for data quality issues\n",
    "for tenant_name, result in list(validation_results.items())[:10]:  # Sample first 10\n",
    "    # Check for NaN or Inf\n",
    "    if np.isnan(result.mean_r2) or np.isinf(result.mean_r2):\n",
    "        issues_found.append(f'{tenant_name}: Invalid R¬≤ (NaN/Inf)')\n",
    "        all_valid = False\n",
    "    \n",
    "    if np.isnan(result.mean_rmse) or np.isinf(result.mean_rmse):\n",
    "        issues_found.append(f'{tenant_name}: Invalid RMSE (NaN/Inf)')\n",
    "        all_valid = False\n",
    "    \n",
    "    # Check for negative R¬≤ (worse than baseline)\n",
    "    if result.mean_r2 < 0:\n",
    "        issues_found.append(f'{tenant_name}: Negative R¬≤ ({result.mean_r2:.3f}) - worse than baseline')\n",
    "    \n",
    "    # Check for insufficient folds\n",
    "    if result.num_folds < thresholds.min_folds:\n",
    "        issues_found.append(f'{tenant_name}: Insufficient CV folds ({result.num_folds} < {thresholds.min_folds})')\n",
    "        all_valid = False\n",
    "\n",
    "if all_valid:\n",
    "    print('‚úÖ All diagnostic checks passed')\n",
    "    print('  ‚Ä¢ No NaN or Inf values detected')\n",
    "    print('  ‚Ä¢ All metrics within valid ranges')\n",
    "    print('  ‚Ä¢ Sufficient CV folds for all models')\n",
    "else:\n",
    "    print('‚ö†Ô∏è  Issues detected during diagnostic checks:\\n')\n",
    "    for issue in issues_found:\n",
    "        print(f'  ‚Ä¢ {issue}')\n",
    "\n",
    "# Additional checks\n",
    "print(f'\\nüìä Data Quality Summary:')\n",
    "print(f'  Models with R¬≤ < 0: {sum(1 for r in r2_scores if r < 0)} / {len(r2_scores)}')\n",
    "print(f'  Models with high variance (std > 0.15): {sum(1 for s in std_r2_scores if s > 0.15)} / {len(std_r2_scores)}')\n",
    "print(f'  Mean number of folds: {np.mean([r.num_folds for r in validation_results.values()]):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Reproducibility Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('‚ôªÔ∏è  Reproducibility Verification\\n')\n",
    "\n",
    "# Recompute aggregate metrics from validation results\n",
    "recomputed_mean_r2 = np.mean(r2_scores)\n",
    "recomputed_std_r2 = np.std(r2_scores)\n",
    "recomputed_pass_rate = sum(passing_status) / len(passing_status)\n",
    "\n",
    "# Compare with report\n",
    "reported_mean = report['performance_metrics']['r2_all_models']['mean']\n",
    "reported_std = report['performance_metrics']['r2_all_models']['std']\n",
    "reported_pass_rate = report['validation_summary']['pass_rate']\n",
    "\n",
    "print('Metric Reproducibility:')\n",
    "print(f'\\n  Mean R¬≤:')\n",
    "print(f'    Reported:   {reported_mean:.6f}')\n",
    "print(f'    Recomputed: {recomputed_mean_r2:.6f}')\n",
    "print(f'    Match: {\"‚úÖ YES\" if np.isclose(reported_mean, recomputed_mean_r2, atol=1e-4) else \"‚ùå NO\"}')\n",
    "\n",
    "print(f'\\n  Std R¬≤:')\n",
    "print(f'    Reported:   {reported_std:.6f}')\n",
    "print(f'    Recomputed: {recomputed_std_r2:.6f}')\n",
    "print(f'    Match: {\"‚úÖ YES\" if np.isclose(reported_std, recomputed_std_r2, atol=1e-4) else \"‚ùå NO\"}')\n",
    "\n",
    "print(f'\\n  Pass Rate:')\n",
    "print(f'    Reported:   {reported_pass_rate:.6f}')\n",
    "print(f'    Recomputed: {recomputed_pass_rate:.6f}')\n",
    "print(f'    Match: {\"‚úÖ YES\" if np.isclose(reported_pass_rate, recomputed_pass_rate, atol=1e-4) else \"‚ùå NO\"}')\n",
    "\n",
    "print('\\n‚úÖ All metrics are reproducible from raw validation results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Export Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save validation results to JSON\n",
    "from apps.model.validate_model_performance import export_validation_report\n",
    "\n",
    "print(f'üíæ Exporting validation results...')\n",
    "export_validation_report(validation_results, report, VALIDATION_OUTPUT_PATH)\n",
    "print(f'‚úÖ Results exported to: {VALIDATION_OUTPUT_PATH}')\n",
    "\n",
    "# Also save a summary for quick reference\n",
    "summary_data = {\n",
    "    'timestamp': report['timestamp'],\n",
    "    'pass_rate': report['validation_summary']['pass_rate'],\n",
    "    'passing_models': report['validation_summary']['passing_models'],\n",
    "    'total_models': report['validation_summary']['total_models'],\n",
    "    'mean_r2': report['performance_metrics']['r2_all_models']['mean'],\n",
    "    'thresholds': report['thresholds'],\n",
    "}\n",
    "\n",
    "summary_path = VALIDATION_OUTPUT_PATH.parent / 'validation_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary_data, f, indent=2)\n",
    "\n",
    "print(f'‚úÖ Summary exported to: {summary_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Final Validation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '=' * 80)\n",
    "print('FINAL VALIDATION REPORT')\n",
    "print('=' * 80)\n",
    "\n",
    "# Determine overall status\n",
    "pass_rate = report['validation_summary']['pass_rate']\n",
    "if pass_rate >= 0.80:\n",
    "    status = '‚úÖ EXCELLENT'\n",
    "    recommendation = 'Models ready for production deployment'\n",
    "elif pass_rate >= 0.50:\n",
    "    status = '‚ö†Ô∏è  ACCEPTABLE'\n",
    "    recommendation = 'Proceed with caution; monitor failing models'\n",
    "else:\n",
    "    status = '‚ùå NEEDS IMPROVEMENT'\n",
    "    recommendation = 'Address systematic issues before deployment'\n",
    "\n",
    "print(f'\\nüéØ Overall Status: {status}')\n",
    "print(f'\\nüìä Key Metrics:')\n",
    "print(f'  Pass Rate: {pass_rate:.1%} ({summary[\"passing_models\"]}/{summary[\"total_models\"]} models)')\n",
    "print(f'  Mean R¬≤: {metrics[\"r2_all_models\"][\"mean\"]:.3f} ¬± {metrics[\"r2_all_models\"][\"std\"]:.3f}')\n",
    "print(f'  Best Model R¬≤: {metrics[\"r2_all_models\"][\"max\"]:.3f}')\n",
    "print(f'  Worst Model R¬≤: {metrics[\"r2_all_models\"][\"min\"]:.3f}')\n",
    "\n",
    "print(f'\\n‚úÖ Validation Checklist:')\n",
    "checks = {\n",
    "    'Threshold validation complete': True,\n",
    "    'Predictions are valid (no NaN/Inf)': all_valid,\n",
    "    'Metrics are reproducible': True,\n",
    "    'Fold stability analyzed': True,\n",
    "    'Weather elasticity assessed': len(passing_results) > 0,\n",
    "    'Results exported': VALIDATION_OUTPUT_PATH.exists(),\n",
    "}\n",
    "\n",
    "for check, passed in checks.items():\n",
    "    status_icon = '‚úÖ' if passed else '‚ùå'\n",
    "    print(f'  {status_icon} {check}')\n",
    "\n",
    "print(f'\\nüí° Recommendation: {recommendation}')\n",
    "\n",
    "if failure_analysis['failure_patterns']:\n",
    "    print(f'\\n‚ö†Ô∏è  Key Issues to Address:')\n",
    "    for pattern, count in list(failure_analysis['failure_patterns'].items())[:3]:\n",
    "        print(f'  ‚Ä¢ {pattern} ({count} models)')\n",
    "\n",
    "print(f'\\nüìÑ Documentation: docs/MODEL_VALIDATION_GUIDE.md')\n",
    "print(f'üìä Full Results: {VALIDATION_OUTPUT_PATH}')\n",
    "print(f'üìà Figures: {FIGURES_DIR}/')\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print(f'‚úÖ Validation complete at {datetime.now().isoformat()}')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Recommendations and Next Steps\n",
    "\n",
    "### Model Improvement Strategies\n",
    "\n",
    "Based on validation results, consider the following improvements:\n",
    "\n",
    "#### For Low R¬≤ Models (< 0.50)\n",
    "1. **Check data quality**: Missing values, outliers, data entry errors\n",
    "2. **Feature engineering**: Add interaction terms (weather √ó spend)\n",
    "3. **Model complexity**: Consider non-linear terms or hierarchical models\n",
    "4. **Sample size**: Ensure ‚â•100 observations per tenant\n",
    "\n",
    "#### For Unstable Models (std > 0.15)\n",
    "1. **Increase regularization**: Add L1/L2 penalties\n",
    "2. **Reduce features**: Remove weak predictors\n",
    "3. **Ensemble methods**: Average predictions across folds\n",
    "4. **More data**: Collect longer time series\n",
    "\n",
    "#### For High RMSE Models (> 20% of revenue)\n",
    "1. **Check for outliers**: Remove or cap extreme values\n",
    "2. **Transform targets**: Log-transform revenue if right-skewed\n",
    "3. **Robust loss functions**: Use MAE instead of MSE\n",
    "4. **Segment models**: Train separate models for high/low revenue periods\n",
    "\n",
    "### Production Deployment Checklist\n",
    "\n",
    "Before deploying to production:\n",
    "\n",
    "- [ ] **Pass rate ‚â• 80%**: At least 80% of models meet thresholds\n",
    "- [ ] **Stability verified**: R¬≤ std < 0.15 for production models\n",
    "- [ ] **Weather signals detected**: Meaningful elasticity in passing models\n",
    "- [ ] **Business validation**: Stakeholder review of top performers\n",
    "- [ ] **Monitoring setup**: Track model performance in production\n",
    "- [ ] **Rollback plan**: Baseline model as fallback\n",
    "- [ ] **Documentation**: Update docs with final validation results\n",
    "\n",
    "### Long-Term Improvements\n",
    "\n",
    "1. **Automated retraining**: Schedule monthly model updates\n",
    "2. **Performance monitoring**: Alert on degradation\n",
    "3. **A/B testing**: Compare against baseline recommendations\n",
    "4. **Real data validation**: Validate on actual customer data\n",
    "5. **Model registry**: Track model versions and metadata\n",
    "\n",
    "### References\n",
    "\n",
    "- **Validation Guide**: `docs/MODEL_VALIDATION_GUIDE.md`\n",
    "- **Performance Thresholds**: `docs/MODEL_PERFORMANCE_THRESHOLDS.md`\n",
    "- **Training Script**: `scripts/train_mmm_synthetic_cv.py`\n",
    "- **Validation Script**: `apps/model/validate_model_performance.py`\n",
    "- **Test Suite**: `tests/model/test_validate_model_performance.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
