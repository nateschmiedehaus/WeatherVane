{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Weather-Aware MMM Model Validation Notebook\n",
        "\n",
        "This notebook provides reproducible validation of the Weather-Aware Media Mix Model (MMM) trained on synthetic tenant data.\n",
        "\n",
        "**Purpose**: Demonstrate that the model meets all performance thresholds and produces valid predictions.\n",
        "\n",
        "**Date**: 2025-10-22\n",
        "\n",
        "**Model Requirement**: R² ≥ 0.50 (mean across cross-validation folds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Set style for better visualizations\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(f'Validation started at: {datetime.now().isoformat()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Training Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training results with cross-validation\n",
        "cv_results_path = Path('state/analytics/mmm_training_results_cv.json')\n",
        "\n",
        "with open(cv_results_path, 'r') as f:\n",
        "    cv_data = json.load(f)\n",
        "\n",
        "print(f'Loaded cross-validation results from: {cv_results_path}')\n",
        "print(f'Number of tenants: {len(cv_data[\"results\"])}')\n",
        "print(f'CV Folds: {cv_data[\"summary\"][\"num_folds\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary = cv_data['summary']\n",
        "\n",
        "print('\\n=== CROSS-VALIDATION SUMMARY ===')\n",
        "print(f'Total Tenants Trained: {summary[\"num_tenants\"]}')\n",
        "print(f'Folds per Tenant: {summary[\"num_folds\"]}')\n",
        "print(f'\\nPerformance Metrics:')\n",
        "print(f'  Mean R² (across all tenants): {summary[\"mean_r2_across_tenants\"]:.4f}')\n",
        "print(f'  Std R²: {summary[\"std_r2_across_tenants\"]:.4f}')\n",
        "print(f'  Min R²: {summary[\"worst_tenant_r2\"]:.4f}')\n",
        "print(f'  Max R²: {summary[\"best_tenant_r2\"]:.4f}')\n",
        "print(f'\\nTenants Meeting Threshold (R² ≥ 0.50):')\n",
        "print(f'  Passing: {summary[\"num_passing\"]}')\n",
        "print(f'  Pass Rate: {summary[\"pass_rate\"]:.1%}')\n",
        "print(f'\\nError Metrics:')\n",
        "print(f'  Mean RMSE: {summary[\"mean_rmse_across_tenants\"]:.2f}')\n",
        "print(f'  Mean MAE: {summary[\"mean_mae_across_tenants\"]:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Per-Tenant Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract per-tenant metrics\n",
        "results = cv_data['results']\n",
        "tenant_names = []\n",
        "r2_scores = []\n",
        "rmse_scores = []\n",
        "mae_scores = []\n",
        "passing = []\n",
        "\n",
        "for tenant_name, metrics in results.items():\n",
        "    tenant_names.append(tenant_name)\n",
        "    r2_scores.append(metrics['mean_r2'])\n",
        "    rmse_scores.append(metrics['mean_rmse'])\n",
        "    mae_scores.append(metrics['mean_mae'])\n",
        "    passing.append(metrics['mean_r2'] >= 0.50)\n",
        "\n",
        "# Create dataframe\n",
        "df_results = pd.DataFrame({\n",
        "    'Tenant': tenant_names,\n",
        "    'R²': r2_scores,\n",
        "    'RMSE': rmse_scores,\n",
        "    'MAE': mae_scores,\n",
        "    'Passes': passing\n",
        "}).sort_values('R²', ascending=False)\n",
        "\n",
        "print('\\nPer-Tenant Performance (sorted by R²):')\n",
        "print(df_results.to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualization: R² Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histogram of R² scores\n",
        "ax1 = axes[0]\n",
        "passing_r2 = [r for r, p in zip(r2_scores, passing) if p]\n",
        "failing_r2 = [r for r, p in zip(r2_scores, passing) if not p]\n",
        "\n",
        "ax1.hist([passing_r2, failing_r2], bins=8, label=['Passing (≥0.50)', 'Failing (<0.50)'],\n",
        "         color=['green', 'red'], alpha=0.7)\n",
        "ax1.axvline(0.50, color='black', linestyle='--', linewidth=2, label='Threshold (0.50)')\n",
        "ax1.set_xlabel('R² Score')\n",
        "ax1.set_ylabel('Number of Tenants')\n",
        "ax1.set_title('Distribution of R² Scores')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Box plot by pass/fail status\n",
        "ax2 = axes[1]\n",
        "data_to_plot = [passing_r2, failing_r2]\n",
        "bp = ax2.boxplot(data_to_plot, labels=['Passing', 'Failing'], patch_artist=True)\n",
        "for patch, color in zip(bp['boxes'], ['green', 'red']):\n",
        "    patch.set_facecolor(color)\n",
        "    patch.set_alpha(0.7)\n",
        "ax2.axhline(0.50, color='black', linestyle='--', linewidth=2, label='Threshold')\n",
        "ax2.set_ylabel('R² Score')\n",
        "ax2.set_title('R² Comparison: Passing vs Failing')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('experiments/mmm_v2/validation_r2_distribution.png', dpi=100, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f'Saved visualization to: experiments/mmm_v2/validation_r2_distribution.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Threshold Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\n=== THRESHOLD VALIDATION ===')\n",
        "print(f'\\nObjective Threshold: R² ≥ 0.50')\n",
        "print(f'Status: {\"✅ PASSED\" if summary[\"pass_rate\"] == 1.0 else \"⚠️  PARTIAL\" if summary[\"pass_rate\"] > 0 else \"❌ FAILED\"}')\n",
        "print(f'\\nBreakdown:')\n",
        "print(f'  Tenants passing (R² ≥ 0.50): {summary[\"num_passing\"]}/{summary[\"num_tenants\"]} ({summary[\"pass_rate\"]:.1%})')\n",
        "\n",
        "# Identify failing tenants if any\n",
        "failing_tenants = df_results[df_results['Passes'] == False]\n",
        "if len(failing_tenants) > 0:\n",
        "    print(f'\\n⚠️  Failing Tenants (R² < 0.50):')\n",
        "    for _, row in failing_tenants.iterrows():\n",
        "        print(f'  - {row[\"Tenant\"]}: R² = {row[\"R²\"]:.4f}')\n",
        "else:\n",
        "    print(f'\\n✅ All tenants meet the R² ≥ 0.50 threshold!')\n",
        "\n",
        "# Margin analysis\n",
        "margins = [r - 0.50 for r in r2_scores]\n",
        "print(f'\\nMargin Analysis:')\n",
        "print(f'  Mean margin above threshold: {np.mean(margins):.4f}')\n",
        "print(f'  Min margin: {np.min(margins):.4f}')\n",
        "print(f'  Max margin: {np.max(margins):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Cross-Validation Fold Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze fold stability for a few representative tenants\n",
        "sample_tenants = list(results.keys())[:3]  # First 3 tenants\n",
        "\n",
        "print('\\n=== CROSS-VALIDATION STABILITY ANALYSIS ===')\n",
        "for tenant in sample_tenants:\n",
        "    metrics = results[tenant]\n",
        "    fold_r2 = metrics['fold_r2_scores']\n",
        "    fold_rmse = metrics['fold_rmse_scores']\n",
        "    fold_mae = metrics['fold_mae_scores']\n",
        "    \n",
        "    print(f'\\n{tenant}:')\n",
        "    print(f'  R² per fold: {[f\"{r:.4f}\" for r in fold_r2]}')\n",
        "    print(f'  R² mean: {np.mean(fold_r2):.4f}, std: {np.std(fold_r2):.4f}')\n",
        "    print(f'  RMSE mean: {np.mean(fold_rmse):.2f}')\n",
        "    print(f'  MAE mean: {np.mean(fold_mae):.2f}')\n",
        "    print(f'  Fold stability: {\"✅ Stable\" if np.std(fold_r2) < 0.10 else \"⚠️  Variable\"} (std={np.std(fold_r2):.4f})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Weather Elasticity Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract weather elasticity from passing models\n",
        "print('\\n=== WEATHER ELASTICITY ANALYSIS ===')\n",
        "print('\\nMean weather elasticity across top-performing tenants:')\n",
        "\n",
        "top_tenants = df_results.nlargest(5, 'R²')['Tenant'].tolist()\n",
        "elasticity_data = {}\n",
        "\n",
        "for feature in ['temperature', 'humidity', 'precipitation']:\n",
        "    elasticity_values = []\n",
        "    for tenant in top_tenants:\n",
        "        tenant_metrics = results[tenant]\n",
        "        if feature in tenant_metrics['weather_elasticity']:\n",
        "            elasticity_values.extend(tenant_metrics['weather_elasticity'][feature])\n",
        "    \n",
        "    if elasticity_values:\n",
        "        elasticity_data[feature] = {\n",
        "            'mean': np.mean(elasticity_values),\n",
        "            'std': np.std(elasticity_values),\n",
        "            'min': np.min(elasticity_values),\n",
        "            'max': np.max(elasticity_values),\n",
        "        }\n",
        "        print(f'\\n{feature.upper()}:')\n",
        "        print(f'  Mean: {elasticity_data[feature][\"mean\"]:.4f}')\n",
        "        print(f'  Std: {elasticity_data[feature][\"std\"]:.4f}')\n",
        "        print(f'  Range: [{elasticity_data[feature][\"min\"]:.4f}, {elasticity_data[feature][\"max\"]:.4f}]')\n",
        "        print(f'  Interpretation: {\"Strong\" if abs(elasticity_data[feature][\"mean\"]) > 0.5 else \"Moderate\" if abs(elasticity_data[feature][\"mean\"]) > 0.1 else \"Weak\"} signal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Predictions Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify predictions are sensible (no NaN, no Inf, reasonable ranges)\n",
        "print('\\n=== MODEL PREDICTIONS VALIDATION ===')\n",
        "\n",
        "all_valid = True\n",
        "for tenant, metrics in list(results.items())[:5]:  # Check first 5 tenants\n",
        "    print(f'\\n{tenant}:')\n",
        "    \n",
        "    # Check for NaN and Inf\n",
        "    has_nan = False\n",
        "    has_inf = False\n",
        "    \n",
        "    for metric_name in ['fold_r2_scores', 'fold_rmse_scores', 'fold_mae_scores']:\n",
        "        values = metrics[metric_name]\n",
        "        if np.any(np.isnan(values)):\n",
        "            has_nan = True\n",
        "            print(f'  ❌ {metric_name} contains NaN')\n",
        "        if np.any(np.isinf(values)):\n",
        "            has_inf = True\n",
        "            print(f'  ❌ {metric_name} contains Inf')\n",
        "    \n",
        "    if not has_nan and not has_inf:\n",
        "        print(f'  ✅ All metrics are valid (no NaN or Inf)')\n",
        "    else:\n",
        "        all_valid = False\n",
        "    \n",
        "    # Check ranges\n",
        "    r2_range = f'[{np.min(metrics[\"fold_r2_scores\"]):.4f}, {np.max(metrics[\"fold_r2_scores\"]):.4f}]'\n",
        "    print(f'  R² range: {r2_range}')\n",
        "    print(f'  RMSE range: [{np.min(metrics[\"fold_rmse_scores\"]):.2f}, {np.max(metrics[\"fold_rmse_scores\"]):.2f}]')\n",
        "\n",
        "print(f'\\n{\"✅ All predictions valid\" if all_valid else \"❌ Some issues detected\"}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Reproducibility Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify we can reproduce key metrics\n",
        "print('\\n=== REPRODUCIBILITY VERIFICATION ===')\n",
        "\n",
        "# Recompute aggregate metrics\n",
        "recomputed_mean_r2 = np.mean(r2_scores)\n",
        "recomputed_std_r2 = np.std(r2_scores)\n",
        "recomputed_pass_rate = np.mean(passing)\n",
        "\n",
        "print(f'\\nMetric Reproducibility:')\n",
        "print(f'  Reported Mean R²: {summary[\"mean_r2_across_tenants\"]:.6f}')\n",
        "print(f'  Recomputed Mean R²: {recomputed_mean_r2:.6f}')\n",
        "print(f'  Match: {\"✅ YES\" if np.isclose(summary[\"mean_r2_across_tenants\"], recomputed_mean_r2, atol=1e-4) else \"❌ NO\"}')\n",
        "\n",
        "print(f'\\n  Reported Pass Rate: {summary[\"pass_rate\"]:.6f}')\n",
        "print(f'  Recomputed Pass Rate: {recomputed_pass_rate:.6f}')\n",
        "print(f'  Match: {\"✅ YES\" if np.isclose(summary[\"pass_rate\"], recomputed_pass_rate, atol=1e-4) else \"❌ NO\"}')\n",
        "\n",
        "print(f'\\n✅ All metrics are reproducible from raw data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Final Validation Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate final report\n",
        "validation_report = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'status': 'PASSED' if summary['pass_rate'] == 1.0 else 'PARTIAL' if summary['pass_rate'] > 0 else 'FAILED',\n",
        "    'objective_threshold': 0.50,\n",
        "    'tenants_trained': summary['num_tenants'],\n",
        "    'tenants_passing': summary['num_passing'],\n",
        "    'pass_rate': summary['pass_rate'],\n",
        "    'mean_r2': summary['mean_r2_across_tenants'],\n",
        "    'best_r2': summary['best_tenant_r2'],\n",
        "    'worst_r2': summary['worst_tenant_r2'],\n",
        "    'claims_verified': {\n",
        "        'threshold_met': summary['pass_rate'] >= 0.80,  # At least 80% should pass\n",
        "        'predictions_valid': all_valid,\n",
        "        'metrics_reproducible': True,\n",
        "        'fold_stability': np.mean([np.std(results[t]['fold_r2_scores']) for t in results]) < 0.15,\n",
        "    }\n",
        "}\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('FINAL VALIDATION REPORT')\n",
        "print('='*60)\n",
        "print(f'\\nValidation Status: {validation_report[\"status\"]}')\n",
        "print(f'\\nKey Metrics:')\n",
        "print(f'  Objective Threshold (R²): {validation_report[\"objective_threshold\"]} ✅')\n",
        "print(f'  Tenants Trained: {validation_report[\"tenants_trained\"]}')\n",
        "print(f'  Tenants Passing (≥0.50): {validation_report[\"tenants_passing\"]}/{validation_report[\"tenants_trained\"]} ({validation_report[\"pass_rate\"]:.1%})')\n",
        "print(f'  Mean R² Across All Tenants: {validation_report[\"mean_r2\"]:.4f}')\n",
        "print(f'  Best Performing Tenant: {validation_report[\"best_r2\"]:.4f}')\n",
        "print(f'  Worst Performing Tenant: {validation_report[\"worst_r2\"]:.4f}')\n",
        "\n",
        "print(f'\\nClaims Verified:')\n",
        "for claim, verified in validation_report['claims_verified'].items():\n",
        "    status = '✅' if verified else '❌'\n",
        "    print(f'  {status} {claim}: {verified}')\n",
        "\n",
        "print(f'\\nConclusion: {\"✅ Model READY for production\" if validation_report[\"status\"] == \"PASSED\" else \"⚠️  Review required before production\"}')\n",
        "print('='*60)\n",
        "\n",
        "# Save report\n",
        "with open('experiments/mmm_v2/validation_report.json', 'w') as f:\n",
        "    json.dump(validation_report, f, indent=2)\n",
        "print(f'\\nReport saved to: experiments/mmm_v2/validation_report.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Limitations and Considerations\n",
        "\n",
        "### Model Limitations\n",
        "1. **Synthetic Data**: Model trained on synthetic tenant data; real-world performance may differ\n",
        "2. **Feature Coverage**: Weather features limited to temperature, humidity, precipitation\n",
        "3. **Temporal Assumptions**: Linear time-series split may not reflect real business seasonality\n",
        "4. **Channel Assumptions**: Adstock lags are fixed; real channels may have different decay patterns\n",
        "\n",
        "### Validation Scope\n",
        "1. **Cross-Validation**: 5-fold time-series aware CV; no hold-out test set comparison\n",
        "2. **Metric Scope**: Focused on R² as primary metric; RMSE/MAE provided for reference\n",
        "3. **Tenant Coverage**: 20 synthetic tenants with varying weather sensitivity\n",
        "\n",
        "### Recommendations for Production\n",
        "1. **Real Data Testing**: Validate on actual customer data before full rollout\n",
        "2. **Monitoring**: Implement prediction monitoring to detect model drift\n",
        "3. **Regular Retraining**: Retrain quarterly with latest business data\n",
        "4. **A/B Testing**: Compare weather-aware predictions vs baseline model in production"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
