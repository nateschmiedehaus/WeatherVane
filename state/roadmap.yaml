epics:
  - description: >
      CRITICAL PRIORITY: Comprehensive quality audit of ALL work completed
      before quality gates.


      Assumption: ALL "done" tasks have quality issues until proven otherwise.


      This epic contains remediation tasks for every major system.
    domain: product
    id: E-REMEDIATION
    priority: critical
    status: in_progress
    title: "[CRITICAL] Quality Remediation - Audit All Completed Work"
    milestones:
      - id: M-REM-1
        priority: critical
        status: in_progress
        tasks:
          - dependencies: []
            description: >
              CRITICAL REMEDIATION: ALL MCP server code completed without
              quality gates.


              **Assumption**: Failures exist in all completed work.


              **Scope**: Audit entire tools/wvo_mcp/src/ directory

              - Orchestrator implementations

              - Model routing

              - State management

              - Telemetry systems

              - Resource management

              - Error handling


              **Required Actions**:

              1. Run quality gate adversarial detector on ALL modules

              2. Verify ALL code has tests (target: 80%+ coverage)

              3. Run build: npm run build (must pass with 0 errors)

              4. Run tests: npm test (must pass 100%)

              5. Run npm audit (must show 0 vulnerabilities)

              6. Verify runtime: actually RUN each system end-to-end

              7. Check for superficial completion (empty metrics, unused
              infrastructure)

              8. Check for documentation lies (claimed features that don't
              exist)

              9. Fix ALL issues found

              10. Provide evidence for ALL checks
            domain: product
            exit_criteria:
              - Build passes with 0 errors
              - ALL tests pass (currently 865 tests)
              - npm audit shows 0 vulnerabilities
              - Quality gate adversarial detector APPROVED
              - Runtime evidence provided for each major system
              - No superficial completion detected
              - No documentation-code mismatches
              - Decision log shows APPROVED status
            id: REMEDIATION-ALL-MCP-SERVER
            priority: critical
            status: pending
            title: "[CRITICAL] Audit ALL MCP server implementation for quality issues"
          - dependencies: []
            description: >
              CRITICAL REMEDIATION: Test infrastructure built without quality
              gates.


              **Assumption**: Tests may be superficial or passing without
              actually testing.


              **Audit Focus**:

              - Test quality (not just passing, but meaningful)

              - Coverage of all 7 dimensions

              - Integration test existence

              - End-to-end test existence

              - Mock/stub usage (are we hiding real problems?)

              - Test expectations (were they weakened to pass?)


              **Actions**:

              1. Run scripts/validate_test_quality.sh on ALL test files

              2. Check for unconditional success mocks

              3. Verify tests cover edge cases, not just happy path

              4. Ensure integration tests exist for key flows

              5. Run tests: npm test (currently 856/865 passing = 9 failures!)

              6. Fix ALL test failures

              7. Verify no tests were weakened to pass
            domain: product
            exit_criteria:
              - npm test shows 865/865 passing (100%)
              - Test quality validation passes on ALL test files
              - No unconditional success mocks detected
              - Integration tests exist and pass
              - Edge case coverage verified
            id: REMEDIATION-ALL-TESTING-INFRASTRUCTURE
            priority: critical
            status: pending
            title: "[CRITICAL] Verify testing infrastructure actually works"
          - dependencies: []
            description: >
              IRONIC REMEDIATION: We built quality gates but didn't run them on
              themselves.


              **Self-Audit**:

              - quality_gate_orchestrator.ts (500+ lines)

              - adversarial_bullshit_detector.ts (600+ lines)

              - Integration with unified_orchestrator.ts


              **Questions**:

              1. Do quality gates have 100% test coverage?

              2. Have quality gates been run end-to-end in production?

              3. Does the decision log show REAL decisions from REAL autopilot
              runs?

              4. Are quality gates actually catching issues or just passing
              everything?

              5. Is post-task verification actually running? (I only see
              pre-task in logs)


              **Actions**:

              1. Verify quality_gate_orchestrator.test.ts covers ALL code paths

              2. Verify adversarial_bullshit_detector.test.ts is comprehensive

              3. Check decision log from REAL autopilot run (not just demo)

              4. Verify post-task verification is actually being called

              5. Test with KNOWN bad code to ensure gates reject it
            domain: product
            exit_criteria:
              - Tests: 36/36 passing (unit + integration)
              - Decision log shows decisions from REAL tasks (not demos)
              - Post-task verification confirmed in autopilot logs
              - Test with bad code: gates correctly REJECT
              - Test with good code: gates correctly APPROVE
            id: REMEDIATION-ALL-QUALITY-GATES-DOGFOOD
            priority: critical
            status: pending
            title: "[CRITICAL] Run quality gates on quality gate system itself"
          - dependencies: []
            description: >-
              REMEDIATION: Task T2.2.1 was marked "done" but implementation is
              missing.


              **Audit Finding**: Documentation exists but NO code found.

              - Expected: apps/modeling/train_weather_gam.py or
              apps/modeling/weather_gam.py

              - Found: Nothing

              - Documentation references features that don't exist


              **Required Work**:

              1. Implement weather-aware GAM baseline training script

              2. Integrate with existing feature pipeline

              3. Produce model artifacts matching documentation claims

              4. Add tests covering training, prediction, and validation

              5. Run end-to-end and provide runtime evidence


              **Verification Requirements**:

              - Build passes

              - Tests pass

              - Script actually runs: python apps/modeling/train_weather_gam.py
              --dry-run

              - Produces expected outputs

              - Documentation matches implementation


              **Severity**: HIGH - Claimed feature completely missing

              **Priority**: CRITICAL - Must fix before claiming task complete
            domain: product
            exit_criteria:
              - Training script apps/modeling/train_weather_gam.py exists and
                runs
              - Script produces model artifacts in expected location
              - Documentation in WEATHER_PROOF_OF_CONCEPT.md references actual
                implementation
              - "Runtime evidence: screenshot of training run with metrics"
              - Tests exist and pass for GAM training pipeline
            id: REMEDIATION-T2.2.1-GAM-BASELINE
            priority: high
            status: pending
            title: "[URGENT] Implement missing Weather-aware GAM baseline"
          - dependencies: []
            description: >-
              REMEDIATION: Task T6.3.1 was marked "done" but system is EMPTY.


              **Audit Finding**: Infrastructure exists but unused (0 metrics
              recorded).

              - Found: state/analytics/orchestration_metrics.json (empty: 0
              decisions)

              - Found: docs/MODEL_PERFORMANCE_THRESHOLDS.md (generic thresholds,
              no real data)

              - Problem: System built but never actually used


              **Required Work**:

              1. Actually USE the performance monitoring system

              2. Collect real performance data from autopilot runs

              3. Measure MCP overhead vs direct calls

              4. Measure checkpoint sizes over time

              5. Track token efficiency metrics

              6. Update docs with ACTUAL measured data


              **Verification Requirements**:

              - Run autopilot for at least 10 iterations

              - Verify metrics file grows (check file size before/after)

              - Extract sample metrics: jq '.decisions | length'
              state/analytics/orchestration_metrics.json

              - Document shows real numbers from real runs


              **Severity**: MEDIUM - Feature exists but superficially completed

              **Priority**: HIGH - Must demonstrate system actually works
            domain: product
            exit_criteria:
              - state/analytics/orchestration_metrics.json contains >0 decision
                entries
              - Performance benchmarks exist in docs with REAL data
              - MCP overhead measured and documented
              - Checkpoint size limits validated
              - Token usage tracked over time
              - "Runtime evidence: metrics collection in action"
            id: REMEDIATION-T6.3.1-PERF-BENCHMARKING
            priority: high
            status: pending
            title: "[URGENT] Fix empty performance benchmarking system"
          - dependencies: []
            description: >-
              REMEDIATION: Task T1.1.2 was marked "done" but WRONG framework
              used.


              **Audit Finding**: Code exists but doesn't use Prefect.

              - Found: shared/libs/ingestion/ contains code

              - Problem: No @flow or @task decorators found

              - Problem: Not using Prefect framework despite task requirement


              **Required Work**:

              1. Convert ingestion pipeline to use Prefect decorators

              2. Define flow with @flow decorator

              3. Define tasks with @task decorator

              4. Integrate with Prefect state/checkpoint system

              5. Test flow registration and execution

              6. Document Prefect-specific features used


              **Verification Requirements**:

              - grep -r "@flow|@task" shared/libs/ingestion/ returns matches

              - Can run: prefect deployment build (or equivalent)

              - Flow appears in Prefect UI (screenshot required)

              - Tests cover Prefect integration


              **Severity**: HIGH - Wrong technology implementation

              **Priority**: HIGH - Must use correct framework
            domain: product
            exit_criteria:
              - Code uses @flow and @task decorators from Prefect
              - Flow can be registered with Prefect server
              - Flow execution produces Prefect UI artifacts
              - Checkpointing uses Prefect state management
              - "Runtime evidence: Prefect UI screenshot showing flow run"
              - Tests validate Prefect integration
            id: REMEDIATION-T1.1.2-PREFECT-FLOW
            priority: high
            status: pending
            title: "[URGENT] Convert ingestion to actual Prefect flow"
          - dependencies: []
            description: >-
              Critic academicrigor is underperforming and needs immediate
              remediation.


              Identity: Academic Rigor (academic_rigor, authority advisory)

              Mission: Safeguard academic_rigor discipline.

              Signature powers: Reports on findings when configuration is
              missing.


              Critic academicrigor failed 10 of the last 11 runs with 0
              consecutive failures.


              Observation window: 11 runs


              Consecutive failures: 0


              Failures: 10 | Successes: 1


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              {
                "epic": "E12",
                "summary": {
                  "done": 15,
                  "in_progress": 0,
                  "pending": 0,
                  "blocked": 0
                },
                "critical_issues": [],
                "high_issues": [],
                "warnings": [],
                "doc_path": "/Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/docs/CRITIQUE_E12_20251022.md"
              }
            domain: product
            exit_criteria: []
            id: CRIT-PERF-ACADEMICRIGOR-b0301a
            status: done
            title: "[Critic:academicrigor] Restore performance"
          - dependencies: []
            description: >-
              Critic academicrigor is underperforming and needs immediate
              remediation.


              Identity: Academic Rigor (academic_rigor, authority advisory)

              Mission: Safeguard academic_rigor discipline.

              Signature powers: Reports on findings when configuration is
              missing.


              Critic academicrigor failed 8 of the last 10 runs with 0
              consecutive failures.


              Observation window: 10 runs


              Consecutive failures: 0


              Failures: 8 | Successes: 2


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              {
                "epic": "E12",
                "summary": {
                  "done": 15,
                  "in_progress": 0,
                  "pending": 0,
                  "blocked": 0
                },
                "critical_issues": [],
                "high_issues": [],
                "warnings": [],
                "doc_path": "/Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/docs/CRITIQUE_E12_20251022.md"
              }
            domain: product
            exit_criteria: []
            id: CRIT-PERF-ACADEMICRIGOR-f89932
            status: done
            title: "[Critic:academicrigor] Restore performance"
          - dependencies: []
            description: >-
              Critic allocator is underperforming and needs immediate
              remediation.


              Identity: Allocator Sentinel (operations, authority advisory)

              Mission: Ensure planner allocation and task routing stay optimal.

              Signature powers: Diagnoses misrouted tasks and capacity
              imbalances.; Suggests rebalancing across agents and squads.

              Autonomy guidance: Auto-adjust planner weights when safe; escalate
              persistent misallocations to Autopilot.


              Critic allocator failed 8 of the last 10 runs with 0 consecutive
              failures.


              Observation window: 10 runs


              Consecutive failures: 0


              Failures: 8 | Successes: 2


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              ============================= test session starts
              ==============================

              platform darwin -- Python 3.10.12, pytest-8.4.2, pluggy-1.6.0

              rootdir:
              /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane

              configfile: pyproject.toml

              plugins: anyio-3.7.1, asyncio-1.2.0

              asyncio: mode=strict, debug=False,
              asyncio_default_fixture_loop_scope=None,
              asyncio_default_test_loop_scope=function

              collected 5 items


              tests/test_allocator_routes.py
              ..                                        [ 40%]

              tests/test_creative_route.py
              .                                           [ 60%]

              tests/apps/model/test_cre...
            domain: product
            exit_criteria: []
            id: CRIT-PERF-ALLOCATOR-bc8604
            status: done
            title: "[Critic:allocator] Restore performance"
          - dependencies: []
            description: >-
              Critic build is underperforming and needs immediate remediation.


              Identity: Build Sentinel (engineering, authority blocking)

              Mission: Guarantee that core build processes remain reproducible
              and optimized across environments.

              Signature powers: Diagnoses build pipeline regressions and
              unstable toolchains.; Flags missing build artifacts or
              misconfigured dependencies before release.

              Autonomy guidance: Attempt automated patching of build scripts
              when safe; escalate infrastructure escalations beyond local fixes.


              Critic build failed 5 of the last 6 runs with 0 consecutive
              failures.


              Observation window: 6 runs


              Consecutive failures: 0


              Failures: 5 | Successes: 1


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              warning: The top-level linter settings are deprecated in favour of
              their counterparts in the `lint` section. Please update the
              following options in `pyproject.toml`:
                - 'select' -> 'lint.select'
            domain: product
            exit_criteria: []
            id: CRIT-PERF-BUILD-958e1f
            status: done
            title: "[Critic:build] Restore performance"
          - dependencies: []
            description: >-
              Critic causal is underperforming and needs immediate remediation.


              Identity: Causal Strategist (ml, authority critical)

              Mission: Guarantee counterfactual validity and causal modeling
              rigor.

              Signature powers: Checks identifying assumptions, invariances, and
              instrumentation.; Constructs mitigation plans for confounding
              risks.

              Autonomy guidance: Partner with Research Orchestrator on complex
              interventions; log learnings for future experiments.


              No successful runs recorded in the last 12 observations; 12
              consecutive failures detected.


              Observation window: 12 runs


              Consecutive failures: 12


              Failures: 12 | Successes: 0


              Assigned to: Director Dana


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              skipped due to capability profile
            domain: product
            exit_criteria: []
            id: CRIT-PERF-CAUSAL-070d3d
            status: done
            title: "[Critic:causal] Restore performance"
          - dependencies: []
            description: >-
              Critic causal is underperforming and needs immediate remediation.


              Identity: Causal Strategist (ml, authority critical)

              Mission: Guarantee counterfactual validity and causal modeling
              rigor.

              Signature powers: Checks identifying assumptions, invariances, and
              instrumentation.; Constructs mitigation plans for confounding
              risks.

              Autonomy guidance: Partner with Research Orchestrator on complex
              interventions; log learnings for future experiments.


              Critic causal failed 8 of the last 10 runs with 0 consecutive
              failures.


              Observation window: 10 runs


              Consecutive failures: 0


              Failures: 8 | Successes: 2


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              {
                "status": "passed",
                "level": "medium",
                "findings": [
                  {
                    "severity": "INFO",
                    "message": "Weather shock estimator present.",
                    "details": null
                  },
                  {
                    "severity": "INFO",
                    "message": "Weather shock tests passed.",
                    "details": "  /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/shared/libs/causal/weather_shock.py:194: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n  (Deprecated in version 0.20.5)\n    pl.count().alias(\"pre_count\"),\n\ntests/shared/libs/causal/test_weather_shock.py::test_weather_shock...
            domain: product
            exit_criteria: []
            id: CRIT-PERF-CAUSAL-e7682e
            status: done
            title: "[Critic:causal] Restore performance"
          - dependencies: []
            description: >-
              Critic designsystem is underperforming and needs immediate
              remediation.


              Identity: Design System (design_system, authority advisory)

              Mission: Safeguard design_system discipline.

              Signature powers: Reports on findings when configuration is
              missing.


              No successful runs recorded in the last 9 observations; 5
              consecutive failures detected.


              Observation window: 9 runs


              Consecutive failures: 5


              Failures: 6 | Successes: 3


              Assigned to: Director Dana


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              ./src/pages/dashboard.tsx

              968:14  Error: Parsing error: ')' expected.


              info  - Need to disable some ESLint rules? Learn more here:
              https://nextjs.org/docs/basic-features/eslint#disabling-rules
            domain: product
            exit_criteria: []
            id: CRIT-PERF-DESIGNSYSTEM-1a886a
            status: done
            title: "[Critic:designsystem] Restore performance"
          - dependencies: []
            description: >-
              Critic execreview is underperforming and needs immediate
              remediation.


              Identity: Exec Review (exec_review, authority advisory)

              Mission: Safeguard exec_review discipline.

              Signature powers: Reports on findings when configuration is
              missing.


              No successful runs recorded in the last 12 observations; 12
              consecutive failures detected.


              Observation window: 12 runs


              Consecutive failures: 12


              Failures: 12 | Successes: 0


              Assigned to: Director Dana


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              skipped due to capability profile
            domain: product
            exit_criteria: []
            id: CRIT-PERF-EXECREVIEW-ef2384
            status: done
            title: "[Critic:execreview] Restore performance"
          - dependencies: []
            description: >-
              Multiple critics are underperforming and require coordinated
              intervention.


              3 critics require director-level intervention after repeated
              failures.


              Affected critics: execreview, integrationfury, managerselfcheck


              Critics evaluated in run: 5


              Reports captured: 3


              Assigned to: Director Dana


              Expectations:

              - Review individual remediation tasks and look for systemic
              issues.

              - Adjust critic configurations, training loops, or staffing mixes.

              - Provide a coordination brief in state/context.md.

              - Close this systemic task once individual critics are back on
              track.
            domain: product
            exit_criteria: []
            id: CRIT-PERF-GLOBAL-9882b7
            status: done
            title: "[Critics] Systemic performance remediation"
          - dependencies: []
            description: >-
              Critic healthcheck is underperforming and needs immediate
              remediation.


              Identity: Health Check (health_check, authority advisory)

              Mission: Safeguard health_check discipline.

              Signature powers: Reports on findings when configuration is
              missing.


              Critic healthcheck failed 4 of the last 5 runs with 0 consecutive
              failures.


              Observation window: 5 runs


              Consecutive failures: 0


              Failures: 4 | Successes: 1


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              warning: The top-level linter settings are deprecated in favour of
              their counterparts in the `lint` section. Please update the
              following options in `pyproject.toml`:
                - 'select' -> 'lint.select'
            domain: product
            exit_criteria: []
            id: CRIT-PERF-HEALTHCHECK-0e6b67
            status: done
            title: "[Critic:healthcheck] Restore performance"
          - dependencies: []
            description: >-
              Critic integrationfury is underperforming and needs immediate
              remediation.


              Identity: Integration Fury (integration_fury, authority advisory)

              Mission: Safeguard integration_fury discipline.

              Signature powers: Reports on findings when configuration is
              missing.


              No successful runs recorded in the last 6 observations; 6
              consecutive failures detected.


              Observation window: 6 runs


              Consecutive failures: 6


              Failures: 6 | Successes: 0


              Assigned to: Director Dana


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              ============================= test session starts
              ==============================

              platform darwin -- Python 3.10.12, pytest-8.4.2, pluggy-1.6.0

              rootdir:
              /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane

              configfile: pyproject.toml

              plugins: anyio-4.11.0, asyncio-1.2.0

              asyncio: mode=strict, debug=False,
              asyncio_default_fixture_loop_scope=None,
              asyncio_default_test_loop_scope=function

              collected 242 items


              tests/api/onboarding/test_progress.py
              ....                               [  1%]

              tests/api/test_ad_push_routes.py
              ....                                    [  3%]

              tests/api/test_dashboa...
            domain: product
            exit_criteria: []
            id: CRIT-PERF-INTEGRATIONFURY-9401af
            status: done
            title: "[Critic:integrationfury] Restore performance"
          - dependencies: []
            description: >-
              Critic managerselfcheck is underperforming and needs immediate
              remediation.


              Identity: Manager Self Check (manager_self_check, authority
              advisory)

              Mission: Safeguard manager_self_check discipline.

              Signature powers: Reports on findings when configuration is
              missing.


              No successful runs recorded in the last 6 observations; 6
              consecutive failures detected.


              Observation window: 6 runs


              Consecutive failures: 6


              Failures: 6 | Successes: 0


              Assigned to: Director Dana


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              Rollback simulation stale
              (simulated_at=2025-10-15T21:05:00+00:00); rerun executor to
              refresh promotion gate.
            domain: product
            exit_criteria: []
            id: CRIT-PERF-MANAGERSELFCHECK-61ab48
            status: done
            title: "[Critic:managerselfcheck] Restore performance"
          - dependencies: []
            description: >-
              Critic orgpm is underperforming and needs immediate remediation.


              Identity: Org Pm (org_pm, authority advisory)

              Mission: Safeguard org_pm discipline.

              Signature powers: Reports on findings when configuration is
              missing.


              Critic orgpm failed 5 of the last 6 runs with 0 consecutive
              failures.


              Observation window: 6 runs


              Consecutive failures: 0


              Failures: 5 | Successes: 1


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              Org PM charter/state checks passed.
            domain: product
            exit_criteria: []
            id: CRIT-PERF-ORGPM-be2140
            status: done
            title: "[Critic:orgpm] Restore performance"
          - dependencies: []
            description: >-
              Critic promptbudget is underperforming and needs immediate
              remediation.


              Identity: Prompt Budget (prompt_budget, authority advisory)

              Mission: Safeguard prompt_budget discipline.

              Signature powers: Reports on findings when configuration is
              missing.


              Critic promptbudget failed 5 of the last 6 runs with 0 consecutive
              failures.


              Observation window: 6 runs


              Consecutive failures: 0


              Failures: 5 | Successes: 1


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              {"level":"warning","message":"Code search index rebuild
              failed","timestamp":"2025-10-16T20:39:47.650Z","error":"The
              database connection is not open"}
            domain: product
            exit_criteria: []
            id: CRIT-PERF-PROMPTBUDGET-2c30f3
            status: done
            title: "[Critic:promptbudget] Restore performance"
          - dependencies: []
            description: >-
              Critic security is underperforming and needs immediate
              remediation.


              Identity: Security Sentinel (security, authority critical)

              Mission: Guard secrets, policies, and attack surfaces throughout
              the stack.

              Signature powers: Identifies credential leaks, insecure defaults,
              and policy gaps.; Cross-references security playbooks to recommend
              mitigations.

              Autonomy guidance: Demand sign-off for high-risk findings;
              coordinate with Director Dana and Security Stewards.


              No successful runs recorded in the last 9 observations; 6
              consecutive failures detected.


              Observation window: 9 runs


              Consecutive failures: 6


              Failures: 6 | Successes: 3


              Assigned to: Director Dana


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              skipped due to capability profile
            domain: product
            exit_criteria: []
            id: CRIT-PERF-SECURITY-645edd
            status: done
            title: "[Critic:security] Restore performance"
          - dependencies: []
            description: "Critic tests is underperforming and needs immediate remediation.


              Identity: Regression Hunter (quality, authority blocking)

              Mission: Keep the test suites healthy and ensure deterministic
              results across flows.

              Signature powers: Surfaces flaky suites and failing assertions
              with reproduction notes.; Synthesizes minimal repro commands for
              Autopilot triage.

              Autonomy guidance: Rerun targeted suites automatically; lean on
              Autopilot only when new failures persist.


              Critic tests failed 5 of the last 6 runs with 0 consecutive
              failures.


              Observation window: 6 runs


              Consecutive failures: 0


              Failures: 5 | Successes: 1


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              \e[33mThe CJS build of Vite's Node API is deprecated. See
              https://vite.dev/guide/troubleshooting.html#vite-cjs-node-api-dep\
              recated for more details.\e[39m

              {\"level\":\"info\",\"message\":\"Subscription limit tracker
              initialized\",\"timestamp\":\"2025-10-16T21:37:34.835Z\",\"provid\
              ers\":[]}

              {\"level\":\"info\",\"message\":\"Provider registered for usage
              tracking\",\"timestamp\":\"2025-10-16T21:37:34.837Z\",\"provider\
              \":\"claude\",\"account\":\"test-account\",\"tier\":\"pro\"}

              {\"level\":\"info\",\"message\":\"Subscription limit tracker
              stopped\",\"timestamp\":\"2025-10-16T21:37:34.840Z\"}

              {\"level\":\"info\",\"message\":\"Subscription limit tracker ..."
            domain: product
            exit_criteria: []
            id: CRIT-PERF-TESTS-426598
            status: done
            title: "[Critic:tests] Restore performance"
          - dependencies: []
            description: "Critic tests is underperforming and needs immediate remediation.


              Identity: Regression Hunter (quality, authority blocking)

              Mission: Keep the test suites healthy and ensure deterministic
              results across flows.

              Signature powers: Surfaces flaky suites and failing assertions
              with reproduction notes.; Synthesizes minimal repro commands for
              Autopilot triage.

              Autonomy guidance: Rerun targeted suites automatically; lean on
              Autopilot only when new failures persist.


              Critic tests failed 5 of the last 6 runs with 4 consecutive
              failures.


              Observation window: 6 runs


              Consecutive failures: 4


              Failures: 5 | Successes: 1


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              \e[33mThe CJS build of Vite's Node API is deprecated. See
              https://vite.dev/guide/troubleshooting.html#vite-cjs-node-api-dep\
              recated for more details.\e[39m

              ⎯⎯⎯⎯⎯⎯⎯ Failed Tests 6 ⎯⎯⎯⎯⎯⎯⎯


              \ FAIL  ../../tests/web/design_system_acceptance.spec.ts > Design
              system acceptance – Stories page > renders skip navigation, main
              landmark, and story metadata tokens

              AssertionError: expected \"error\" to not be called at all, but
              actually been called 5 times


              Received:\ 


              \  1st error call:


              \    Array [

              \      \"Warning: An update to %s inside a test was not wrapped in
              act(...).

              \   \ 

              \    When testing, code that cau..."
            domain: product
            exit_criteria: []
            id: CRIT-PERF-TESTS-c3fce7
            status: done
            title: "[Critic:tests] Restore performance"
          - dependencies: []
            description: BLOCKING – must complete before other work (disable YAML writes,
              real usage/cost, correlation IDs, coordinator failover)
            domain: product
            exit_criteria: []
            id: PHASE-1-HARDENING
            status: done
            title: "Phase 1: MCP Hardening"
          - dependencies: []
            description: BLOCKING – must complete before other work (compact context
              assembler, snapshot selfcheck)
            domain: product
            exit_criteria: []
            id: PHASE-2-COMPACT
            status: done
            title: "Phase 2: Compact Prompts + Selfchecks"
          - dependencies: []
            description: BLOCKING – must complete before other work (batch queue, stable
              headers, token heuristics)
            domain: product
            exit_criteria: []
            id: PHASE-3-BATCH
            status: done
            title: "Phase 3: Batch Queue & Prompt Headers"
          - dependencies: []
            description: Consolidate API capabilities, credential flows, and compliance
              requirements so E5 automation tasks can launch with allocator and
              security critics satisfied.
            domain: product
            exit_criteria:
              - doc:docs/api/ads_capability_matrix.md
              - doc:docs/security/ads_automation_sop.md
              - artifact:state/artifacts/research/ads_api_compliance.json
            id: TASK-RESEARCH-AD-AUTOMATION
            status: done
            title: Research Meta/Google ads automation constraints
          - dependencies: []
            description: Collect decision workload traces, benchmark quorum cost, and codify
              staffing heuristics so T3.3.x tasks can wire consensus + telemetry
              with real evidence.
            domain: product
            exit_criteria:
              - artifact:state/analytics/consensus_workload.json
              - doc:docs/research/consensus_staffing_playbook.md
              - artifact:state/analytics/orchestration_metrics.json
            id: TASK-RESEARCH-CONSENSUS-BENCHMARKS
            status: done
            title: Research staffing heuristics for consensus engine rollout
          - dependencies: []
            description: Define geocoding coverage thresholds, schema validation rules, and
              incremental dedupe checks so E7 and E12/E13 work inherit trusted
              data.
            domain: product
            exit_criteria:
              - doc:docs/research/data_quality_guardrails.md
              - artifact:state/analytics/data_quality_baselines.json
              - artifact:state/artifacts/research/geocoding_coverage_report.json
            id: TASK-RESEARCH-DATA-GUARDRAILS
            status: done
            title: Research ingestion data-quality guardrails
          - dependencies: []
            description: Investigate academic cache warming strategies and summarize
              findings for orchestration upgrades.
            domain: product
            exit_criteria: []
            id: TASK-RESEARCH-DEMO
            status: done
            title: "Research: evaluate cache warming pattern"
          - dependencies: []
            description: Run moderated sessions across Sarah, Leo, and Priya personas,
              produce evidence-backed acceptance metrics, and update UX briefs
              so T3.4.x implementation unblocks without rework.
            domain: product
            exit_criteria:
              - doc:docs/research/experiments_reports_validation.md
              - artifact:state/artifacts/research/experiments_sessions
              - doc:docs/UX_CRITIQUE.md
            id: TASK-RESEARCH-EXPERIENCE-VALIDATION
            status: done
            title: Research Experiments/Reports validation with core personas
          - dependencies: []
            description: Identify high-impact roadmap areas lacking research coverage and
              propose follow-up research tasks.
            domain: product
            exit_criteria: []
            id: TASK-RESEARCH-SWEEP
            status: done
            title: Research backlog sweep
          - dependencies: []
            description: Execute npm run build in tools/wvo_mcp and verify 0 TypeScript
              errors
            domain: product
            exit_criteria: []
            id: TEST-1
            status: pending
            title: Run build and verify no errors
          - dependencies: []
            description: Execute npm audit and verify 0 vulnerabilities found
            domain: product
            exit_criteria: []
            id: TEST-2
            status: done
            title: Run npm audit and verify no vulnerabilities
          - dependencies: []
            description: Read CLAUDE.md and verify all verification loop documentation is
              present
            domain: product
            exit_criteria: []
            id: TEST-3
            status: done
            title: Review CLAUDE.md for completeness
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T0.1.1


              **Original Task**: Implement geo holdout plumbing

              **Epic**: Phase 0: Measurement & Confidence (E-PHASE0)

              **Milestone**: M0.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Wire apps/validation/incrementality.py into ingestion runs with
              nightly job execution
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T0.1.1
            milestone_id: M-REM-1
            original_epic: E-PHASE0
            original_task_id: T0.1.1
            priority: high
            status: pending
            title: "[REM] Verify: Implement geo holdout plumbing"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T0.1.2


              **Original Task**: Build lift & confidence UI surfaces

              **Epic**: Phase 0: Measurement & Confidence (E-PHASE0)

              **Milestone**: M0.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Plan API surfaces experiment payloads; Plan UI renders
              lift/confidence cards with download
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T0.1.2
            milestone_id: M-REM-1
            original_epic: E-PHASE0
            original_task_id: T0.1.2
            priority: high
            status: pending
            title: "[REM] Verify: Build lift & confidence UI surfaces"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T0.1.3


              **Original Task**: Generate forecast calibration report

              **Epic**: Phase 0: Measurement & Confidence (E-PHASE0)

              **Milestone**: M0.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Quantile calibration metrics with summary published to docs
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T0.1.3
            milestone_id: M-REM-1
            original_epic: E-PHASE0
            original_task_id: T0.1.3
            priority: high
            status: pending
            title: "[REM] Verify: Generate forecast calibration report"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T1.1.1


              **Original Task**: Design Open-Meteo + Shopify connectors and data
              contracts

              **Epic**: Epic 1 — Ingest & Weather Foundations (E1)

              **Milestone**: M1.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Interactive scenario flows with API endpoints for scenario
              snapshots and storybook coverage
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T1.1.1
            milestone_id: M-REM-1
            original_epic: E1
            original_task_id: T1.1.1
            priority: high
            status: pending
            title: "[REM] Verify: Design Open-Meteo + Shopify connectors and data contracts"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T1.1.2


              **Original Task**: Implement ingestion Prefect flow with
              checkpointing

              **Epic**: Epic 1 — Ingest & Weather Foundations (E1)

              **Milestone**: M1.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Map + chart overlays with export service (PPT/CSV)
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T1.1.2
            milestone_id: M-REM-1
            original_epic: E1
            original_task_id: T1.1.2
            priority: high
            status: pending
            title: "[REM] Verify: Implement ingestion Prefect flow with checkpointing"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T1.2.1


              **Original Task**: Blend historical + forecast weather, enforce
              timezone alignment

              **Epic**: Epic 1 — Ingest & Weather Foundations (E1)

              **Milestone**: M1.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T1.2.1
            milestone_id: M-REM-1
            original_epic: E1
            original_task_id: T1.2.1
            priority: high
            status: pending
            title: "[REM] Verify: Blend historical + forecast weather, enforce timezone
              alignm"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T1.2.2


              **Original Task**: Add leakage guardrails to feature builder

              **Epic**: Epic 1 — Ingest & Weather Foundations (E1)

              **Milestone**: M1.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T1.2.2
            milestone_id: M-REM-1
            original_epic: E1
            original_task_id: T1.2.2
            priority: high
            status: pending
            title: "[REM] Verify: Add leakage guardrails to feature builder"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T1.1.3


              **Original Task**: Wire onboarding progress API

              **Epic**: Phase 1: Experience Delivery (E-PHASE1)

              **Milestone**: M1.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Implement GET/POST /onboarding/progress routes with telemetry
              instrumentation
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T1.1.3
            milestone_id: M-REM-1
            original_epic: E-PHASE1
            original_task_id: T1.1.3
            priority: high
            status: pending
            title: "[REM] Verify: Wire onboarding progress API"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T2.1.1


              **Original Task**: Build lag/rolling feature generators with
              deterministic seeds

              **Epic**: Epic 2 — Features & Modeling Baseline (E2)

              **Milestone**: M2.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T2.1.1
            milestone_id: M-REM-1
            original_epic: E2
            original_task_id: T2.1.1
            priority: high
            status: pending
            title: "[REM] Verify: Build lag/rolling feature generators with deterministic
              seed"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T2.2.1


              **Original Task**: Train weather-aware GAM baseline and document
              methodology

              **Epic**: Epic 2 — Features & Modeling Baseline (E2)

              **Milestone**: M2.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T2.2.1
            milestone_id: M-REM-1
            original_epic: E2
            original_task_id: T2.2.1
            priority: high
            status: pending
            title: "[REM] Verify: Train weather-aware GAM baseline and document methodology"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T7.1.1


              **Original Task**: Complete geocoding integration (city->lat/lon,
              cache strategy)

              **Epic**: Data Pipeline Hardening (E7)

              **Milestone**: M7.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T7.1.1
            milestone_id: M-REM-1
            original_epic: E7
            original_task_id: T7.1.1
            priority: high
            status: pending
            title: "[REM] Verify: Complete geocoding integration (city->lat/lon, cache
              strateg"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T7.1.2


              **Original Task**: Weather feature join to model matrix (prevent
              future leakage)

              **Epic**: Data Pipeline Hardening (E7)

              **Milestone**: M7.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T7.1.2
            milestone_id: M-REM-1
            original_epic: E7
            original_task_id: T7.1.2
            priority: high
            status: pending
            title: "[REM] Verify: Weather feature join to model matrix (prevent future
              leakage"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T7.1.3


              **Original Task**: Data contract schema validation (Shopify,
              weather, ads)

              **Epic**: Data Pipeline Hardening (E7)

              **Milestone**: M7.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T7.1.3
            milestone_id: M-REM-1
            original_epic: E7
            original_task_id: T7.1.3
            priority: high
            status: pending
            title: "[REM] Verify: Data contract schema validation (Shopify, weather, ads)"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T7.2.1


              **Original Task**: Incremental ingestion with deduplication &
              checkpointing

              **Epic**: Data Pipeline Hardening (E7)

              **Milestone**: M7.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T7.2.1
            milestone_id: M-REM-1
            original_epic: E7
            original_task_id: T7.2.1
            priority: high
            status: pending
            title: "[REM] Verify: Incremental ingestion with deduplication & checkpointing"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T7.2.2


              **Original Task**: Data quality monitoring & alerting (anomaly
              detection)

              **Epic**: Data Pipeline Hardening (E7)

              **Milestone**: M7.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T7.2.2
            milestone_id: M-REM-1
            original_epic: E7
            original_task_id: T7.2.2
            priority: high
            status: pending
            title: "[REM] Verify: Data quality monitoring & alerting (anomaly detection)"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T3.1.1


              **Original Task**: Implement budget allocator stress tests and
              regret bounds

              **Epic**: Epic 3 — Allocation & UX (E3)

              **Milestone**: M3.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T3.1.1
            milestone_id: M-REM-1
            original_epic: E3
            original_task_id: T3.1.1
            priority: high
            status: pending
            title: "[REM] Verify: Implement budget allocator stress tests and regret bounds"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T3.2.1


              **Original Task**: Run design system critic and ensure
              accessibility coverage

              **Epic**: Epic 3 — Allocation & UX (E3)

              **Milestone**: M3.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T3.2.1
            milestone_id: M-REM-1
            original_epic: E3
            original_task_id: T3.2.1
            priority: high
            status: pending
            title: "[REM] Verify: Run design system critic and ensure accessibility
              coverage"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T3.2.2


              **Original Task**: Elevate dashboard storytelling & UX

              **Epic**: Epic 3 — Allocation & UX (E3)

              **Milestone**: M3.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T3.2.2
            milestone_id: M-REM-1
            original_epic: E3
            original_task_id: T3.2.2
            priority: high
            status: pending
            title: "[REM] Verify: Elevate dashboard storytelling & UX"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T3.3.1


              **Original Task**: Draft multi-agent charter & delegation mesh
              (AutoGen/Swarm patterns)

              **Epic**: Epic 3 — Allocation & UX (E3)

              **Milestone**: M3.3


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T3.3.1
            milestone_id: M-REM-1
            original_epic: E3
            original_task_id: T3.3.1
            priority: high
            status: pending
            title: "[REM] Verify: Draft multi-agent charter & delegation mesh (AutoGen/Swarm
              p"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T3.3.2


              **Original Task**: Implement hierarchical consensus & escalation
              engine

              **Epic**: Epic 3 — Allocation & UX (E3)

              **Milestone**: M3.3


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T3.3.2
            milestone_id: M-REM-1
            original_epic: E3
            original_task_id: T3.3.2
            priority: high
            status: pending
            title: "[REM] Verify: Implement hierarchical consensus & escalation engine"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T3.3.3


              **Original Task**: Build closed-loop simulation harness for
              autonomous teams

              **Epic**: Epic 3 — Allocation & UX (E3)

              **Milestone**: M3.3


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T3.3.3
            milestone_id: M-REM-1
            original_epic: E3
            original_task_id: T3.3.3
            priority: high
            status: pending
            title: "[REM] Verify: Build closed-loop simulation harness for autonomous teams"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T3.3.4


              **Original Task**: Instrument dynamic staffing telemetry &
              learning pipeline

              **Epic**: Epic 3 — Allocation & UX (E3)

              **Milestone**: M3.3


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T3.3.4
            milestone_id: M-REM-1
            original_epic: E3
            original_task_id: T3.3.4
            priority: high
            status: pending
            title: "[REM] Verify: Instrument dynamic staffing telemetry & learning pipeline"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T3.4.1


              **Original Task**: Implement Plan overview page with
              weather-driven insights

              **Epic**: Epic 3 — Allocation & UX (E3)

              **Milestone**: M3.4


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T3.4.1
            milestone_id: M-REM-1
            original_epic: E3
            original_task_id: T3.4.1
            priority: high
            status: pending
            title: "[REM] Verify: Implement Plan overview page with weather-driven insights"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T3.4.2


              **Original Task**: Build WeatherOps dashboard with allocator +
              weather KPIs

              **Epic**: Epic 3 — Allocation & UX (E3)

              **Milestone**: M3.4


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T3.4.2
            milestone_id: M-REM-1
            original_epic: E3
            original_task_id: T3.4.2
            priority: high
            status: pending
            title: "[REM] Verify: Build WeatherOps dashboard with allocator + weather KPIs"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T3.4.3


              **Original Task**: Ship Experiments hub UI for uplift &
              incrementality reviews

              **Epic**: Epic 3 — Allocation & UX (E3)

              **Milestone**: M3.4


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T3.4.3
            milestone_id: M-REM-1
            original_epic: E3
            original_task_id: T3.4.3
            priority: high
            status: pending
            title: "[REM] Verify: Ship Experiments hub UI for uplift & incrementality
              reviews"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T3.4.4


              **Original Task**: Deliver storytelling Reports view with weather
              + spend narratives

              **Epic**: Epic 3 — Allocation & UX (E3)

              **Milestone**: M3.4


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T3.4.4
            milestone_id: M-REM-1
            original_epic: E3
            original_task_id: T3.4.4
            priority: high
            status: pending
            title: "[REM] Verify: Deliver storytelling Reports view with weather + spend
              narra"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T3.4.5


              **Original Task**: Conduct design_system + UX acceptance review
              across implemented pages

              **Epic**: Epic 3 — Allocation & UX (E3)

              **Milestone**: M3.4


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T3.4.5
            milestone_id: M-REM-1
            original_epic: E3
            original_task_id: T3.4.5
            priority: high
            status: pending
            title: "[REM] Verify: Conduct design_system + UX acceptance review across
              implemen"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T3.4.6


              **Original Task**: Rewrite WeatherOps dashboard around
              plain-language decisions

              **Epic**: Epic 3 — Allocation & UX (E3)

              **Milestone**: M3.4


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Requirements:
                - Show operators exactly what WeatherVane changed or recommends changing, with clear “what/why/next” messaging.
                - Remove jargon (“guardrail”, “triage”) in favour of user-facing language (e.g., “Overspend alert”, “Weather action”).
                - Keep first view scannable: one hero recommendation, secondary cards, optional detail drill-down.
              Standards:
                - Copy: conversational, action-oriented, no internal terminology.
                - UX: minimalist layout, responsive to desktop/tablet, accessible (WC
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T3.4.6
            milestone_id: M-REM-1
            original_epic: E3
            original_task_id: T3.4.6
            priority: high
            status: pending
            title: "[REM] Verify: Rewrite WeatherOps dashboard around plain-language
              decisions"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T3.4.7


              **Original Task**: Reimagine Automations change log as a
              trust-first narrative

              **Epic**: Epic 3 — Allocation & UX (E3)

              **Milestone**: M3.4


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Requirements:
                - Explain every autonomous change in plain language (what changed, when, why, impact).
                - Provide explicit approval/rollback affordances for humans and highlight pending reviews.
                - Surface evidence (metrics, weather context, spend forecasts) inline or a click away.
              Standards:
                - Copy: transparent, confidence-building, avoids “audit/guardrail” jargon.
                - UX: timeline or table must prioritise newest changes, support filtering; accessible controls for approvals.
                - Engineering
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T3.4.7
            milestone_id: M-REM-1
            original_epic: E3
            original_task_id: T3.4.7
            priority: high
            status: pending
            title: "[REM] Verify: Reimagine Automations change log as a trust-first
              narrative"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T5.1.1


              **Original Task**: Implement Meta Marketing API client (creative +
              campaign management)

              **Epic**: Ad Platform Execution & Automation (E5)

              **Milestone**: M5.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T5.1.1
            milestone_id: M-REM-1
            original_epic: E5
            original_task_id: T5.1.1
            priority: high
            status: pending
            title: "[REM] Verify: Implement Meta Marketing API client (creative + campaign
              man"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T5.1.2


              **Original Task**: Meta sandbox and dry-run executor with
              credential vaulting

              **Epic**: Ad Platform Execution & Automation (E5)

              **Milestone**: M5.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T5.1.2
            milestone_id: M-REM-1
            original_epic: E5
            original_task_id: T5.1.2
            priority: high
            status: pending
            title: "[REM] Verify: Meta sandbox and dry-run executor with credential
              vaulting"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T5.2.1


              **Original Task**: Google Ads API integration (campaign
              create/update, shared budgets)

              **Epic**: Ad Platform Execution & Automation (E5)

              **Milestone**: M5.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T5.2.1
            milestone_id: M-REM-1
            original_epic: E5
            original_task_id: T5.2.1
            priority: high
            status: pending
            title: "[REM] Verify: Google Ads API integration (campaign create/update, shared
              b"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T5.2.2


              **Original Task**: Budget reconciliation & spend guardrails across
              platforms

              **Epic**: Ad Platform Execution & Automation (E5)

              **Milestone**: M5.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T5.2.2
            milestone_id: M-REM-1
            original_epic: E5
            original_task_id: T5.2.2
            priority: high
            status: pending
            title: "[REM] Verify: Budget reconciliation & spend guardrails across platforms"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T5.3.1


              **Original Task**: Dry-run & diff visualizer for ad pushes
              (pre-flight checks)

              **Epic**: Ad Platform Execution & Automation (E5)

              **Milestone**: M5.3


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T5.3.1
            milestone_id: M-REM-1
            original_epic: E5
            original_task_id: T5.3.1
            priority: high
            status: pending
            title: "[REM] Verify: Dry-run & diff visualizer for ad pushes (pre-flight
              checks)"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T5.3.2


              **Original Task**: Automated rollback + alerting when
              performance/regression detected

              **Epic**: Ad Platform Execution & Automation (E5)

              **Milestone**: M5.3


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T5.3.2
            milestone_id: M-REM-1
            original_epic: E5
            original_task_id: T5.3.2
            priority: high
            status: pending
            title: "[REM] Verify: Automated rollback + alerting when performance/regression
              de"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T11.1.1


              **Original Task**: Implement hardware probe & profile persistence

              **Epic**: Resource-Aware Intelligence & Personalisation (E11)

              **Milestone**: M11.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T11.1.1
            milestone_id: M-REM-1
            original_epic: E11
            original_task_id: T11.1.1
            priority: high
            status: pending
            title: "[REM] Verify: Implement hardware probe & profile persistence"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T11.1.2


              **Original Task**: Adaptive scheduling for heavy tasks

              **Epic**: Resource-Aware Intelligence & Personalisation (E11)

              **Milestone**: M11.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T11.1.2
            milestone_id: M-REM-1
            original_epic: E11
            original_task_id: T11.1.2
            priority: high
            status: pending
            title: "[REM] Verify: Adaptive scheduling for heavy tasks"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T11.2.1


              **Original Task**: Design system elevation (motion, typography,
              theming)

              **Epic**: Resource-Aware Intelligence & Personalisation (E11)

              **Milestone**: M11.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T11.2.1
            milestone_id: M-REM-1
            original_epic: E11
            original_task_id: T11.2.1
            priority: high
            status: pending
            title: "[REM] Verify: Design system elevation (motion, typography, theming)"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T11.2.2


              **Original Task**: Award-level experience audit & remediation

              **Epic**: Resource-Aware Intelligence & Personalisation (E11)

              **Milestone**: M11.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T11.2.2
            milestone_id: M-REM-1
            original_epic: E11
            original_task_id: T11.2.2
            priority: high
            status: pending
            title: "[REM] Verify: Award-level experience audit & remediation"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T11.2.3


              **Original Task**: Extend calm/aero theme tokens to Automations
              and Experiments surfaces

              **Epic**: Resource-Aware Intelligence & Personalisation (E11)

              **Milestone**: M11.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T11.2.3
            milestone_id: M-REM-1
            original_epic: E11
            original_task_id: T11.2.3
            priority: high
            status: pending
            title: "[REM] Verify: Extend calm/aero theme tokens to Automations and
              Experiments"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T11.2.4


              **Original Task**: Refactor landing/marketing gradients into
              reusable tokens

              **Epic**: Resource-Aware Intelligence & Personalisation (E11)

              **Milestone**: M11.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T11.2.4
            milestone_id: M-REM-1
            original_epic: E11
            original_task_id: T11.2.4
            priority: high
            status: pending
            title: "[REM] Verify: Refactor landing/marketing gradients into reusable tokens"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T11.2.5


              **Original Task**: Centralize retry button styles in shared
              component once App Router lands

              **Epic**: Resource-Aware Intelligence & Personalisation (E11)

              **Milestone**: M11.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T11.2.5
            milestone_id: M-REM-1
            original_epic: E11
            original_task_id: T11.2.5
            priority: high
            status: pending
            title: "[REM] Verify: Centralize retry button styles in shared component once
              App "
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T11.2.6


              **Original Task**: Formalize shared panel mixin (border + shadow)
              to reduce overrides

              **Epic**: Resource-Aware Intelligence & Personalisation (E11)

              **Milestone**: M11.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T11.2.6
            milestone_id: M-REM-1
            original_epic: E11
            original_task_id: T11.2.6
            priority: high
            status: pending
            title: "[REM] Verify: Formalize shared panel mixin (border + shadow) to reduce
              ove"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T4.1.10


              **Original Task**: Cross-market saturation optimization
              (fairness-aware)

              **Epic**: Epic 4 — Operational Excellence (E4)

              **Milestone**: M4.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T4.1.10
            milestone_id: M-REM-1
            original_epic: E4
            original_task_id: T4.1.10
            priority: high
            status: pending
            title: "[REM] Verify: Cross-market saturation optimization (fairness-aware)"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T4.1.3


              **Original Task**: Causal uplift modeling & incremental lift
              validation

              **Epic**: Epic 4 — Operational Excellence (E4)

              **Milestone**: M4.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T4.1.3
            milestone_id: M-REM-1
            original_epic: E4
            original_task_id: T4.1.3
            priority: high
            status: pending
            title: "[REM] Verify: Causal uplift modeling & incremental lift validation"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T4.1.4


              **Original Task**: Multi-horizon ensemble forecasting

              **Epic**: Epic 4 — Operational Excellence (E4)

              **Milestone**: M4.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T4.1.4
            milestone_id: M-REM-1
            original_epic: E4
            original_task_id: T4.1.4
            priority: high
            status: pending
            title: "[REM] Verify: Multi-horizon ensemble forecasting"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T4.1.5


              **Original Task**: Non-linear allocation optimizer with
              constraints (ROAS, spend caps)

              **Epic**: Epic 4 — Operational Excellence (E4)

              **Milestone**: M4.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T4.1.5
            milestone_id: M-REM-1
            original_epic: E4
            original_task_id: T4.1.5
            priority: high
            status: pending
            title: "[REM] Verify: Non-linear allocation optimizer with constraints (ROAS,
              spen"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T4.1.6


              **Original Task**: High-frequency spend response modeling
              (intraday adjustments)

              **Epic**: Epic 4 — Operational Excellence (E4)

              **Milestone**: M4.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T4.1.6
            milestone_id: M-REM-1
            original_epic: E4
            original_task_id: T4.1.6
            priority: high
            status: pending
            title: "[REM] Verify: High-frequency spend response modeling (intraday
              adjustments"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T4.1.7


              **Original Task**: Marketing mix budget solver (multi-channel,
              weather-aware)

              **Epic**: Epic 4 — Operational Excellence (E4)

              **Milestone**: M4.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T4.1.7
            milestone_id: M-REM-1
            original_epic: E4
            original_task_id: T4.1.7
            priority: high
            status: pending
            title: "[REM] Verify: Marketing mix budget solver (multi-channel,
              weather-aware)"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T4.1.8


              **Original Task**: Reinforcement-learning shadow mode (safe
              exploration)

              **Epic**: Epic 4 — Operational Excellence (E4)

              **Milestone**: M4.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T4.1.8
            milestone_id: M-REM-1
            original_epic: E4
            original_task_id: T4.1.8
            priority: high
            status: pending
            title: "[REM] Verify: Reinforcement-learning shadow mode (safe exploration)"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T4.1.9


              **Original Task**: Creative-level response modeling with brand
              safety guardrails

              **Epic**: Epic 4 — Operational Excellence (E4)

              **Milestone**: M4.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T4.1.9
            milestone_id: M-REM-1
            original_epic: E4
            original_task_id: T4.1.9
            priority: high
            status: pending
            title: "[REM] Verify: Creative-level response modeling with brand safety
              guardrail"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T6.1.1


              **Original Task**: MCP server integration tests (all 25 tools
              across both providers)

              **Epic**: MCP Orchestrator Production Readiness (E6)

              **Milestone**: M6.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T6.1.1
            milestone_id: M-REM-1
            original_epic: E6
            original_task_id: T6.1.1
            priority: high
            status: pending
            title: "[REM] Verify: MCP server integration tests (all 25 tools across both
              provi"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T6.1.2


              **Original Task**: Provider failover testing (token limit
              simulation & automatic switching)

              **Epic**: MCP Orchestrator Production Readiness (E6)

              **Milestone**: M6.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T6.1.2
            milestone_id: M-REM-1
            original_epic: E6
            original_task_id: T6.1.2
            priority: high
            status: pending
            title: "[REM] Verify: Provider failover testing (token limit simulation &
              automati"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T6.1.3


              **Original Task**: State persistence testing (checkpoint recovery
              across sessions)

              **Epic**: MCP Orchestrator Production Readiness (E6)

              **Milestone**: M6.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T6.1.3
            milestone_id: M-REM-1
            original_epic: E6
            original_task_id: T6.1.3
            priority: high
            status: pending
            title: "[REM] Verify: State persistence testing (checkpoint recovery across
              sessio"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T6.1.4


              **Original Task**: Quality framework validation (10 dimensions
              operational)

              **Epic**: MCP Orchestrator Production Readiness (E6)

              **Milestone**: M6.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T6.1.4
            milestone_id: M-REM-1
            original_epic: E6
            original_task_id: T6.1.4
            priority: high
            status: pending
            title: "[REM] Verify: Quality framework validation (10 dimensions operational)"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T6.2.1


              **Original Task**: Credentials security audit (auth.json, API
              keys, token rotation)

              **Epic**: MCP Orchestrator Production Readiness (E6)

              **Milestone**: M6.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T6.2.1
            milestone_id: M-REM-1
            original_epic: E6
            original_task_id: T6.2.1
            priority: high
            status: pending
            title: "[REM] Verify: Credentials security audit (auth.json, API keys, token
              rotat"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T6.2.2


              **Original Task**: Error recovery testing (graceful degradation,
              retry logic)

              **Epic**: MCP Orchestrator Production Readiness (E6)

              **Milestone**: M6.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T6.2.2
            milestone_id: M-REM-1
            original_epic: E6
            original_task_id: T6.2.2
            priority: high
            status: pending
            title: "[REM] Verify: Error recovery testing (graceful degradation, retry
              logic)"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T6.2.3


              **Original Task**: Schema validation enforcement (all data
              contracts validated)

              **Epic**: MCP Orchestrator Production Readiness (E6)

              **Milestone**: M6.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T6.2.3
            milestone_id: M-REM-1
            original_epic: E6
            original_task_id: T6.2.3
            priority: high
            status: pending
            title: "[REM] Verify: Schema validation enforcement (all data contracts
              validated)"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T6.2.4


              **Original Task**: API rate limiting & exponential backoff
              (Open-Meteo, Shopify, Ads APIs)

              **Epic**: MCP Orchestrator Production Readiness (E6)

              **Milestone**: M6.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T6.2.4
            milestone_id: M-REM-1
            original_epic: E6
            original_task_id: T6.2.4
            priority: high
            status: pending
            title: "[REM] Verify: API rate limiting & exponential backoff (Open-Meteo,
              Shopify"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T6.3.1


              **Original Task**: Performance benchmarking (MCP overhead,
              checkpoint size, token usage)

              **Epic**: MCP Orchestrator Production Readiness (E6)

              **Milestone**: M6.3


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T6.3.1
            milestone_id: M-REM-1
            original_epic: E6
            original_task_id: T6.3.1
            priority: high
            status: pending
            title: "[REM] Verify: Performance benchmarking (MCP overhead, checkpoint size,
              tok"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T6.3.2


              **Original Task**: Enhanced observability export (structured logs,
              metrics dashboards)

              **Epic**: MCP Orchestrator Production Readiness (E6)

              **Milestone**: M6.3


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T6.3.2
            milestone_id: M-REM-1
            original_epic: E6
            original_task_id: T6.3.2
            priority: high
            status: pending
            title: "[REM] Verify: Enhanced observability export (structured logs, metrics
              dash"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T6.3.3


              **Original Task**: Autopilot loop end-to-end testing (full
              autonomous cycle validation)

              **Epic**: MCP Orchestrator Production Readiness (E6)

              **Milestone**: M6.3


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T6.3.3
            milestone_id: M-REM-1
            original_epic: E6
            original_task_id: T6.3.3
            priority: high
            status: pending
            title: "[REM] Verify: Autopilot loop end-to-end testing (full autonomous cycle
              val"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T6.4.0


              **Original Task**: Upgrade invariants & preflight guardrails

              **Epic**: MCP Orchestrator Production Readiness (E6)

              **Milestone**: M6.4


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Define upgrade preflight: clean git, version sanity, ≥500MB disk,
              SQLite lock probe, and single-flight upgrade.lock. Gate promotion
              through build → unit → selfchecks → canary, aborting with
              {error:"upgrade_aborted"} on any failure.
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T6.4.0
            milestone_id: M-REM-1
            original_epic: E6
            original_task_id: T6.4.0
            priority: high
            status: pending
            title: "[REM] Verify: Upgrade invariants & preflight guardrails"
          - dependencies: []
            description: >+
              REMEDIATION: Verify completed task T6.4.1


              **Original Task**: Live feature flag store with kill switch

              **Epic**: MCP Orchestrator Production Readiness (E6)

              **Milestone**: M6.4


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Replace environment toggles with a SQLite-backed `settings` table,
              seed

              defaults, and hot-refresh cached flags (≤500 ms poll). Include a

              `DISABLE_NEW` global kill switch that forces legacy behaviour
              instantly.

            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T6.4.1
            milestone_id: M-REM-1
            original_epic: E6
            original_task_id: T6.4.1
            priority: high
            status: pending
            title: "[REM] Verify: Live feature flag store with kill switch"
          - dependencies: []
            description: >+
              REMEDIATION: Verify completed task T6.4.2


              **Original Task**: Blue/green worker manager & front-end proxy

              **Epic**: MCP Orchestrator Production Readiness (E6)

              **Milestone**: M6.4


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Keep the MCP front-end process stable while managing active and
              canary

              worker children over IPC. Ensure requests route through a proxy
              that can

              atomically switch to the validated canary without disconnecting
              clients.

            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T6.4.2
            milestone_id: M-REM-1
            original_epic: E6
            original_task_id: T6.4.2
            priority: high
            status: pending
            title: "[REM] Verify: Blue/green worker manager & front-end proxy"
          - dependencies: []
            description: >+
              REMEDIATION: Verify completed task T6.4.3


              **Original Task**: Worker entrypoint with DRY_RUN safeguards

              **Epic**: MCP Orchestrator Production Readiness (E6)

              **Milestone**: M6.4


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Implement a dedicated worker entry that routes RPCs, enforces
              DRY_RUN=1

              by opening the state DB read-only, refuses mutating calls, and
              confirms

              legacy behaviour when DRY_RUN=0.

            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T6.4.3
            milestone_id: M-REM-1
            original_epic: E6
            original_task_id: T6.4.3
            priority: high
            status: pending
            title: "[REM] Verify: Worker entrypoint with DRY_RUN safeguards"
          - dependencies: []
            description: >+
              REMEDIATION: Verify completed task T6.4.4


              **Original Task**: Canary upgrade harness & shadow validation

              **Epic**: MCP Orchestrator Production Readiness (E6)

              **Milestone**: M6.4


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Automate the upgrade flow: create a separate git worktree,
              build/test new

              code, spawn a DRY_RUN canary, run shadow health/plan/report
              checks, then

              promote only if outputs match expectations.

            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T6.4.4
            milestone_id: M-REM-1
            original_epic: E6
            original_task_id: T6.4.4
            priority: high
            status: pending
            title: "[REM] Verify: Canary upgrade harness & shadow validation"
          - dependencies: []
            description: >+
              REMEDIATION: Verify completed task T6.4.7


              **Original Task**: Automatic rollback monitors & kill-switch reset

              **Epic**: MCP Orchestrator Production Readiness (E6)

              **Milestone**: M6.4


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Add health monitoring that reverts to the previous worker and
              resets

              flags when error rates spike post-promotion. Document on-call
              rollback

              steps and ensure DISABLE_NEW restores legacy behaviour.

            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T6.4.7
            milestone_id: M-REM-1
            original_epic: E6
            original_task_id: T6.4.7
            priority: high
            status: pending
            title: "[REM] Verify: Automatic rollback monitors & kill-switch reset"
          - dependencies: []
            description: >+
              REMEDIATION: Verify completed task T6.4.8


              **Original Task**: Observability & resource budgets during upgrade

              **Epic**: MCP Orchestrator Production Readiness (E6)

              **Milestone**: M6.4


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Emit OTel spans (or structured JSON logs) for every worker call
              with

              timing, lane, task, and outcome metadata. Enforce concurrency,
              timeout,

              and RSS guards to prevent runaway resource usage.

            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T6.4.8
            milestone_id: M-REM-1
            original_epic: E6
            original_task_id: T6.4.8
            priority: high
            status: pending
            title: "[REM] Verify: Observability & resource budgets during upgrade"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T8.1.1


              **Original Task**: Lock MCP schemas to Zod shapes (SAFE:
              guardrail)

              **Epic**: PHASE-4-POLISH — MCP Production Hardening (E8)

              **Milestone**: M8.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T8.1.1
            milestone_id: M-REM-1
            original_epic: E8
            original_task_id: T8.1.1
            priority: high
            status: pending
            title: "[REM] Verify: Lock MCP schemas to Zod shapes (SAFE: guardrail)"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T8.1.2


              **Original Task**: Implement command allow-list in guardrails
              (SAFE: additive security)

              **Epic**: PHASE-4-POLISH — MCP Production Hardening (E8)

              **Milestone**: M8.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T8.1.2
            milestone_id: M-REM-1
            original_epic: E8
            original_task_id: T8.1.2
            priority: high
            status: pending
            title: "[REM] Verify: Implement command allow-list in guardrails (SAFE: additive
              s"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T8.1.3


              **Original Task**: Thread correlation IDs through state
              transitions (SAFE: observability only)

              **Epic**: PHASE-4-POLISH — MCP Production Hardening (E8)

              **Milestone**: M8.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T8.1.3
            milestone_id: M-REM-1
            original_epic: E8
            original_task_id: T8.1.3
            priority: high
            status: pending
            title: "[REM] Verify: Thread correlation IDs through state transitions (SAFE:
              obse"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T8.2.1


              **Original Task**: Implement compact evidence-pack prompt mode
              (SAFE: new function, backward compatible)

              **Epic**: PHASE-4-POLISH — MCP Production Hardening (E8)

              **Milestone**: M8.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T8.2.1
            milestone_id: M-REM-1
            original_epic: E8
            original_task_id: T8.2.1
            priority: high
            status: pending
            title: "[REM] Verify: Implement compact evidence-pack prompt mode (SAFE: new
              funct"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T8.2.2


              **Original Task**: Finalize Claude↔Codex coordinator failover
              (SAFE: expose existing functionality)

              **Epic**: PHASE-4-POLISH — MCP Production Hardening (E8)

              **Milestone**: M8.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T8.2.2
            milestone_id: M-REM-1
            original_epic: E8
            original_task_id: T8.2.2
            priority: high
            status: pending
            title: "[REM] Verify: Finalize Claude↔Codex coordinator failover (SAFE: expose
              exi"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T9.1.1


              **Original Task**: Stable prompt headers with provider caching
              (SAFE: additive optimization)

              **Epic**: PHASE-5-OPTIMIZATION — Performance & Observability (E9)

              **Milestone**: M9.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T9.1.1
            milestone_id: M-REM-1
            original_epic: E9
            original_task_id: T9.1.1
            priority: high
            status: pending
            title: "[REM] Verify: Stable prompt headers with provider caching (SAFE:
              additive "
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T9.1.2


              **Original Task**: Batch queue for non-urgent prompts (SAFE: new
              queueing system)

              **Epic**: PHASE-5-OPTIMIZATION — Performance & Observability (E9)

              **Milestone**: M9.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T9.1.2
            milestone_id: M-REM-1
            original_epic: E9
            original_task_id: T9.1.2
            priority: high
            status: pending
            title: "[REM] Verify: Batch queue for non-urgent prompts (SAFE: new queueing
              syste"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T9.2.1


              **Original Task**: Strict output DSL validation (SAFE: validation
              layer only)

              **Epic**: PHASE-5-OPTIMIZATION — Performance & Observability (E9)

              **Milestone**: M9.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T9.2.1
            milestone_id: M-REM-1
            original_epic: E9
            original_task_id: T9.2.1
            priority: high
            status: pending
            title: "[REM] Verify: Strict output DSL validation (SAFE: validation layer
              only)"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T9.2.2


              **Original Task**: Idempotency keys for mutating tools (SAFE:
              caching layer)

              **Epic**: PHASE-5-OPTIMIZATION — Performance & Observability (E9)

              **Milestone**: M9.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T9.2.2
            milestone_id: M-REM-1
            original_epic: E9
            original_task_id: T9.2.2
            priority: high
            status: pending
            title: "[REM] Verify: Idempotency keys for mutating tools (SAFE: caching layer)"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T9.3.1


              **Original Task**: OpenTelemetry spans for all operations (SAFE:
              tracing wrapper)

              **Epic**: PHASE-5-OPTIMIZATION — Performance & Observability (E9)

              **Milestone**: M9.3


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T9.3.1
            milestone_id: M-REM-1
            original_epic: E9
            original_task_id: T9.3.1
            priority: high
            status: pending
            title: "[REM] Verify: OpenTelemetry spans for all operations (SAFE: tracing
              wrappe"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T9.4.1


              **Original Task**: SQLite FTS5 index for code search (SAFE: new
              index)

              **Epic**: PHASE-5-OPTIMIZATION — Performance & Observability (E9)

              **Milestone**: M9.4


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T9.4.1
            milestone_id: M-REM-1
            original_epic: E9
            original_task_id: T9.4.1
            priority: high
            status: pending
            title: "[REM] Verify: SQLite FTS5 index for code search (SAFE: new index)"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T10.1.1


              **Original Task**: Cost telemetry and budget alerts

              **Epic**: PHASE-6-COST — Usage-Based Optimisations (E10)

              **Milestone**: M10.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T10.1.1
            milestone_id: M-REM-1
            original_epic: E10
            original_task_id: T10.1.1
            priority: high
            status: pending
            title: "[REM] Verify: Cost telemetry and budget alerts"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T12.0.3


              **Original Task**: Document synthetic tenant characteristics

              **Epic**: Epic 12 — Weather Model Production Validation (E12)

              **Milestone**: M12.0


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Create data dictionary with tenant profiles, weather sensitivity,
              expected model behaviors
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T12.0.3
            milestone_id: M-REM-1
            original_epic: E12
            original_task_id: T12.0.3
            priority: high
            status: pending
            title: "[REM] Verify: Document synthetic tenant characteristics"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T12.1.2


              **Original Task**: Validate feature store joins against historical
              weather baselines

              **Epic**: Epic 12 — Weather Model Production Validation (E12)

              **Milestone**: M12.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T12.1.2
            milestone_id: M-REM-1
            original_epic: E12
            original_task_id: T12.1.2
            priority: high
            status: pending
            title: "[REM] Verify: Validate feature store joins against historical weather
              base"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T12.2.1


              **Original Task**: Backtest weather-aware model vs control across
              top tenants

              **Epic**: Epic 12 — Weather Model Production Validation (E12)

              **Milestone**: M12.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T12.2.1
            milestone_id: M-REM-1
            original_epic: E12
            original_task_id: T12.2.1
            priority: high
            status: pending
            title: "[REM] Verify: Backtest weather-aware model vs control across top
              tenants"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T12.PoC.3


              **Original Task**: Create PoC demo results and proof brief

              **Epic**: Epic 12 — Weather Model Production Validation (E12)

              **Milestone**: M12.PoC


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Create executive brief demonstrating that weather-aware modeling
              works: show before/after model performance, weather elasticity
              coefficients, and prediction examples. Use this to get stakeholder
              buy-in before full MMM training.
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T12.PoC.3
            milestone_id: M-REM-1
            original_epic: E12
            original_task_id: T12.PoC.3
            priority: high
            status: pending
            title: "[REM] Verify: Create PoC demo results and proof brief"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T13.1.1


              **Original Task**: Validate 90-day tenant data coverage across
              sales, spend, and weather

              **Epic**: Epic 13 — Weather-Aware Modeling Reality (E13)

              **Milestone**: M13.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Ensure the Shopify/ads/weather ingestion flows actually populate
              product_daily with geocoded spend and 90+ days of history so MMM
              inputs are real, not theoretical.
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T13.1.1
            milestone_id: M-REM-1
            original_epic: E13
            original_task_id: T13.1.1
            priority: high
            status: pending
            title: "[REM] Verify: Validate 90-day tenant data coverage across sales, spend,
              an"
          - dependencies: []
            description: >+
              REMEDIATION: Verify completed task T13.1.3


              **Original Task**: Implement product taxonomy auto-classification
              with weather affinity

              **Epic**: Epic 13 — Weather-Aware Modeling Reality (E13)

              **Milestone**: M13.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Auto-classify products from Shopify/Meta/Google using LLM
              (Claude/GPT-4) to tag products with weather affinity and category
              hierarchy. This enables product-level modeling (not just
              brand-level) and cold-start for new brands.

            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T13.1.3
            milestone_id: M-REM-1
            original_epic: E13
            original_task_id: T13.1.3
            priority: high
            status: pending
            title: "[REM] Verify: Implement product taxonomy auto-classification with
              weather "
          - dependencies: []
            description: >+
              REMEDIATION: Verify completed task T13.1.4


              **Original Task**: Data quality validation framework (verify data
              fitness for ML)

              **Epic**: Epic 13 — Weather-Aware Modeling Reality (E13)

              **Milestone**: M13.1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Implement data quality checks to verify data is ready for ML
              training. Prevents training models on insufficient/corrupted data.

            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T13.1.4
            milestone_id: M-REM-1
            original_epic: E13
            original_task_id: T13.1.4
            priority: high
            status: pending
            title: "[REM] Verify: Data quality validation framework (verify data fitness for
              M"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T13.2.1


              **Original Task**: Replace heuristic MMM with LightweightMMM
              adstock+saturation fit

              **Epic**: Epic 13 — Weather-Aware Modeling Reality (E13)

              **Milestone**: M13.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Integrate the existing LightweightMMM wrapper so allocations use
              Bayesian adstock/saturation estimates instead of covariance
              heuristics.
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T13.2.1
            milestone_id: M-REM-1
            original_epic: E13
            original_task_id: T13.2.1
            priority: high
            status: pending
            title: "[REM] Verify: Replace heuristic MMM with LightweightMMM
              adstock+saturation"
          - dependencies: []
            description: >+
              REMEDIATION: Verify completed task T13.2.3


              **Original Task**: Replace heuristic allocator with
              constraint-aware optimizer

              **Epic**: Epic 13 — Weather-Aware Modeling Reality (E13)

              **Milestone**: M13.2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Replace heuristic allocation rules (±10% budget adjustments) with
              proper constrained optimization. Use cvxpy or OR-Tools to maximize
              ROAS subject to budget, inventory, and platform constraints.

            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T13.2.3
            milestone_id: M-REM-1
            original_epic: E13
            original_task_id: T13.2.3
            priority: high
            status: pending
            title: "[REM] Verify: Replace heuristic allocator with constraint-aware
              optimizer"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T13.3.2


              **Original Task**: Implement DMA-first geographic aggregation with
              hierarchical fallback

              **Epic**: Epic 13 — Weather-Aware Modeling Reality (E13)

              **Milestone**: M13.3


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Resolve the open question on geographic granularity by codifying
              DMA-first modeling with automatic fallback.
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T13.3.2
            milestone_id: M-REM-1
            original_epic: E13
            original_task_id: T13.3.2
            priority: high
            status: pending
            title: "[REM] Verify: Implement DMA-first geographic aggregation with
              hierarchical"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T13.4.1


              **Original Task**: Add modeling reality critic to Autopilot

              **Epic**: Epic 13 — Weather-Aware Modeling Reality (E13)

              **Milestone**: M13.4


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Teach Autopilot to generate the sort of gap analysis we just
              performed so future discrepancies surface automatically.
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T13.4.1
            milestone_id: M-REM-1
            original_epic: E13
            original_task_id: T13.4.1
            priority: high
            status: pending
            title: "[REM] Verify: Add modeling reality critic to Autopilot"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T13.5.1


              **Original Task**: Train weather-aware allocation model on top of
              MMM baseline

              **Epic**: Epic 13 — Weather-Aware Modeling Reality (E13)

              **Milestone**: M13.5


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Build allocation optimization model that incorporates
              weather-driven demand elasticity from MMM training
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T13.5.1
            milestone_id: M-REM-1
            original_epic: E13
            original_task_id: T13.5.1
            priority: high
            status: pending
            title: "[REM] Verify: Train weather-aware allocation model on top of MMM
              baseline"
          - dependencies: []
            description: >+
              REMEDIATION: Verify completed task T-MLR-0.1


              **Original Task**: Create ModelingReality critic with quantitative
              thresholds

              **Epic**: ML Model Remediation - From Prototype to Production
              (E-ML-REMEDIATION)

              **Milestone**: M-MLR-0


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              Create critic that enforces quantitative thresholds: R² > 0.50,
              correct

              elasticity signs, baseline comparison required, no subjective
              judgment.

            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T-MLR-0.1
            milestone_id: M-REM-1
            original_epic: E-ML-REMEDIATION
            original_task_id: T-MLR-0.1
            priority: high
            status: pending
            title: "[REM] Verify: Create ModelingReality critic with quantitative
              thresholds"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T-MLR-0.2


              **Original Task**: Update all ML task exit criteria with objective
              metrics

              **Epic**: ML Model Remediation - From Prototype to Production
              (E-ML-REMEDIATION)

              **Milestone**: M-MLR-0


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T-MLR-0.2
            milestone_id: M-REM-1
            original_epic: E-ML-REMEDIATION
            original_task_id: T-MLR-0.2
            priority: high
            status: pending
            title: "[REM] Verify: Update all ML task exit criteria with objective metrics"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T-MLR-1.1


              **Original Task**: Debug and fix weather multiplier logic in data
              generator

              **Epic**: ML Model Remediation - From Prototype to Production
              (E-ML-REMEDIATION)

              **Milestone**: M-MLR-1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T-MLR-1.1
            milestone_id: M-REM-1
            original_epic: E-ML-REMEDIATION
            original_task_id: T-MLR-1.1
            priority: high
            status: pending
            title: "[REM] Verify: Debug and fix weather multiplier logic in data generator"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T-MLR-1.3


              **Original Task**: Create validation tests for synthetic data
              quality

              **Epic**: ML Model Remediation - From Prototype to Production
              (E-ML-REMEDIATION)

              **Milestone**: M-MLR-1


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T-MLR-1.3
            milestone_id: M-REM-1
            original_epic: E-ML-REMEDIATION
            original_task_id: T-MLR-1.3
            priority: high
            status: pending
            title: "[REM] Verify: Create validation tests for synthetic data quality"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T-MLR-2.1


              **Original Task**: Implement proper train/val/test splitting with
              no leakage

              **Epic**: ML Model Remediation - From Prototype to Production
              (E-ML-REMEDIATION)

              **Milestone**: M-MLR-2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T-MLR-2.1
            milestone_id: M-REM-1
            original_epic: E-ML-REMEDIATION
            original_task_id: T-MLR-2.1
            priority: high
            status: pending
            title: "[REM] Verify: Implement proper train/val/test splitting with no leakage"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T-MLR-2.2


              **Original Task**: Implement LightweightMMM with weather features

              **Epic**: ML Model Remediation - From Prototype to Production
              (E-ML-REMEDIATION)

              **Milestone**: M-MLR-2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T-MLR-2.2
            milestone_id: M-REM-1
            original_epic: E-ML-REMEDIATION
            original_task_id: T-MLR-2.2
            priority: high
            status: pending
            title: "[REM] Verify: Implement LightweightMMM with weather features"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T-MLR-2.5


              **Original Task**: Compare models to baseline
              (naive/seasonal/linear)

              **Epic**: ML Model Remediation - From Prototype to Production
              (E-ML-REMEDIATION)

              **Milestone**: M-MLR-2


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T-MLR-2.5
            milestone_id: M-REM-1
            original_epic: E-ML-REMEDIATION
            original_task_id: T-MLR-2.5
            priority: high
            status: pending
            title: "[REM] Verify: Compare models to baseline (naive/seasonal/linear)"
          - dependencies: []
            description: >
              REMEDIATION: Verify completed task T-MLR-3.2


              **Original Task**: Write comprehensive ML validation documentation

              **Epic**: ML Model Remediation - From Prototype to Production
              (E-ML-REMEDIATION)

              **Milestone**: M-MLR-3


              **Assumption**: This task was completed WITHOUT quality gates.
              Assume failures until proven otherwise.


              **Required Verification**:


              1. **Implementation Check**:
                 - Does the code actually exist?
                 - Is it used or just scaffolded?
                 - Are there empty files or stub implementations?

              2. **Test Verification**:
                 - Do tests exist for this feature?
                 - Are tests meaningful or superficial?
                 - Do tests actually run (not just pass trivially)?
                 - Coverage check: are all code paths tested?

              3. **Quality Gate Review**:
                 - Run adversarial bullshit detector on this task's work
                 - Check for documentation-code mismatches
                 - Verify no superficial completion

              4. **Runtime Evidence**:
                 - Can you actually RUN this feature end-to-end?
                 - Does it work with realistic data?
                 - Any crashes or errors?

              5. **Fix Issues**:
                 - Document ALL issues found
                 - Fix critical issues immediately
                 - Create follow-up tasks for non-critical issues

              **Exit Criteria**:

              - Implementation verified (code exists and is used)

              - Tests exist and are meaningful

              - Quality gate review: APPROVED

              - Runtime verification: PASSED

              - All critical issues: FIXED

              - Evidence logged to remediation report


              **Original Description**:

              No description
            domain: product
            epic_id: E-REMEDIATION
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            id: REM-T-MLR-3.2
            milestone_id: M-REM-1
            original_epic: E-ML-REMEDIATION
            original_task_id: T-MLR-3.2
            priority: high
            status: pending
            title: "[REM] Verify: Write comprehensive ML validation documentation"
        title: "[CRITICAL] Core Infrastructure Audit"
      - id: M-MLR-2
        status: pending
        tasks:
          - dependencies:
              - T-MLR-2.2
            description: Train models on all 20 synthetic tenants with cross-validation
            domain: product
            exit_criteria: []
            id: T-MLR-2.3
            status: pending
            title: Train models on all 20 synthetic tenants with cross-validation
        title: M-MLR-2
      - id: M-MLR-3
        status: pending
        tasks:
          - dependencies:
              - T-MLR-3.2
            description: Package all evidence artifacts for review
            domain: product
            exit_criteria: []
            id: T-MLR-3.3
            status: in_progress
            title: Package all evidence artifacts for review
        title: M-MLR-3
      - id: M-MLR-4
        status: pending
        tasks:
          - dependencies:
              - T-MLR-0.1
              - T-MLR-3.3
            description: Deploy ModelingReality_v2 critic to production
            domain: product
            exit_criteria: []
            id: T-MLR-4.1
            status: pending
            title: Deploy ModelingReality_v2 critic to production
          - dependencies:
              - T-MLR-4.1
            description: Update autopilot policy to require critic approval
            domain: product
            exit_criteria: []
            id: T-MLR-4.2
            status: done
            title: Update autopilot policy to require critic approval
          - dependencies:
              - T-MLR-4.2
            description: Create meta-critic to review past completed ML tasks
            domain: product
            exit_criteria: []
            id: T-MLR-4.3
            status: pending
            title: Create meta-critic to review past completed ML tasks
          - dependencies:
              - T-MLR-4.3
            description: Document lessons learned and update contributor guide
            domain: product
            exit_criteria: []
            id: T-MLR-4.4
            status: pending
            title: Document lessons learned and update contributor guide
        title: M-MLR-4
  - domain: modeling
    id: E-ML-REMEDIATION
    milestones:
      - id: M-MLR-0
        status: pending
        tasks:
          - dependencies: []
            description: >
              Create critic that enforces quantitative thresholds: R² > 0.50,
              correct

              elasticity signs, baseline comparison required, no subjective
              judgment.
            domain: modeling
            exit_criteria:
              - artifact:tools/wvo_mcp/src/critics/modeling_reality_v2.ts
              - test:Critic FAILS when R² < 0.50
              - test:Critic FAILS when no baseline comparison
              - test:Critic FAILS when weather elasticity signs wrong
              - metric:critic_strictness = 1.0
              - critic:tests
            id: T-MLR-0.1
            status: done
            title: Create ModelingReality critic with quantitative thresholds
          - dependencies: []
            domain: modeling
            exit_criteria:
              - artifact:state/roadmap.yaml (T12.*, T13.* updated)
              - verification:All ML tasks have "metric:r2 > 0.50"
              - verification:All ML tasks have "metric:beats_baseline > 1.10"
              - verification:All ML tasks have "critic:modeling_reality_v2"
            id: T-MLR-0.2
            status: done
            title: Update all ML task exit criteria with objective metrics
          - dependencies: []
            domain: modeling
            exit_criteria:
              - artifact:docs/ML_QUALITY_STANDARDS.md
              - verification:Numeric thresholds for all metrics
              - verification:Baseline comparison requirements
              - review:External ML practitioner peer review
            id: T-MLR-0.3
            status: pending
            title: Document world-class quality standards for ML
        title: "Foundation: Truth & Accountability"
      - id: M-MLR-1
        status: pending
        tasks:
          - dependencies: []
            domain: modeling
            exit_criteria:
              - artifact:scripts/weather/generate_synthetic_tenants_v2.py
              - test:Extreme correlation = 0.85 ± 0.05
              - test:High correlation = 0.70 ± 0.05
              - test:Medium correlation = 0.40 ± 0.05
              - test:None correlation < 0.10
              - critic:data_quality
            id: T-MLR-1.1
            status: done
            title: Debug and fix weather multiplier logic in data generator
          - dependencies: []
            domain: modeling
            exit_criteria:
              - artifact:storage/seeds/synthetic_v2/*.parquet (20 files)
              - metric:total_rows = 219000
              - metric:date_range = 2022-01-01 to 2024-12-31
              - metric:weather_correlations_within_target >= 0.90
              - test:pytest tests/data_gen/test_synthetic_v2_quality.py
              - critic:data_quality
            id: T-MLR-1.2
            status: pending
            title: Generate 3 years of synthetic data for 20 tenants
          - dependencies: []
            domain: modeling
            exit_criteria:
              - artifact:tests/data_gen/test_synthetic_v2_quality.py
              - test:20/20 tenant tests pass
              - artifact:experiments/data_validation/correlation_plots.pdf
              - critic:tests
            id: T-MLR-1.3
            status: done
            title: Create validation tests for synthetic data quality
        title: "Phase 1: Fix Synthetic Data (2 weeks)"
      - id: M-MLR-2
        status: pending
        tasks:
          - dependencies: []
            domain: modeling
            exit_criteria:
              - artifact:shared/libs/modeling/time_series_split.py
              - test:Validation after training (no date overlap)
              - test:Test after validation (no date overlap)
              - test:Split percentages 70/15/15
              - critic:leakage
            id: T-MLR-2.1
            status: done
            title: Implement proper train/val/test splitting with no leakage
          - dependencies: []
            domain: modeling
            exit_criteria:
              - artifact:apps/model/mmm_lightweight_weather.py
              - test:Adstock transformation applied
              - test:Hill saturation curves applied
              - test:Weather interaction terms included
              - test:pytest tests/model/test_mmm_lightweight.py (12/12 pass)
              - critic:academic_rigor
            id: T-MLR-2.2
            status: done
            title: Implement LightweightMMM with weather features
          - dependencies: []
            domain: modeling
            exit_criteria:
              - artifact:experiments/mmm_v2/validation_report.json
              - metric:weather_sensitive_r2_pass_rate >= 0.80
              - metric:weather_elasticity_sign_correct = 1.0
              - metric:no_overfitting_detected = true
              - critic:modeling_reality_v2
              - critic:academic_rigor
            id: T-MLR-2.4
            status: pending
            title: Validate model performance against objective thresholds
          - dependencies: []
            domain: modeling
            exit_criteria:
              - artifact:experiments/mmm_v2/baseline_comparison.json
              - metric:beats_naive_by >= 1.10
              - metric:beats_seasonal_by >= 1.05
              - metric:beats_linear_by >= 1.05
              - artifact:experiments/mmm_v2/baseline_comparison_plots.pdf
              - critic:modeling_reality_v2
            id: T-MLR-2.5
            status: done
            title: Compare models to baseline (naive/seasonal/linear)
          - dependencies: []
            domain: modeling
            exit_criteria:
              - artifact:experiments/mmm_v2/robustness_report.json
              - test:Model handles extreme weather without crash
              - test:Model handles missing data gracefully
              - test:Zero ad spend predicts organic baseline
              - test:pytest tests/model/test_mmm_robustness.py (15/15 pass)
              - critic:modeling_reality_v2
            id: T-MLR-2.6
            status: pending
            title: Run robustness tests (outliers, missing data, edge cases)
        title: "Phase 2: Rigorous MMM Training (3 weeks)"
      - id: M-MLR-3
        status: pending
        tasks:
          - dependencies: []
            domain: modeling
            exit_criteria:
              - artifact:experiments/mmm_v2/validation_notebook.ipynb
              - test:Notebook runs end-to-end without errors
              - test:Output matches claimed metrics
              - artifact:experiments/mmm_v2/validation_notebook.html
              - critic:academic_rigor
            id: T-MLR-3.1
            status: pending
            title: Create reproducible validation notebook
          - dependencies: []
            domain: modeling
            exit_criteria:
              - artifact:docs/ML_VALIDATION_COMPLETE.md
              - verification:Links to reproducible notebook
              - verification:Includes limitations section
              - verification:Includes baseline comparisons
              - review:External ML practitioner peer review
            id: T-MLR-3.2
            status: done
            title: Write comprehensive ML validation documentation
        title: "Phase 3: Reproducibility & Documentation (1 week)"
    status: pending
    title: ML Model Remediation - From Prototype to Production
  - domain: product
    id: E-PHASE0
    milestones:
      - id: M0.1
        status: done
        tasks:
          - dependencies: []
            description: Wire apps/validation/incrementality.py into ingestion runs with
              nightly job execution
            domain: product
            exit_criteria:
              - artifact:state/analytics/experiments/geo_holdouts/*.json
              - artifact:state/telemetry/experiments/geo_holdout_runs.jsonl
              - critic:data_quality
            id: T0.1.1
            status: done
            title: Implement geo holdout plumbing
          - dependencies: []
            description: Plan API surfaces experiment payloads; Plan UI renders
              lift/confidence cards with download
            domain: product
            exit_criteria:
              - artifact:apps/api/schemas/plan.py
              - artifact:apps/web/src/pages/plan.tsx
              - critic:tests
              - critic:design_system
            id: T0.1.2
            status: done
            title: Build lift & confidence UI surfaces
          - dependencies: []
            description: Quantile calibration metrics with summary published to docs
            domain: product
            exit_criteria:
              - artifact:docs/modeling/forecast_calibration_report.md
              - artifact:state/telemetry/calibration/*.json
              - critic:forecast_stitch
            id: T0.1.3
            status: done
            title: Generate forecast calibration report
        title: Measurement & Confidence Foundations
    status: done
    title: "Phase 0: Measurement & Confidence"
  - domain: product
    id: E-PHASE1
    milestones:
      - id: M1.1
        status: done
        tasks:
          - dependencies: []
            description: Implement GET/POST /onboarding/progress routes with telemetry
              instrumentation
            domain: product
            exit_criteria:
              - artifact:apps/api/routes/onboarding.py
              - artifact:apps/web/src/hooks/useOnboardingProgress.ts
              - critic:tests
            id: T1.1.3
            status: done
            title: Wire onboarding progress API
        title: Experience Delivery MVP
    status: done
    title: "Phase 1: Experience Delivery"
  - description: Stand up weather + marketing ingestion, harmonise geo/time, and
      validate data quality.
    domain: product
    id: E1
    milestones:
      - id: M1.1
        status: done
        tasks:
          - dependencies: []
            description: Interactive scenario flows with API endpoints for scenario
              snapshots and storybook coverage
            domain: product
            exit_criteria:
              - critic:build
              - critic:tests
              - doc:docs/INGESTION.md
            id: T1.1.1
            status: done
            title: Design Open-Meteo + Shopify connectors and data contracts
          - dependencies: []
            description: Map + chart overlays with export service (PPT/CSV)
            domain: product
            exit_criteria:
              - critic:data_quality
              - critic:org_pm
              - artifact:experiments/ingest/dq_report.json
            id: T1.1.2
            status: done
            title: Implement ingestion Prefect flow with checkpointing
        title: Connector scaffolding
      - id: M1.2
        status: done
        tasks:
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:forecast_stitch
              - doc:docs/weather/blending.md
            id: T1.2.1
            status: done
            title: Blend historical + forecast weather, enforce timezone alignment
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:leakage
              - critic:tests
            id: T1.2.2
            status: done
            title: Add leakage guardrails to feature builder
        title: Weather harmonisation
    status: done
    title: Epic 1 — Ingest & Weather Foundations
  - domain: mcp
    id: E10
    milestones:
      - id: M10.1
        status: done
        tasks:
          - dependencies: []
            domain: mcp
            exit_criteria:
              - Provider cost telemetry recorded in
                state/telemetry/operations.jsonl
              - Budget thresholds configurable per environment
              - Alert surfaced via state/context.md and orchestration logs
            id: T10.1.1
            status: done
            title: Cost telemetry and budget alerts
        title: Usage telemetry & guardrails
    status: done
    title: PHASE-6-COST — Usage-Based Optimisations
  - description: Auto-detect hardware, adapt workloads, and guarantee great
      performance on constrained machines.
    domain: product
    id: E11
    milestones:
      - id: M11.1
        status: done
        tasks:
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:build
              - doc:docs/ROADMAP.md
            id: T11.1.1
            status: done
            title: Implement hardware probe & profile persistence
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:tests
              - artifact:state/device_profiles.json
            id: T11.1.2
            status: done
            title: Adaptive scheduling for heavy tasks
        title: Capability Detection
      - id: M11.2
        status: done
        tasks:
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:design_system
              - doc:docs/WEB_DESIGN_SYSTEM.md
            id: T11.2.1
            status: done
            title: Design system elevation (motion, typography, theming)
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:exec_review
              - artifact:docs/UX_CRITIQUE.md
            id: T11.2.2
            status: done
            title: Award-level experience audit & remediation
          - dependencies: []
            domain: product
            exit_criteria:
              - artifact:apps/web/styles/themes/calm.ts
              - artifact:apps/web/styles/themes/aero.ts
              - critic:design_system
            id: T11.2.3
            status: done
            title: Extend calm/aero theme tokens to Automations and Experiments surfaces
          - dependencies: []
            domain: product
            exit_criteria:
              - artifact:apps/web/styles/tokens/gradients.md
              - critic:design_system
            id: T11.2.4
            status: done
            title: Refactor landing/marketing gradients into reusable tokens
          - dependencies: []
            domain: product
            exit_criteria:
              - artifact:apps/web/components/buttons/RetryButton.tsx
              - unknown
              - critic:design_system
            id: T11.2.5
            status: done
            title: Centralize retry button styles in shared component once App Router lands
          - dependencies: []
            domain: product
            exit_criteria:
              - artifact:apps/web/styles/mixins/panel.css
              - critic:design_system
            id: T11.2.6
            status: done
            title: Formalize shared panel mixin (border + shadow) to reduce overrides
        title: Falcon Design System & Award-ready UX
    status: done
    title: Resource-Aware Intelligence & Personalisation
  - description: Prioritise end-to-end weather ingestion QA, model backtests, and
      operational readiness so WeatherVane shiproom can demo weather insights
      with confidence.
    domain: product
    id: E12
    milestones:
      - id: M12.0
        status: pending
        tasks:
          - dependencies: []
            description: Create 4 simulated tenants with Shopify products, Meta/Google ads
              spend, Klaviyo data, and weather-driven demand patterns
            domain: product
            exit_criteria:
              - artifact:storage/seeds/synthetic/*.parquet
              - artifact:state/analytics/synthetic_tenant_profiles.json
              - critic:data_quality
            id: T12.0.1
            status: pending
            title: Generate synthetic multi-tenant dataset with weather-sensitive products
          - dependencies: []
            description: Confirm data volume, coverage, completeness; measure weather
              elasticity for each tenant
            domain: product
            exit_criteria:
              - artifact:state/analytics/synthetic_data_validation.json
              - artifact:docs/DATA_GENERATION.md
              - critic:data_quality
            id: T12.0.2
            status: pending
            title: Validate synthetic data quality and weather correlation
          - dependencies: []
            description: Create data dictionary with tenant profiles, weather sensitivity,
              expected model behaviors
            domain: product
            exit_criteria:
              - artifact:docs/SYNTHETIC_TENANTS.md
              - artifact:state/analytics/tenant_weather_profiles.json
              - critic:data_quality
            id: T12.0.3
            status: done
            title: Document synthetic tenant characteristics
        title: Synthetic Multi-Tenant Dataset Generation
      - id: M12.1
        status: pending
        tasks:
          - dependencies: []
            domain: product
            exit_criteria:
              - artifact:state/telemetry/weather_ingestion.json
              - critic:data_quality
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:causal
            id: T12.1.1
            status: pending
            title: Run smoke-context and weather ingestion regression suite nightly
          - dependencies: []
            domain: product
            exit_criteria:
              - artifact:experiments/weather/feature_backfill_report.md
              - critic:forecast_stitch
              - critic:tests
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:data_quality
              - critic:causal
            id: T12.1.2
            status: done
            title: Validate feature store joins against historical weather baselines
        title: Weather ingestion + feature QA
      - id: M12.2
        status: pending
        tasks:
          - dependencies: []
            domain: product
            exit_criteria:
              - artifact:experiments/weather/model_backtest_summary.md
              - critic:causal
              - critic:allocator
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
            id: T12.2.1
            status: done
            title: Backtest weather-aware model vs control across top tenants
          - dependencies: []
            domain: product
            exit_criteria:
              - artifact:apps/web/pages/ops/weather-capabilities.tsx
              - critic:org_pm
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:causal
            id: T12.2.2
            status: pending
            title: Publish weather capability runbook and monitoring dashboards
        title: Weather model capability sign-off
      - id: M12.3
        status: pending
        tasks:
          - dependencies: []
            description: Train multi-channel MMM using validated 90-day data with weather
              features integrated
            domain: product
            exit_criteria:
              - artifact:experiments/mcp/mmm_weather_model.json
              - critic:causal
              - critic:academic_rigor
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
            id: T12.3.1
            status: pending
            title: Train weather-aware MMM on validated 90-day tenant data
          - dependencies: []
            description: Quantify how demand elasticity varies by weather (temperature
              sensitivity, rain impact, seasonal patterns)
            domain: product
            exit_criteria:
              - artifact:docs/models/weather_elasticity_analysis.md
              - critic:causal
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
            id: T12.3.2
            status: pending
            title: Implement weather sensitivity elasticity estimation
          - dependencies: []
            description: Deploy MMM with weather features as production inference service
            domain: product
            exit_criteria:
              - artifact:apps/api/routes/models/weather_mixin.py
              - artifact:apps/worker/models/mmm_weather_inference.py
              - critic:tests
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:causal
            id: T12.3.3
            status: pending
            title: Ship production MMM inference service with real-time weather scoring
        title: Weather-Aware MMM Model Training
      - id: M12.UXExcellence
        status: pending
        tasks:
          - dependencies:
              - T12.0.1
            description: Configure screenshot_session, screenshot_capture_multiple tools for
              visual design iteration. Document workflow for
              Build→Screenshot→Critique→Refine loop. PRIORITY NOTE - Execute
              only after T12.0.1 (data generation) completes.
            domain: product
            exit_criteria:
              - artifact:state/screenshot_config.yaml
              - artifact:docs/DESIGN_WORKFLOW.md
              - critic:tests
            id: T12.UX.1
            status: pending
            title: Setup Playwright screenshot workflow for design iteration
          - dependencies:
              - T12.0.1
            description: Document process for researching award-winning UIs (Awwwards, FWA,
              Stripe, Linear, Observable). Create reference library of
              world-class design patterns to inform WeatherVane UI decisions.
              Extract principles from Dieter Rams, Müller-Brockmann, Vignelli,
              Paul Rand.
            domain: product
            exit_criteria:
              - artifact:docs/DESIGN_INSPIRATION.md
              - artifact:state/design_references.json
            id: T12.UX.2
            status: pending
            title: Create design research process and inspiration library
          - dependencies:
              - T12.0.1
            description: Create checklist for trivial delights (micro-interactions, loading
              states, empty states, witty copy) and non-trivial delights
              (anticipatory UX, intelligent recovery, time-saving magic).
              Validation - Would user screenshot and share? Would Don
              Norman/Kathy Sierra/Julie Zhuo approve?
            domain: product
            exit_criteria:
              - artifact:docs/SURPRISE_DELIGHT_CHECKLIST.md
              - artifact:tools/wvo_mcp/src/critics/ux_delight_scorer.ts
            id: T12.UX.3
            status: pending
            title: Establish surprise & delight checklist and validation criteria
          - dependencies:
              - T12.UX.1
              - T12.UX.2
              - T12.UX.3
            description: Enhance design_system critic to detect generic gradients, stock
              layouts, template components. Enforce heritage design principles.
              Integrate with screenshot workflow to compare against world-class
              references. Block merge if AI aesthetic detected.
            domain: product
            exit_criteria:
              - artifact:tools/wvo_mcp/src/critics/design_system_enhanced.ts
              - critic:design_system
            id: T12.UX.4
            status: pending
            title: Configure design_system critic for zero-AI-aesthetic enforcement
        title: Design Excellence Infrastructure (Lower Priority - After Data Gen)
      - id: M12.Demo
        status: pending
        tasks:
          - dependencies:
              - T12.UX.1
              - T12.UX.2
              - T12.UX.3
              - T12.UX.4
            description: Create web UI that lets stakeholders toggle weather on/off and see
              impact on predicted revenue and ROAS for each synthetic tenant.
              Interactive proof that weather matters for demand forecasting.
              DESIGN EXCELLENCE REQUIRED - Follow M12.UXExcellence standards
              (Playwright iteration, world-class inspiration, surprise &
              delight, zero AI aesthetic). Use screenshot_session for visual QA.
              Validate with design_system critic.
            domain: product
            exit_criteria:
              - artifact:apps/web/src/pages/demo-weather-analysis.tsx
              - critic:design_system
              - artifact:state/artifacts/screenshots/demo_ui_iteration/
            id: T12.Demo.1
            notes: "Auto-unblocked by CriticAvailabilityGuardian: Critics design_system are
              offline. Proceeding with implementation; gather QA evidence for
              eventual review. UPDATED - Now depends on M12.UXExcellence
              infrastructure."
            status: pending
            title: Build interactive demo UI showing weather impact on ROAS
          - dependencies: []
            description: Record 5-min demo video showing weather-aware model in action.
              Create 1-page brief for executives explaining business impact
              (revenue upside, forecast accuracy improvement, ROAS optimization
              potential).
            domain: product
            exit_criteria:
              - artifact:docs/WEATHER_DEMO_BRIEF.md
              - artifact:state/artifacts/stakeholder/weather_demo_script.md
            id: T12.Demo.2
            status: pending
            title: Record demo video and create stakeholder brief
        title: Executive Demo & Stakeholder Sign-Off
      - id: M12.PoC
        status: pending
        tasks:
          - dependencies: []
            description: Train a baseline weather-aware regression model on each synthetic
              tenant (high/extreme/medium/none sensitivity) to validate the
              weather correlation detection works across different product types
              and sensitivity profiles.
            domain: product
            exit_criteria:
              - artifact:experiments/mcp/weather_poc_model.pkl
              - artifact:experiments/mcp/weather_poc_metrics.json
              - critic:academic_rigor
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
            id: T12.PoC.1
            status: pending
            title: Train weather-aware model on synthetic tenant data
          - dependencies: []
            description: "Test PoC model on final 30 days of each synthetic tenant. Verify
              that: (1) High/Extreme tenants show strong weather effects, (2)
              None tenant shows no weather effect, (3) Model R² > 0.6 on
              validation set"
            domain: product
            exit_criteria:
              - artifact:experiments/mcp/weather_poc_validation.json
              - critic:causal
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
            id: T12.PoC.2
            status: pending
            title: Validate PoC model predictions on hold-out data
          - dependencies: []
            description: "Create executive brief demonstrating that weather-aware modeling
              works: show before/after model performance, weather elasticity
              coefficients, and prediction examples. Use this to get stakeholder
              buy-in before full MMM training."
            domain: product
            exit_criteria:
              - artifact:docs/WEATHER_PROOF_OF_CONCEPT.md
              - artifact:experiments/mcp/poc_demo_charts.json
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
            id: T12.PoC.3
            status: done
            title: Create PoC demo results and proof brief
        title: Proof of Concept & Model Testing
    status: pending
    title: Epic 12 — Weather Model Production Validation
  - description: Close the execution gap between sophisticated modeling plans and
      the current codebase, while embedding Autopilot self-critique so these
      regressions cannot hide in the future.
    domain: product
    id: E13
    milestones:
      - id: M13.1
        status: in_progress
        tasks:
          - dependencies: []
            description: Ensure the Shopify/ads/weather ingestion flows actually populate
              product_daily with geocoded spend and 90+ days of history so MMM
              inputs are real, not theoretical.
            domain: product
            exit_criteria:
              - artifact:experiments/features/weather_join_validation.json
              - critic:data_quality passes with weather join metrics captured
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:data_quality
              - critic:causal
            id: T13.1.1
            status: done
            title: Validate 90-day tenant data coverage across sales, spend, and weather
          - dependencies: []
            description: Bake data completeness checks into Autopilot so future
              weather-awareness regressions trigger automated investigations.
            domain: product
            exit_criteria:
              - critic:weather_coverage autop-run nightly with failure
                escalation to Atlas
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:data_quality
              - critic:causal
            id: T13.1.2
            status: pending
            title: Autopilot guardrail for ingestion + weather drift
          - dependencies: []
            description: >
              Auto-classify products from Shopify/Meta/Google using LLM
              (Claude/GPT-4) to tag products with weather affinity and category
              hierarchy. This enables product-level modeling (not just
              brand-level) and cold-start for new brands.
            domain: product
            exit_criteria:
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:data_quality
              - critic:causal
            id: T13.1.3
            status: done
            title: Implement product taxonomy auto-classification with weather affinity
          - dependencies: []
            description: >
              Implement data quality checks to verify data is ready for ML
              training. Prevents training models on insufficient/corrupted data.
            domain: product
            exit_criteria:
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:causal
            id: T13.1.4
            status: done
            title: Data quality validation framework (verify data fitness for ML)
        title: Data Backbone Verified
      - id: M13.2
        status: pending
        tasks:
          - dependencies: []
            description: Integrate the existing LightweightMMM wrapper so allocations use
              Bayesian adstock/saturation estimates instead of covariance
              heuristics.
            domain: product
            exit_criteria:
              - critic:model_fit passes with synthetic recovery tests
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:causal
            id: T13.2.1
            status: done
            title: Replace heuristic MMM with LightweightMMM adstock+saturation fit
          - dependencies: []
            description: Establish out-of-sample evaluation so MMM recommendations are
              validated continuously.
            domain: product
            exit_criteria:
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:causal
            id: T13.2.2
            status: pending
            title: Build MMM backtesting + regression suite
          - dependencies: []
            description: >
              Replace heuristic allocation rules (±10% budget adjustments) with
              proper constrained optimization. Use cvxpy or OR-Tools to maximize
              ROAS subject to budget, inventory, and platform constraints.
            domain: product
            exit_criteria:
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:causal
            id: T13.2.3
            status: done
            title: Replace heuristic allocator with constraint-aware optimizer
        title: MMM Upgrade & Backtests
      - id: M13.3
        status: pending
        tasks:
          - dependencies: []
            description: Adopt causal estimators appropriate for non-manipulable treatments
              so weather impact claims are statistically defensible.
            domain: product
            exit_criteria:
              - critic:causal passes with new methodology notes in
                docs/CAUSAL_LIMITATIONS.md
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:causal
            id: T13.3.1
            status: pending
            title: Swap uplift propensity scoring with DID/synthetic control for weather
              shocks
          - dependencies: []
            description: Resolve the open question on geographic granularity by codifying
              DMA-first modeling with automatic fallback.
            domain: product
            exit_criteria:
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:causal
            id: T13.3.2
            status: done
            title: Implement DMA-first geographic aggregation with hierarchical fallback
        title: Causal & Geography Alignment
      - id: M13.4
        status: pending
        tasks:
          - dependencies: []
            description: Teach Autopilot to generate the sort of gap analysis we just
              performed so future discrepancies surface automatically.
            domain: product
            exit_criteria:
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
            id: T13.4.1
            status: done
            title: Add modeling reality critic to Autopilot
          - dependencies: []
            description: Provide a repeatable process—documentation, scheduling, and
              telemetry—for leadership to review modeling execution against
              strategy.
            domain: product
            exit_criteria:
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
            id: T13.4.2
            status: pending
            title: Meta-evaluation playbook for modeling roadmap
        title: Autopilot Meta-Critique Loop
      - id: M13.5
        status: pending
        tasks:
          - dependencies: []
            description: Build allocation optimization model that incorporates
              weather-driven demand elasticity from MMM training
            domain: product
            exit_criteria:
              - artifact:experiments/allocation/weather_aware_model.json
              - critic:allocator
              - critic:causal
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
            id: T13.5.1
            status: done
            title: Train weather-aware allocation model on top of MMM baseline
          - dependencies: []
            description: Add constraints that adjust budget allocation based on weather
              forecasts (e.g., reduce spend on low-demand weather days)
            domain: product
            exit_criteria:
              - artifact:experiments/allocation/constraint_validation.json
              - critic:tests
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:causal
            id: T13.5.2
            status: pending
            title: Implement weather-responsive budget allocation constraints
          - dependencies: []
            description: Ship weather-aware allocation as the primary recommendation engine
            domain: product
            exit_criteria:
              - artifact:apps/api/routes/allocate.py updated with weather model
              - artifact:apps/worker/allocation_service.py deployed
              - critic:tests
              - critic:allocator
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
            id: T13.5.3
            status: pending
            title: Deploy weather-aware allocator to production
        title: Weather-Aware Allocation Model Deployment
    status: pending
    title: Epic 13 — Weather-Aware Modeling Reality
  - description: Ship lagged features, baseline models, and evaluation harness.
    domain: product
    id: E2
    milestones:
      - id: M2.1
        status: done
        tasks:
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:build
              - critic:tests
              - critic:data_quality
            id: T2.1.1
            status: done
            title: Build lag/rolling feature generators with deterministic seeds
        title: Feature pipeline
      - id: M2.2
        status: done
        tasks:
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:causal
              - critic:academic_rigor
              - doc:docs/models/baseline.md
            id: T2.2.1
            status: done
            title: Train weather-aware GAM baseline and document methodology
        title: Baseline modeling
    status: done
    title: Epic 2 — Features & Modeling Baseline
  - description: Allocator robustness checks, dashboards, and UI polish.
    domain: product
    id: E3
    milestones:
      - id: M3.1
        status: done
        tasks:
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:allocator
              - critic:cost_perf
              - artifact:experiments/policy/regret.json
            id: T3.1.1
            status: done
            title: Implement budget allocator stress tests and regret bounds
        title: Allocator guardrails
      - id: M3.2
        status: done
        tasks:
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:design_system
            id: T3.2.1
            status: done
            title: Run design system critic and ensure accessibility coverage
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:design_system
              - critic:exec_review
              - doc:docs/UX_CRITIQUE.md
              - artifact:docs/product/UX_CRITIQUE.md
            id: T3.2.2
            status: done
            title: Elevate dashboard storytelling & UX
        title: Dashboard + UX review
      - id: M3.3
        status: done
        tasks:
          - dependencies: []
            domain: product
            exit_criteria:
              - artifact:docs/orchestration/multi_agent_charter.md
              - critic:manager_self_check
              - critic:org_pm
            id: T3.3.1
            status: done
            title: Draft multi-agent charter & delegation mesh (AutoGen/Swarm patterns)
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:integration_fury
              - critic:manager_self_check
              - doc:docs/orchestration/consensus_engine.md
            id: T3.3.2
            status: done
            title: Implement hierarchical consensus & escalation engine
          - dependencies: []
            domain: product
            exit_criteria:
              - artifact:experiments/orchestration/simulation_report.md
              - critic:tests
              - critic:health_check
            id: T3.3.3
            status: done
            title: Build closed-loop simulation harness for autonomous teams
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:prompt_budget
              - critic:exec_review
              - artifact:state/analytics/orchestration_metrics.json
            id: T3.3.4
            status: done
            title: Instrument dynamic staffing telemetry & learning pipeline
        title: Autonomous Orchestration Blueprints
      - id: M3.4
        status: done
        tasks:
          - dependencies: []
            domain: product
            exit_criteria:
              - artifact:apps/web/pages/plan.tsx
              - unknown
              - critic:design_system
              - unknown
            id: T3.4.1
            status: done
            title: Implement Plan overview page with weather-driven insights
          - dependencies: []
            domain: product
            exit_criteria:
              - artifact:apps/web/pages/dashboard.tsx
              - unknown
              - critic:design_system
              - unknown
            id: T3.4.2
            status: done
            title: Build WeatherOps dashboard with allocator + weather KPIs
          - dependencies: []
            domain: product
            exit_criteria:
              - artifact:apps/web/pages/experiments.tsx
              - unknown
              - critic:design_system
              - unknown
            id: T3.4.3
            status: done
            title: Ship Experiments hub UI for uplift & incrementality reviews
          - dependencies: []
            domain: product
            exit_criteria:
              - artifact:apps/web/pages/reports.tsx
              - unknown
              - critic:design_system
              - unknown
            id: T3.4.4
            status: done
            title: Deliver storytelling Reports view with weather + spend narratives
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:design_system
              - doc:docs/product/acceptance_report.md
              - artifact:state/critics/designsystem.json
            id: T3.4.5
            status: done
            title: Conduct design_system + UX acceptance review across implemented pages
          - dependencies: []
            description: >-
              Requirements:
                - Show operators exactly what WeatherVane changed or recommends changing, with clear “what/why/next” messaging.
                - Remove jargon (“guardrail”, “triage”) in favour of user-facing language (e.g., “Overspend alert”, “Weather action”).
                - Keep first view scannable: one hero recommendation, secondary cards, optional detail drill-down.
              Standards:
                - Copy: conversational, action-oriented, no internal terminology.
                - UX: minimalist layout, responsive to desktop/tablet, accessible (WCAG AA).
                - Engineering: Playwright smoke must pass; analytics instrumentation preserved; Vitest coverage for helpers.
              Implementation Plan:
                - Draft/record design brief in docs/UX_CRITIQUE.md.
                - Refactor hero + summary components around new copy and layout.
                - Update analytics helpers/tests, run Vitest + Playwright.
                - Capture iteration in state/context.md with screenshots and critic notes.
              Deliverables:
                - Updated React/CSS modules under apps/web/src/pages/dashboard.tsx and styles.
                - Revised helper libraries/tests (apps/web/src/lib/**, tests/web/**).
                - Playwright report + screenshots stored under state/artifacts/ui/weatherops.
              Integration Points:
                - API suggestion telemetry (shared/services/dashboard_analytics_ingestion.py) ensuring copy aligns with payload fields.
                - Analytics events (`trackDashboardEvent`) and downstream dashboards; coordinate with data/ML owners if field names change.
                - Worker-generated suggestion summaries (apps/worker/flows/poc_pipeline.py) to maintain consistency across channels.
              Evidence:
                - Playwright run ID + html report.
                - design_system + exec_review critic outputs (once available).
                - Context entry summarising decisions, open questions, next iteration.
            domain: product
            exit_criteria:
              - unknown
              - unknown
              - critic:design_system
              - critic:exec_review
              - unknown
            id: T3.4.6
            status: done
            title: Rewrite WeatherOps dashboard around plain-language decisions
          - dependencies: []
            description: >-
              Requirements:
                - Explain every autonomous change in plain language (what changed, when, why, impact).
                - Provide explicit approval/rollback affordances for humans and highlight pending reviews.
                - Surface evidence (metrics, weather context, spend forecasts) inline or a click away.
              Standards:
                - Copy: transparent, confidence-building, avoids “audit/guardrail” jargon.
                - UX: timeline or table must prioritise newest changes, support filtering; accessible controls for approvals.
                - Engineering: tests updated (Vitest, Playwright), telemetry preserved, change log data schema documented.
                - ML context (if applicable): explain model confidence/reason codes clearly.
              Implementation Plan:
                - Extend docs/UX_CRITIQUE.md with Automations brief, list user questions + acceptance metrics.
                - Redesign components/layout in apps/web/src/pages/automations.tsx; integrate evidence panels.
                - Update helpers/tests, run Vitest + Playwright, capture critics.
                - Log iterations in state/context.md with before/after screenshots and open questions.
              Deliverables:
                - Updated Automations page/components/styles.
                - Supporting helper modules/tests (automationInsights, validation, etc.).
                - Evidence artifacts (Playwright report, screenshots, context notes).
              Integration Points:
                - Automation audit APIs/events (apps/api/services/dashboard_service.py, shared schemas) so reason codes remain synchronized.
                - Worker automation execution logs (`apps/worker/flows/**`) and telemetry exports consumed by directors/Dana.
                - Notification channels or forthcoming approval workflows (e.g., Slack/email) to ensure new statuses map correctly.
              Evidence:
                - Playwright run + report stored under state/artifacts/ui/automations.
                - design_system + exec_review critic confirmation.
                - Context log summarising decisions, trade-offs, next steps.
            domain: product
            exit_criteria:
              - unknown
              - unknown
              - critic:design_system
              - critic:exec_review
              - unknown
            id: T3.4.7
            status: done
            title: Reimagine Automations change log as a trust-first narrative
        title: Experience Implementation
    status: done
    title: Epic 3 — Allocation & UX
  - description: Maintain velocity while hardening performance and delivery processes.
    domain: product
    id: E4
    milestones:
      - id: M4.1
        status: done
        tasks:
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:allocator
              - artifact:experiments/allocator/saturation_report.json
            id: T4.1.10
            status: done
            title: Cross-market saturation optimization (fairness-aware)
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:causal
              - artifact:experiments/causal/uplift_report.json
            id: T4.1.3
            status: done
            title: Causal uplift modeling & incremental lift validation
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:forecast_stitch
              - artifact:experiments/forecast/ensemble_metrics.json
            id: T4.1.4
            status: done
            title: Multi-horizon ensemble forecasting
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:allocator
              - unknown
            id: T4.1.5
            status: done
            title: Non-linear allocation optimizer with constraints (ROAS, spend caps)
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:allocator
              - artifact:experiments/allocator/hf_response.json
            id: T4.1.6
            status: done
            title: High-frequency spend response modeling (intraday adjustments)
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:allocator
              - unknown
            id: T4.1.7
            status: done
            title: Marketing mix budget solver (multi-channel, weather-aware)
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:allocator
              - artifact:experiments/rl/shadow_mode.json
            id: T4.1.8
            status: done
            title: Reinforcement-learning shadow mode (safe exploration)
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:design_system
              - artifact:experiments/creative/response_scores.json
            id: T4.1.9
            status: done
            title: Creative-level response modeling with brand safety guardrails
        title: Optimization sprint
    status: done
    title: Epic 4 — Operational Excellence
  - description: Enable WeatherVane to programmatically create, update, monitor, and
      rollback ads across major platforms.
    domain: product
    id: E5
    milestones:
      - id: M5.1
        status: done
        tasks:
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:allocator
              - unknown
            id: T5.1.1
            status: done
            title: Implement Meta Marketing API client (creative + campaign management)
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:security
              - artifact:experiments/meta/sandbox_run.json
            id: T5.1.2
            status: done
            title: Meta sandbox and dry-run executor with credential vaulting
        title: Meta Ads Command Pipeline
      - id: M5.2
        status: done
        tasks:
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:allocator
              - unknown
            id: T5.2.1
            status: done
            title: Google Ads API integration (campaign create/update, shared budgets)
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:allocator
              - artifact:experiments/allocator/spend_guardrails.json
            id: T5.2.2
            status: done
            title: Budget reconciliation & spend guardrails across platforms
        title: Google Ads Execution & Budget Sync
      - id: M5.3
        status: done
        tasks:
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:tests
              - artifact:state/ad_push_diffs.json
            id: T5.3.1
            status: done
            title: Dry-run & diff visualizer for ad pushes (pre-flight checks)
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:manager_self_check
              - artifact:experiments/allocator/rollback_sim.json
            id: T5.3.2
            status: done
            title: Automated rollback + alerting when performance/regression detected
        title: QA, Rollback & Safety Harness
    status: done
    title: Ad Platform Execution & Automation
  - description: Validate and harden the dual-provider MCP orchestrator for
      autonomous operation while preserving 100% run safety. All milestones
      under this epic must enforce the blue/green upgrade guardrails defined in
      docs/MCP_ORCHESTRATOR.md#1113-tight-integration-playbook-steps-0-15.
    domain: mcp
    id: E6
    milestones:
      - id: M6.1
        status: done
        tasks:
          - dependencies: []
            domain: mcp
            exit_criteria:
              - critic:tests
              - artifact:tests/test_mcp_tools.py
              - "Guardrail: integration suite enforces blue/green safety
                invariants (no unhandled throws, DRY_RUN parity)"
            id: T6.1.1
            status: done
            title: MCP server integration tests (all 25 tools across both providers)
          - dependencies: []
            domain: mcp
            exit_criteria:
              - critic:manager_self_check
              - artifact:experiments/mcp/failover_test.json
              - "Guardrail: circuit-breaker rollback and DISABLE_NEW kill switch
                verified under simulated failures"
            id: T6.1.2
            status: done
            title: Provider failover testing (token limit simulation & automatic switching)
          - dependencies: []
            domain: mcp
            exit_criteria:
              - critic:tests
              - artifact:tests/test_state_persistence.py
              - "Guardrail: recovery flow preserves upgrade locks and safety
                state without manual intervention"
            id: T6.1.3
            status: done
            title: State persistence testing (checkpoint recovery across sessions)
          - dependencies: []
            domain: mcp
            exit_criteria:
              - critic:manager_self_check
              - artifact:state/quality/assessment_log.json
              - "Guardrail: quality checks confirm run-safety metrics from
                blue/green playbook remain green"
            id: T6.1.4
            status: done
            title: Quality framework validation (10 dimensions operational)
        title: Core Infrastructure Validation
      - id: M6.2
        status: done
        tasks:
          - dependencies: []
            domain: mcp
            exit_criteria:
              - critic:security
              - doc:docs/SECURITY_AUDIT.md
              - "Guardrail: audit verifies secrets handling inside blue/green
                upgrade flow and DRY_RUN constraints"
            id: T6.2.1
            status: done
            title: Credentials security audit (auth.json, API keys, token rotation)
          - dependencies: []
            domain: mcp
            exit_criteria:
              - critic:tests
              - artifact:experiments/mcp/error_recovery.json
              - "Guardrail: automated rollback path exercised with observation
                window + DISABLE_NEW reset"
            id: T6.2.2
            status: done
            title: Error recovery testing (graceful degradation, retry logic)
          - dependencies: []
            domain: mcp
            exit_criteria:
              - critic:data_quality
              - artifact:shared/contracts/*.schema.json
              - "Guardrail: dual-write / expand-cutover-contract workflow logged
                for 100% safe migrations"
            id: T6.2.3
            status: done
            title: Schema validation enforcement (all data contracts validated)
          - dependencies: []
            domain: mcp
            exit_criteria:
              - critic:allocator
              - unknown
              - "Guardrail: rate-limit handling respects worker timeouts and
                prevents cascading failures during upgrades"
            id: T6.2.4
            status: done
            title: API rate limiting & exponential backoff (Open-Meteo, Shopify, Ads APIs)
        title: Security & Reliability Hardening
      - id: M6.3
        status: done
        tasks:
          - dependencies: []
            domain: mcp
            exit_criteria:
              - critic:cost_perf
              - artifact:experiments/mcp/performance_benchmarks.json
              - "Guardrail: benchmarks include worker swap scenarios and confirm
                resource limits (timeouts, RSS) hold"
            id: T6.3.1
            status: done
            title: Performance benchmarking (MCP overhead, checkpoint size, token usage)
          - dependencies: []
            domain: mcp
            exit_criteria:
              - critic:manager_self_check
              - artifact:state/telemetry/metrics_summary.json
              - "Guardrail: telemetry captures Step 0–15 safety signals with
                alerting on breaches"
            id: T6.3.2
            status: done
            title: Enhanced observability export (structured logs, metrics dashboards)
          - dependencies: []
            domain: mcp
            exit_criteria:
              - critic:manager_self_check
              - artifact:experiments/mcp/autopilot_e2e.json
              - "Guardrail: autonomous loop validates automatic promotion +
                rollback without manual resets"
            id: T6.3.3
            status: done
            title: Autopilot loop end-to-end testing (full autonomous cycle validation)
        title: Observability & Performance
      - id: M6.4
        status: pending
        tasks:
          - dependencies: []
            description: 'Define upgrade preflight: clean git, version sanity, ≥500MB disk,
              SQLite lock probe, and single-flight upgrade.lock. Gate promotion
              through build → unit → selfchecks → canary, aborting with
              {error:"upgrade_aborted"} on any failure.'
            domain: mcp
            exit_criteria:
              - state/upgrade.lock created before work and removed on exit
              - Preflight script validates git status, Node/npm versions, disk
                ≥500MB, sandbox availability
              - Four-step gate recorded in logs; any failure returns
                {error:"upgrade_aborted"}
            id: T6.4.0
            status: done
            title: Upgrade invariants & preflight guardrails
          - dependencies: []
            description: >
              Replace environment toggles with a SQLite-backed `settings` table,
              seed

              defaults, and hot-refresh cached flags (≤500 ms poll). Include a

              `DISABLE_NEW` global kill switch that forces legacy behaviour
              instantly.
            domain: mcp
            exit_criteria:
              - settings table created with defaults + DISABLE_NEW
              - LiveFlags poller refreshes in-memory cache during runtime
              - Integration test flips PROMPT_MODE without restart
            id: T6.4.1
            status: done
            title: Live feature flag store with kill switch
          - dependencies: []
            description: >
              Keep the MCP front-end process stable while managing active and
              canary

              worker children over IPC. Ensure requests route through a proxy
              that can

              atomically switch to the validated canary without disconnecting
              clients.
            domain: mcp
            exit_criteria:
              - WorkerManager exposes startActive/startCanary/switchToCanary
              - Front-end tool handlers call workers.getActive().call(...)
              - RPC protocol enforces ready handshake, 30s timeouts, and
                structured {ok,error} results
              - Test demonstrates zero-downtime swap between worker binaries
            id: T6.4.2
            status: done
            title: Blue/green worker manager & front-end proxy
          - dependencies: []
            description: >
              Implement a dedicated worker entry that routes RPCs, enforces
              DRY_RUN=1

              by opening the state DB read-only, refuses mutating calls, and
              confirms

              legacy behaviour when DRY_RUN=0.
            domain: mcp
            exit_criteria:
              - Route function covers
                health/plan/dispatch/runTool/verify/report.mo
              - SQLite opened via file:state/state.db?mode=ro when DRY_RUN=1
              - applyPatch/mutate operations rejected while DRY_RUN=1
              - tests/test_worker_dry_run.py captures read-only guarantees
            id: T6.4.3
            status: done
            title: Worker entrypoint with DRY_RUN safeguards
          - dependencies: []
            description: >
              Automate the upgrade flow: create a separate git worktree,
              build/test new

              code, spawn a DRY_RUN canary, run shadow health/plan/report
              checks, then

              promote only if outputs match expectations.
            domain: mcp
            exit_criteria:
              - scripts/mcp_safe_upgrade.sh orchestrates worktree build + tests
              - Shadow checks compare active vs canary outputs in logs
              - Promotion flow documents gate order and staged routing (DRY →
                live) with metrics snapshots
              - experiments/mcp/upgrade/<ts>/report.json recorded for each run
            id: T6.4.4
            status: done
            title: Canary upgrade harness & shadow validation
          - dependencies: []
            description: >
              Gate compact prompt headers, sandbox pooling, scheduler WSJF mode,

              selective tests, danger gates, and MO engine behind live flags so
              they

              only activate after successful canary validation.
            domain: mcp
            exit_criteria:
              - PROMPT_MODE, SANDBOX_MODE, SCHEDULER_MODE, SELECTIVE_TESTS,
                DANGER_GATES, MO_ENGINE read from LiveFlags
              - Regression fixtures cover legacy vs new mode per feature
              - docs/MCP_ORCHESTRATOR.md updated with flag toggle order
            id: T6.4.5
            status: pending
            title: Feature flag gating for compact prompts & sandbox pool
          - dependencies: []
            description: >
              Ensure tool surfaces remain stable while routing to v1/v2 handlers
              based

              on flags. Provide an MCP admin tool or CLI to update settings
              atomically

              without restarts.
            domain: mcp
            exit_criteria:
              - Tool handlers return 'disabled' until corresponding flag enabled
              - settings.update, upgrade.applyPatch, route.switch commands
                exposed with structured errors
              - Operator guide added under docs/MCP_AUTOMATION.md#live-flags
            id: T6.4.6
            status: pending
            title: Runtime tool registration & admin flag controls
          - dependencies: []
            description: >
              Add health monitoring that reverts to the previous worker and
              resets

              flags when error rates spike post-promotion. Document on-call
              rollback

              steps and ensure DISABLE_NEW restores legacy behaviour.
            domain: mcp
            exit_criteria:
              - Heartbeat every 2s with 3-strike circuit breaker routes back to
                standby
              - Error budget (5%/2min) and SLO monitors trigger automatic
                rollback
              - DISABLE_NEW flag automatically flipped during rollback
              - docs/MCP_ORCHESTRATOR.md includes rollback playbook
            id: T6.4.7
            status: done
            title: Automatic rollback monitors & kill-switch reset
          - dependencies: []
            description: >
              Emit OTel spans (or structured JSON logs) for every worker call
              with

              timing, lane, task, and outcome metadata. Enforce concurrency,
              timeout,

              and RSS guards to prevent runaway resource usage.
            domain: mcp
            exit_criteria:
              - Span/log attributes include method, lane, ok/error, duration,
                task.id
              - runTool/plan timeouts (30s/120s) & lane concurrency limits
                enforced
              - RSS watchdog throttles batch lane when >1.5x baseline
            id: T6.4.8
            status: done
            title: Observability & resource budgets during upgrade
        title: Zero-downtime self-upgrade
    status: done
    title: MCP Orchestrator Production Readiness
  - description: Complete geocoding integration, weather feature joins, and data
      quality validation.
    domain: product
    id: E7
    milestones:
      - id: M7.1
        status: done
        tasks:
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:data_quality
              - unknown
            id: T7.1.1
            status: done
            title: Complete geocoding integration (city->lat/lon, cache strategy)
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:leakage
              - artifact:experiments/features/weather_join_validation.json
            id: T7.1.2
            status: done
            title: Weather feature join to model matrix (prevent future leakage)
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:data_quality
              - unknown
            id: T7.1.3
            status: done
            title: Data contract schema validation (Shopify, weather, ads)
        title: Geocoding & Weather Integration
      - id: M7.2
        status: done
        tasks:
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:data_quality
              - unknown
            id: T7.2.1
            status: done
            title: Incremental ingestion with deduplication & checkpointing
          - dependencies: []
            domain: product
            exit_criteria:
              - critic:data_quality
              - artifact:state/dq_monitoring.json
            id: T7.2.2
            status: done
            title: Data quality monitoring & alerting (anomaly detection)
        title: Pipeline Robustness
    status: done
    title: Data Pipeline Hardening
  - description: Critical production readiness tasks for MCP orchestrator. Complete
      before WeatherVane v1 launch while maintaining the Step 0–15 run-safety
      guardrails
      (docs/MCP_ORCHESTRATOR.md#1113-tight-integration-playbook-steps-0-15).
    domain: mcp
    id: E8
    milestones:
      - id: M8.1
        status: done
        tasks:
          - dependencies: []
            domain: mcp
            exit_criteria:
              - utils/schema.ts returns schema.shape with guardrail comment
              - MCP entrypoints register raw shapes only
              - Autopilot documentation updated to reflect guardrail
              - critic:build passes
              - "Guardrail: validation confirms schema handling does not weaken
                blue/green safety gates"
            id: T8.1.1
            status: done
            title: "Lock MCP schemas to Zod shapes (SAFE: guardrail)"
          - dependencies: []
            domain: mcp
            exit_criteria:
              - ALLOWED_COMMANDS constant defined
              - isCommandAllowed() enforced before execution
              - Deny-list kept as secondary check
              - critic:tests passes with new test_command_allowlist.py
              - critic:manager_self_check passes
              - "Guardrail: allow-list integration verified against blue/green
                upgrade scenarios"
            id: T8.1.2
            status: done
            title: "Implement command allow-list in guardrails (SAFE: additive security)"
          - dependencies: []
            domain: mcp
            exit_criteria:
              - All tool handlers generate correlationId
              - All state transitions include correlationId
              - Events in SQLite include correlation_id column populated
              - critic:manager_self_check passes
              - End-to-end trace visible in state/orchestrator.db
              - "Guardrail: correlation IDs trace compliance with Step 0–15
                safety checks"
            id: T8.1.3
            status: done
            title: "Thread correlation IDs through state transitions (SAFE: observability
              only)"
        title: MCP Compliance & Security
      - id: M8.2
        status: done
        tasks:
          - dependencies: []
            domain: mcp
            exit_criteria:
              - formatForPromptCompact() returns JSON evidence pack
              - unknown
              - All coordinator calls use compact mode
              - critic:build passes
              - critic:manager_self_check passes
              - unknown
              - "Guardrail: compact mode flip integrated with Step 15 staged
                flag process"
            id: T8.2.1
            status: done
            title: "Implement compact evidence-pack prompt mode (SAFE: new function,
              backward compatible)"
          - dependencies: []
            domain: mcp
            exit_criteria:
              - orchestrator_status tool shows coordinator type and availability
              - Telemetry includes coordinator field in execution logs
              - Documentation updated in IMPLEMENTATION_STATUS.md
              - critic:manager_self_check passes
              - Failover behavior visible and logged
              - "Guardrail: failover reporting feeds SLO/error budget monitors
                for auto rollback"
            id: T8.2.2
            status: done
            title: "Finalize Claude↔Codex coordinator failover (SAFE: expose existing
              functionality)"
        title: Context & Performance Optimization
    status: done
    title: PHASE-4-POLISH — MCP Production Hardening
  - description: Post-v1 performance improvements and production observability. High
      ROI optimizations that still honour the blue/green guardrail contract.
    domain: mcp
    id: E9
    milestones:
      - id: M9.1
        status: done
        tasks:
          - dependencies: []
            domain: mcp
            exit_criteria:
              - standardPromptHeader() returns deterministic header
              - All prompts include standard header
              - Header enables provider caching (verified with API logs)
              - critic:cost_perf shows token cache hit rate
              - critic:manager_self_check passes
              - "Guardrail: caching rollout assessed via Step 0–15 safety checks
                before staying live"
            id: T9.1.1
            status: done
            title: "Stable prompt headers with provider caching (SAFE: additive
              optimization)"
          - dependencies: []
            domain: mcp
            exit_criteria:
              - Priority queue with 3 lanes operational
              - Semaphore limits enforced per lane
              - Interactive tasks always get priority
              - critic:tests passes
              - critic:manager_self_check passes
              - "Guardrail: queue respects worker concurrency caps from
                blue/green playbook"
            id: T9.1.2
            status: done
            title: "Batch queue for non-urgent prompts (SAFE: new queueing system)"
        title: Cost Optimization & Caching
      - id: M9.2
        status: pending
        tasks:
          - dependencies: []
            domain: mcp
            exit_criteria:
              - validateDiff() rejects non-diff outputs
              - validateJSON() rejects invalid JSON
              - Retry rate reduction measured
              - critic:tests passes
              - "Guardrail: validation enforced in canary shadow runs before
                live promotion"
            id: T9.2.1
            status: done
            title: "Strict output DSL validation (SAFE: validation layer only)"
          - dependencies: []
            domain: mcp
            exit_criteria:
              - Idempotency cache operational
              - Duplicate operations return cached results
              - 1-hour TTL enforced
              - critic:tests passes
              - "Guardrail: cache respects DRY_RUN mode and avoids side effects
                during canary runs"
            id: T9.2.2
            status: done
            title: "Idempotency keys for mutating tools (SAFE: caching layer)"
        title: Reliability & Quality Improvements
      - id: M9.3
        status: pending
        tasks:
          - dependencies: []
            domain: mcp
            exit_criteria:
              - All tool handlers instrumented
              - Spans exported to tracing backend
              - End-to-end traces visible
              - Performance insights available
              - critic:manager_self_check passes
              - "Guardrail: telemetry alerts on Step 0–15 safety breaches"
            id: T9.3.1
            status: done
            title: "OpenTelemetry spans for all operations (SAFE: tracing wrapper)"
          - dependencies: []
            domain: mcp
            exit_criteria:
              - Sandbox pool with 3 pre-warmed containers
              - Test execution uses pooled sandboxes
              - 10x speedup measured
              - Fallback to non-pooled works
              - critic:tests passes
              - "Guardrail: pool enforces DRY_RUN read-only mode during canary
                validation"
            id: T9.3.2
            status: pending
            title: "Sandbox pooling for test execution (SAFE: new executor)"
        title: Production Observability
      - id: M9.4
        status: pending
        tasks:
          - dependencies: []
            domain: mcp
            exit_criteria:
              - code_fts virtual table created
              - Index populated on repo sync
              - Search performance <50ms
              - critic:tests passes
            id: T9.4.1
            status: done
            title: "SQLite FTS5 index for code search (SAFE: new index)"
          - dependencies: []
            domain: mcp
            exit_criteria:
              - tsserver and pyright proxies running
              - lsp.definition and lsp.references tools work
              - Context assembler uses LSP for code slices
              - Context relevance measured and improved
              - critic:tests passes
              - "Guardrail: LSP tools routed through worker proxy with Step 0–15
                safety enforcement"
            id: T9.4.2
            status: pending
            title: "LSP proxy tools for symbol-aware context (SAFE: new tools)"
        title: Advanced Context & Search
    status: pending
    title: PHASE-5-OPTIMIZATION — Performance & Observability
