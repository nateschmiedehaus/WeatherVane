epics:
  - id: E-AUTOPILOT-BOOTSTRAP
    title: Autopilot Bootstrap
    status: in_progress
    domain: mcp
    milestones:
      - id: E-AUTOPILOT-BOOTSTRAP-mvp
        title: MVP Bootstrap Stream
        status: in_progress
        tasks:
          - id: AFP-ROADMAP-FAST-AUTONOMY-AUDIT
            title: Audit roadmap for fast autonomy + self-improvement
            status: pending
            dependencies: []
            exit_criteria:
              - Roadmap restructured for critical path to autonomy
              - Parallel execution opportunities identified
              - Self-editing capability tasks added
              - Self-improvement loop tasks added
              - Autopilot can execute roadmap autonomously after bootstrap
            domain: mcp
            description: |
              Audit and restructure roadmap to achieve:

              1. FAST AUTONOMY (weeks not months):
                 - Identify critical path to working autopilot
                 - Reorder tasks for parallel execution where possible
                 - Remove or defer blockers that slow bootstrap
                 - Ensure scaffolds (supervisor/agents/libs/adapters) can build concurrently

              2. SELF-EDITING CAPABILITY:
                 - Add tasks for autopilot to modify its own roadmap
                 - Define roadmap schema validation
                 - Implement roadmap mutation API with guardrails
                 - Enable autopilot to add/remove/reorder tasks

              3. SELF-IMPROVEMENT LOOP:
                 - Add tasks for autopilot to improve its own code
                 - Define quality metrics autopilot tracks
                 - Implement feedback loop (execute → measure → learn → improve)
                 - Enable autopilot to refactor its own scaffolds

              4. AUTONOMOUS EXECUTION:
                 - Validate entire roadmap is executable by autopilot
                 - Add missing tasks for full autonomy
                 - Remove tasks that require human intervention
                 - Ensure autopilot can truck through roadmap independently

              Deliverables:
              - Updated roadmap.yaml with restructured dependencies
              - Critical path analysis (what blocks autonomy)
              - Parallel execution plan (4 scaffolds → weeks not months)
              - Self-improvement task additions
              - Evidence of fast path (timeline with milestones)

              Success criteria:
              - Autopilot bootstrap achievable in <4 weeks
              - 4 scaffolds buildable in parallel (not sequential)
              - Self-editing capability in roadmap
              - Self-improvement loop in roadmap
              - Autopilot can execute 90%+ of roadmap without human intervention
          - id: AFP-AUTOPILOT-MVP-STRANGLER-20251115
            title: Autopilot MVP clean-room integration
            status: blocked
            dependencies:
              - AFP-MVP-SUPERVISOR-SCAFFOLD
              - AFP-MVP-AGENTS-SCAFFOLD
              - AFP-MVP-LIBS-SCAFFOLD
              - AFP-MVP-ADAPTERS-SCAFFOLD
            exit_criteria: []
            domain: mcp
            description: Meta task aggregating supervisor/agents/libs/adapters scaffolds;
              closes after demo passes CI.
          - id: AFP-MVP-SUPERVISOR-SCAFFOLD
            title: MVP Supervisor scaffold
            status: pending
            dependencies: []
            exit_criteria: []
            domain: mcp
            description: Introduce autopilot_mvp/supervisor with lease management and
              evidence harness stubs.
          - id: AFP-MVP-AGENTS-SCAFFOLD
            title: MVP Agents scaffold
            status: pending
            dependencies: []
            exit_criteria: []
            domain: mcp
            description: Stub planner, builder, reviewer, SRE agents referencing shared
              concepts.
          - id: AFP-MVP-LIBS-SCAFFOLD
            title: MVP Libs scaffold
            status: pending
            dependencies: []
            exit_criteria: []
            domain: mcp
            description: Shared MVP utilities (telemetry hooks, common types).
          - id: AFP-MVP-ADAPTERS-SCAFFOLD
            title: MVP Adapters scaffold
            status: pending
            dependencies: []
            exit_criteria: []
            domain: mcp
            description: Adapters bridging legacy orchestrator paths with autopilot_mvp
              stubs; TTL 90 days.
          - id: AFP-DPS-BUILD-20251116
            title: Dynamic Prompt System build
            status: blocked
            dependencies:
              - AFP-MVP-SUPERVISOR-SCAFFOLD
              - AFP-MVP-AGENTS-SCAFFOLD
            exit_criteria: []
            domain: mcp
            description: Scaffold DPS templates/registry once supervisor + agent stubs
              exist.
          - id: AFP-MEMORY-CORE-20251117
            title: Memory core bootstrap
            status: blocked
            dependencies:
              - AFP-DPS-BUILD-20251116
            exit_criteria: []
            domain: mcp
            description: Semantic memory and lesson registry once DPS schemas compile.
          - id: CRIT-PERF-TESTS-fcee61
            title: "[Critic:tests] Restore performance"
            status: in_progress
            dependencies: []
            exit_criteria: []
            domain: product
            description: >-
              Critic tests is underperforming and needs immediate remediation.


              Identity: Regression Hunter (quality, authority blocking)

              Mission: Keep the test suites healthy and ensure deterministic
              results across flows.

              Signature powers: Surfaces flaky suites and failing assertions
              with reproduction notes.; Synthesizes minimal repro commands for
              Autopilot triage.

              Autonomy guidance: Rerun targeted suites automatically; lean on
              Autopilot only when new failures persist.


              No successful runs recorded in the last 6 observations; 6
              consecutive failures detected.


              Observation window: 6 runs


              Consecutive failures: 6


              Failures: 6 | Successes: 0


              Assigned to: Director Dana


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              == Python test suite ==

              ============================= test session starts
              ==============================

              platform darwin -- Python 3.10.12, pytest-8.4.2, pluggy-1.6.0

              rootdir:
              /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane

              configfile: pyproject.toml

              plugins: anyio-4.11.0, asyncio-1.2.0

              asyncio: mode=auto, debug=False,
              asyncio_default_fixture_loop_scope=None,
              asyncio_default_test_loop_scope=function

              collected 1134 items / 19 errors


              ==================================== ERRORS
              ====================================

              ____________ ERROR collecting
              tests/api/onboarding/test_progress.p...
          - id: CRIT-PERF-TESTS-fcee61.1
            title: Research and design for [Critic:tests] Restore performance
            status: pending
            dependencies: []
            exit_criteria: []
            domain: product
            description: "Research phase: Understand requirements and design approach for
              [Critic:tests] Restore performance"
          - id: CRIT-PERF-TESTS-fcee61.2
            title: Implement [Critic:tests] Restore performance
            status: pending
            dependencies:
              - CRIT-PERF-TESTS-fcee61.1
            exit_criteria: []
            domain: product
            description: "Implementation phase: Execute the plan for [Critic:tests] Restore
              performance"
          - id: CRIT-PERF-TESTS-fcee61.3
            title: Validate and test [Critic:tests] Restore performance
            status: pending
            dependencies:
              - CRIT-PERF-TESTS-fcee61.2
            exit_criteria: []
            domain: product
            description: "Validation phase: Test and verify [Critic:tests] Restore
              performance"
          - id: E-GENERAL.1
            title: Research and design for E-GENERAL
            status: pending
            dependencies: []
            exit_criteria: []
            domain: product
            description: "Research phase: Understand requirements and design approach for
              E-GENERAL"
          - id: E-GENERAL.2
            title: Implement E-GENERAL
            status: pending
            dependencies:
              - E-GENERAL.1
            exit_criteria: []
            domain: product
            description: "Implementation phase: Execute the plan for E-GENERAL"
          - id: E-GENERAL.3
            title: Validate and test E-GENERAL
            status: pending
            dependencies:
              - E-GENERAL.2
            exit_criteria: []
            domain: product
            description: "Validation phase: Test and verify E-GENERAL"
          - id: E-ML-REMEDIATION.1
            title: Research and design for ML Model Remediation - From Prototype to
              Production
            status: pending
            dependencies: []
            exit_criteria: []
            domain: product
            description: "Research phase: Understand requirements and design approach for ML
              Model Remediation - From Prototype to Production"
          - id: E-ML-REMEDIATION.2
            title: Implement ML Model Remediation - From Prototype to Production
            status: pending
            dependencies:
              - E-ML-REMEDIATION.1
            exit_criteria: []
            domain: product
            description: "Implementation phase: Execute the plan for ML Model Remediation -
              From Prototype to Production"
          - id: E-ML-REMEDIATION.3
            title: Validate and test ML Model Remediation - From Prototype to Production
            status: pending
            dependencies:
              - E-ML-REMEDIATION.2
            exit_criteria: []
            domain: product
            description: "Validation phase: Test and verify ML Model Remediation - From
              Prototype to Production"
          - id: REM-T-MLR-0.1.1
            title: Implementation verified
            status: pending
            dependencies: []
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Create ModelingReality critic with
              quantitative thresholds: Implementation verified"
          - id: REM-T-MLR-0.1.2
            title: Tests verified
            status: pending
            dependencies:
              - REM-T-MLR-0.1.1
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Create ModelingReality critic with
              quantitative thresholds: Tests verified"
          - id: REM-T-MLR-0.1.3
            title: Quality gate APPROVED
            status: pending
            dependencies:
              - REM-T-MLR-0.1.2
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Create ModelingReality critic with
              quantitative thresholds: Quality gate APPROVED"
          - id: REM-T-MLR-0.1.4
            title: Runtime verification PASSED
            status: pending
            dependencies:
              - REM-T-MLR-0.1.3
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Create ModelingReality critic with
              quantitative thresholds: Runtime verification PASSED"
          - id: REM-T-MLR-0.1.5
            title: Critical issues fixed
            status: pending
            dependencies:
              - REM-T-MLR-0.1.4
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Create ModelingReality critic with
              quantitative thresholds: Critical issues fixed"
          - id: REM-T-MLR-0.2.1
            title: Implementation verified
            status: pending
            dependencies: []
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Update all ML task exit criteria with
              objective metrics: Implementation verified"
          - id: REM-T-MLR-0.2.2
            title: Tests verified
            status: pending
            dependencies:
              - REM-T-MLR-0.2.1
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Update all ML task exit criteria with
              objective metrics: Tests verified"
          - id: REM-T-MLR-0.2.3
            title: Quality gate APPROVED
            status: pending
            dependencies:
              - REM-T-MLR-0.2.2
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Update all ML task exit criteria with
              objective metrics: Quality gate APPROVED"
          - id: REM-T-MLR-0.2.4
            title: Runtime verification PASSED
            status: pending
            dependencies:
              - REM-T-MLR-0.2.3
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Update all ML task exit criteria with
              objective metrics: Runtime verification PASSED"
          - id: REM-T-MLR-0.2.5
            title: Critical issues fixed
            status: pending
            dependencies:
              - REM-T-MLR-0.2.4
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Update all ML task exit criteria with
              objective metrics: Critical issues fixed"
          - id: REM-T-MLR-1.1.1
            title: Implementation verified
            status: pending
            dependencies: []
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Debug and fix weather multiplier logic in
              data generator: Implementation verified"
          - id: REM-T-MLR-1.1.2
            title: Tests verified
            status: pending
            dependencies:
              - REM-T-MLR-1.1.1
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Debug and fix weather multiplier logic in
              data generator: Tests verified"
          - id: REM-T-MLR-1.1.3
            title: Quality gate APPROVED
            status: pending
            dependencies:
              - REM-T-MLR-1.1.2
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Debug and fix weather multiplier logic in
              data generator: Quality gate APPROVED"
          - id: REM-T-MLR-1.1.4
            title: Runtime verification PASSED
            status: pending
            dependencies:
              - REM-T-MLR-1.1.3
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Debug and fix weather multiplier logic in
              data generator: Runtime verification PASSED"
          - id: REM-T-MLR-1.1.5
            title: Critical issues fixed
            status: pending
            dependencies:
              - REM-T-MLR-1.1.4
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Debug and fix weather multiplier logic in
              data generator: Critical issues fixed"
          - id: REM-T-MLR-1.3.1
            title: Implementation verified
            status: pending
            dependencies: []
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Create validation tests for synthetic data
              quality: Implementation verified"
          - id: REM-T-MLR-1.3.2
            title: Tests verified
            status: pending
            dependencies:
              - REM-T-MLR-1.3.1
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Create validation tests for synthetic data
              quality: Tests verified"
          - id: REM-T-MLR-1.3.3
            title: Quality gate APPROVED
            status: pending
            dependencies:
              - REM-T-MLR-1.3.2
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Create validation tests for synthetic data
              quality: Quality gate APPROVED"
          - id: REM-T-MLR-1.3.4
            title: Runtime verification PASSED
            status: pending
            dependencies:
              - REM-T-MLR-1.3.3
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Create validation tests for synthetic data
              quality: Runtime verification PASSED"
          - id: REM-T-MLR-1.3.5
            title: Critical issues fixed
            status: pending
            dependencies:
              - REM-T-MLR-1.3.4
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Create validation tests for synthetic data
              quality: Critical issues fixed"
          - id: REM-T-MLR-2.1.1
            title: Implementation verified
            status: pending
            dependencies: []
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Implement proper train/val/test splitting
              with no leakage: Implementation verified"
          - id: REM-T-MLR-2.1.2
            title: Tests verified
            status: pending
            dependencies:
              - REM-T-MLR-2.1.1
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Implement proper train/val/test splitting
              with no leakage: Tests verified"
          - id: REM-T-MLR-2.1.3
            title: Quality gate APPROVED
            status: pending
            dependencies:
              - REM-T-MLR-2.1.2
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Implement proper train/val/test splitting
              with no leakage: Quality gate APPROVED"
          - id: REM-T-MLR-2.1.4
            title: Runtime verification PASSED
            status: pending
            dependencies:
              - REM-T-MLR-2.1.3
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Implement proper train/val/test splitting
              with no leakage: Runtime verification PASSED"
          - id: REM-T-MLR-2.1.5
            title: Critical issues fixed
            status: pending
            dependencies:
              - REM-T-MLR-2.1.4
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Implement proper train/val/test splitting
              with no leakage: Critical issues fixed"
          - id: REM-T-MLR-2.2.1
            title: Implementation verified
            status: pending
            dependencies: []
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Implement LightweightMMM with weather
              features: Implementation verified"
          - id: REM-T-MLR-2.2.2
            title: Tests verified
            status: pending
            dependencies:
              - REM-T-MLR-2.2.1
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Implement LightweightMMM with weather
              features: Tests verified"
          - id: REM-T-MLR-2.2.3
            title: Quality gate APPROVED
            status: pending
            dependencies:
              - REM-T-MLR-2.2.2
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Implement LightweightMMM with weather
              features: Quality gate APPROVED"
          - id: REM-T-MLR-2.2.4
            title: Runtime verification PASSED
            status: pending
            dependencies:
              - REM-T-MLR-2.2.3
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Implement LightweightMMM with weather
              features: Runtime verification PASSED"
          - id: REM-T-MLR-2.2.5
            title: Critical issues fixed
            status: pending
            dependencies:
              - REM-T-MLR-2.2.4
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Implement LightweightMMM with weather
              features: Critical issues fixed"
          - id: REM-T-MLR-2.5.1
            title: Implementation verified
            status: pending
            dependencies: []
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Compare models to baseline
              (naive/seasonal/linear): Implementation verified"
          - id: REM-T-MLR-2.5.2
            title: Tests verified
            status: pending
            dependencies:
              - REM-T-MLR-2.5.1
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Compare models to baseline
              (naive/seasonal/linear): Tests verified"
          - id: REM-T-MLR-2.5.3
            title: Quality gate APPROVED
            status: pending
            dependencies:
              - REM-T-MLR-2.5.2
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Compare models to baseline
              (naive/seasonal/linear): Quality gate APPROVED"
          - id: REM-T-MLR-2.5.4
            title: Runtime verification PASSED
            status: pending
            dependencies:
              - REM-T-MLR-2.5.3
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Compare models to baseline
              (naive/seasonal/linear): Runtime verification PASSED"
          - id: REM-T-MLR-2.5.5
            title: Critical issues fixed
            status: pending
            dependencies:
              - REM-T-MLR-2.5.4
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Compare models to baseline
              (naive/seasonal/linear): Critical issues fixed"
          - id: REM-T-MLR-3.2.1
            title: Implementation verified
            status: pending
            dependencies: []
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Write comprehensive ML validation
              documentation: Implementation verified"
          - id: REM-T-MLR-3.2.2
            title: Tests verified
            status: pending
            dependencies:
              - REM-T-MLR-3.2.1
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Write comprehensive ML validation
              documentation: Tests verified"
          - id: REM-T-MLR-3.2.3
            title: Quality gate APPROVED
            status: pending
            dependencies:
              - REM-T-MLR-3.2.2
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Write comprehensive ML validation
              documentation: Quality gate APPROVED"
          - id: REM-T-MLR-3.2.4
            title: Runtime verification PASSED
            status: pending
            dependencies:
              - REM-T-MLR-3.2.3
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Write comprehensive ML validation
              documentation: Runtime verification PASSED"
          - id: REM-T-MLR-3.2.5
            title: Critical issues fixed
            status: pending
            dependencies:
              - REM-T-MLR-3.2.4
            exit_criteria: []
            domain: product
            description: "Part of [REM] Verify: Write comprehensive ML validation
              documentation: Critical issues fixed"
          - id: T-MLR-0.3.1
            title: artifact:docs/ML_QUALITY_STANDARDS.md
            status: pending
            dependencies: []
            exit_criteria: []
            domain: product
            description: "Part of Document world-class quality standards for ML:
              artifact:docs/ML_QUALITY_STANDARDS.md"
          - id: T-MLR-0.3.2
            title: verification:Numeric thresholds for all metrics
            status: pending
            dependencies:
              - T-MLR-0.3.1
            exit_criteria: []
            domain: product
            description: "Part of Document world-class quality standards for ML:
              verification:Numeric thresholds for all metrics"
          - id: T-MLR-0.3.3
            title: verification:Baseline comparison requirements
            status: pending
            dependencies:
              - T-MLR-0.3.2
            exit_criteria: []
            domain: product
            description: "Part of Document world-class quality standards for ML:
              verification:Baseline comparison requirements"
          - id: T-MLR-0.3.4
            title: review:External ML practitioner peer review
            status: pending
            dependencies:
              - T-MLR-0.3.3
            exit_criteria: []
            domain: product
            description: "Part of Document world-class quality standards for ML:
              review:External ML practitioner peer review"
          - id: T-MLR-1.2.1
            title: artifact:storage/seeds/synthetic_v2/*.parquet (20 files)
            status: pending
            dependencies: []
            exit_criteria: []
            domain: product
            description: "Part of Generate 3 years of synthetic data for 20 tenants:
              artifact:storage/seeds/synthetic_v2/*.parquet (20 files)"
          - id: T-MLR-1.2.2
            title: metric:total_rows = 219000
            status: pending
            dependencies:
              - T-MLR-1.2.1
            exit_criteria: []
            domain: product
            description: "Part of Generate 3 years of synthetic data for 20 tenants:
              metric:total_rows = 219000"
          - id: T-MLR-1.2.3
            title: metric:date_range = 2022-01-01 to 2024-12-31
            status: pending
            dependencies:
              - T-MLR-1.2.2
            exit_criteria: []
            domain: product
            description: "Part of Generate 3 years of synthetic data for 20 tenants:
              metric:date_range = 2022-01-01 to 2024-12-31"
          - id: T-MLR-1.2.4
            title: metric:weather_correlations_within_target >= 0.90
            status: pending
            dependencies:
              - T-MLR-1.2.3
            exit_criteria: []
            domain: product
            description: "Part of Generate 3 years of synthetic data for 20 tenants:
              metric:weather_correlations_within_target >= 0.90"
          - id: T-MLR-1.2.5
            title: test:pytest tests/data_gen/test_synthetic_v2_quality.py
            status: pending
            dependencies:
              - T-MLR-1.2.4
            exit_criteria: []
            domain: product
            description: "Part of Generate 3 years of synthetic data for 20 tenants:
              test:pytest tests/data_gen/test_synthetic_v2_quality.py"
          - id: T-MLR-1.2.6
            title: critic:data_quality
            status: pending
            dependencies:
              - T-MLR-1.2.5
            exit_criteria: []
            domain: product
            description: "Part of Generate 3 years of synthetic data for 20 tenants:
              critic:data_quality"
          - id: T-MLR-2.4.1
            title: artifact:experiments/mmm_v2/validation_report.json
            status: pending
            dependencies: []
            exit_criteria: []
            domain: product
            description: "Part of Validate model performance against objective thresholds:
              artifact:experiments/mmm_v2/validation_report.json"
          - id: T-MLR-2.4.2
            title: metric:weather_sensitive_r2_pass_rate >= 0.80
            status: pending
            dependencies:
              - T-MLR-2.4.1
            exit_criteria: []
            domain: product
            description: "Part of Validate model performance against objective thresholds:
              metric:weather_sensitive_r2_pass_rate >= 0.80"
          - id: T-MLR-2.4.3
            title: metric:weather_elasticity_sign_correct = 1.0
            status: pending
            dependencies:
              - T-MLR-2.4.2
            exit_criteria: []
            domain: product
            description: "Part of Validate model performance against objective thresholds:
              metric:weather_elasticity_sign_correct = 1.0"
          - id: T-MLR-2.4.4
            title: metric:no_overfitting_detected = true
            status: pending
            dependencies:
              - T-MLR-2.4.3
            exit_criteria: []
            domain: product
            description: "Part of Validate model performance against objective thresholds:
              metric:no_overfitting_detected = true"
          - id: T-MLR-2.4.5
            title: critic:modeling_reality_v2
            status: pending
            dependencies:
              - T-MLR-2.4.4
            exit_criteria: []
            domain: product
            description: "Part of Validate model performance against objective thresholds:
              critic:modeling_reality_v2"
          - id: T-MLR-2.4.6
            title: critic:academic_rigor
            status: pending
            dependencies:
              - T-MLR-2.4.5
            exit_criteria: []
            domain: product
            description: "Part of Validate model performance against objective thresholds:
              critic:academic_rigor"
          - id: T-MLR-2.6.1
            title: artifact:experiments/mmm_v2/robustness_report.json
            status: pending
            dependencies: []
            exit_criteria: []
            domain: product
            description: "Part of Run robustness tests (outliers, missing data, edge cases):
              artifact:experiments/mmm_v2/robustness_report.json"
          - id: T-MLR-2.6.2
            title: test:Model handles extreme weather without crash
            status: pending
            dependencies:
              - T-MLR-2.6.1
            exit_criteria: []
            domain: product
            description: "Part of Run robustness tests (outliers, missing data, edge cases):
              test:Model handles extreme weather without crash"
          - id: T-MLR-2.6.3
            title: test:Model handles missing data gracefully
            status: pending
            dependencies:
              - T-MLR-2.6.2
            exit_criteria: []
            domain: product
            description: "Part of Run robustness tests (outliers, missing data, edge cases):
              test:Model handles missing data gracefully"
          - id: T-MLR-2.6.4
            title: test:Zero ad spend predicts organic baseline
            status: pending
            dependencies:
              - T-MLR-2.6.3
            exit_criteria: []
            domain: product
            description: "Part of Run robustness tests (outliers, missing data, edge cases):
              test:Zero ad spend predicts organic baseline"
          - id: T-MLR-2.6.5
            title: test:pytest tests/model/test_mmm_robustness.py (15/15 pass)
            status: pending
            dependencies:
              - T-MLR-2.6.4
            exit_criteria: []
            domain: product
            description: "Part of Run robustness tests (outliers, missing data, edge cases):
              test:pytest tests/model/test_mmm_robustness.py (15/15 pass)"
          - id: T-MLR-2.6.6
            title: critic:modeling_reality_v2
            status: pending
            dependencies:
              - T-MLR-2.6.5
            exit_criteria: []
            domain: product
            description: "Part of Run robustness tests (outliers, missing data, edge cases):
              critic:modeling_reality_v2"
          - id: T-MLR-3.1.1
            title: artifact:experiments/mmm_v2/validation_notebook.ipynb
            status: pending
            dependencies: []
            exit_criteria: []
            domain: product
            description: "Part of Create reproducible validation notebook:
              artifact:experiments/mmm_v2/validation_notebook.ipynb"
          - id: T-MLR-3.1.2
            title: test:Notebook runs end-to-end without errors
            status: pending
            dependencies:
              - T-MLR-3.1.1
            exit_criteria: []
            domain: product
            description: "Part of Create reproducible validation notebook: test:Notebook
              runs end-to-end without errors"
          - id: T-MLR-3.1.3
            title: test:Output matches claimed metrics
            status: pending
            dependencies:
              - T-MLR-3.1.2
            exit_criteria: []
            domain: product
            description: "Part of Create reproducible validation notebook: test:Output
              matches claimed metrics"
          - id: T-MLR-3.1.4
            title: artifact:experiments/mmm_v2/validation_notebook.html
            status: pending
            dependencies:
              - T-MLR-3.1.3
            exit_criteria: []
            domain: product
            description: "Part of Create reproducible validation notebook:
              artifact:experiments/mmm_v2/validation_notebook.html"
          - id: T-MLR-3.1.5
            title: critic:academic_rigor
            status: pending
            dependencies:
              - T-MLR-3.1.4
            exit_criteria: []
            domain: product
            description: "Part of Create reproducible validation notebook:
              critic:academic_rigor"
      - id: E-AUTOPILOT-BOOTSTRAP-agentic-review
        title: Agentic Review System
        status: pending
        tasks:
          - id: AFP-AGENTIC-REVIEW-RESEARCH
            title: Research agentic review architecture
            status: pending
            dependencies: []
            exit_criteria: []
            domain: mcp
            description: Research LLM-based dialogue systems, counter-argumentation patterns, and multi-agent debate frameworks for strategic review capability. REQUIRED for autopilot.
          - id: AFP-AGENTIC-REVIEW-DESIGN
            title: Design agentic review system
            status: pending
            dependencies:
              - AFP-AGENTIC-REVIEW-RESEARCH
            exit_criteria: []
            domain: mcp
            description: Design architecture separating agentic review from mechanical critics. Define LLM integration, dialogue protocol, belief updating mechanism, selective usage strategy.
          - id: AFP-AGENTIC-REVIEW-PILOT
            title: Pilot agentic reviewer implementation
            status: pending
            dependencies:
              - AFP-AGENTIC-REVIEW-DESIGN
            exit_criteria: []
            domain: mcp
            description: Implement pilot agentic reviewer (DesignReviewer candidate). Measure cost per review, latency, quality improvement vs mechanical checks. ROI analysis.
          - id: AFP-AGENTIC-REVIEW-AUTOPILOT-INTEGRATION
            title: Integrate agentic review with autopilot
            status: pending
            dependencies:
              - AFP-AGENTIC-REVIEW-PILOT
              - AFP-MVP-SUPERVISOR-SCAFFOLD
            exit_criteria: []
            domain: mcp
            description: Integrate agentic review into autopilot workflow. Define when to use (complex decisions, strategic planning) vs mechanical checks (routine validation). Selective deployment strategy.
          - id: AFP-AGENTIC-REVIEW-SCALE
            title: Scale agentic review deployment
            status: pending
            dependencies:
              - AFP-AGENTIC-REVIEW-AUTOPILOT-INTEGRATION
            exit_criteria: []
            domain: mcp
            description: Deploy agentic review to full autopilot. Monitor cost/latency/quality metrics. Establish SLAs and fallback to mechanical checks when needed. Production readiness.
      - id: E-AUTOPILOT-BOOTSTRAP-self-capabilities
        title: Self-Editing & Self-Improvement
        status: pending
        tasks:
          - id: AFP-ROADMAP-SCHEMA
            title: Define roadmap schema and validation rules
            status: pending
            dependencies: []
            exit_criteria:
              - TypeScript schema for roadmap structure
              - JSON Schema for YAML validation
              - Validation tests pass
            domain: mcp
            description: Define formal schema for roadmap.yaml structure to enable programmatic validation (prevent invalid states), type-safe API (TypeScript interfaces), and documentation (self-documenting schema). Deliverables - src/roadmap/schema.ts (TypeScript types), schema/roadmap.json (JSON Schema), tests validating current roadmap.yaml passes schema. Ensures autopilot cannot create invalid roadmap states.
          - id: AFP-ROADMAP-MUTATION-API
            title: Implement roadmap mutation API
            status: pending
            dependencies:
              - AFP-ROADMAP-SCHEMA
            exit_criteria:
              - API can add/remove/update tasks
              - API validates changes against schema
              - Tests cover all mutation operations
            domain: mcp
            description: Build API for autopilot to modify roadmap programmatically. API methods - addTask, removeTask, updateTask, reorderTasks, updateDependencies. All mutations validate against schema before applying, log to audit trail, persist to roadmap.yaml atomically. Enables autopilot to edit roadmap without manual YAML.
          - id: AFP-ROADMAP-GUARDRAILS
            title: Implement roadmap mutation guardrails
            status: pending
            dependencies:
              - AFP-ROADMAP-MUTATION-API
            exit_criteria:
              - Max tasks per run enforced (prevent infinite loops)
              - Circular dependency detection
              - Exit criteria required for new tasks
              - Tests verify guardrails block violations
            domain: mcp
            description: Prevent autopilot from creating pathological roadmap states. Guardrails - max tasks per mutation (10), circular dependencies (blocked via DAG validation), exit criteria (required for all tasks), rate limiting (max 100 mutations/day), audit trail (all changes logged to state/roadmap-changes.jsonl). Ensures autopilot cannot break roadmap with self-edits.
          - id: AFP-ROADMAP-VALIDATION
            title: Add pre-commit validation for roadmap
            status: pending
            dependencies:
              - AFP-ROADMAP-SCHEMA
              - AFP-ROADMAP-GUARDRAILS
            exit_criteria:
              - Git hook validates roadmap.yaml on commit
              - Validation runs in CI
              - Invalid roadmap blocks commit
            domain: mcp
            description: Prevent invalid roadmap from being committed. Validation checks - schema conformance (all required fields present), dependency DAG (no cycles), guardrails (max tasks, exit criteria), referenced tasks exist (dependencies are valid). Implementation in .githooks/pre-commit and .github/workflows/ci.yml. Ensures roadmap.yaml is always valid.
          - id: AFP-QUALITY-METRICS
            title: Define and track autopilot quality metrics
            status: pending
            dependencies: []
            exit_criteria:
              - Metrics defined (test coverage, critic pass rate, evidence completeness)
              - Tracking implemented (metrics saved per task)
              - Dashboard shows trends over time
            domain: mcp
            description: Track quality metrics to measure autopilot improvement. Metrics - test coverage (% of code covered), critic pass rate (% of tasks passing all critics first try), evidence completeness (% of tasks with full evidence bundle), build health (% of builds passing), execution time (average time per task). Storage in state/analytics/autopilot_metrics.jsonl (append-only log) and per-task metrics in evidence bundles. Enables measuring self-improvement effectiveness.
          - id: AFP-FEEDBACK-LOOP
            title: Implement execute → measure → learn → improve loop
            status: pending
            dependencies:
              - AFP-QUALITY-METRICS
              - AFP-MEMORY-CORE-20251117
            exit_criteria:
              - Autopilot records lessons from each task
              - Lessons stored in memory core
              - Lessons applied to future tasks
              - Improvement measurable via metrics
            domain: mcp
            description: Build feedback loop for continuous improvement. Loop stages - EXECUTE (autopilot completes task), MEASURE (quality metrics recorded), LEARN (patterns extracted - what worked/failed), IMPROVE (apply lessons to next task). Lesson format includes pattern, context, solution, evidence path, effectiveness (0.0-1.0). Enables autopilot to learn from experience.
          - id: AFP-SELF-REFACTOR
            title: Enable autopilot to refactor its own code
            status: pending
            dependencies:
              - AFP-FEEDBACK-LOOP
            exit_criteria:
              - Autopilot can identify refactoring opportunities
              - Autopilot can execute refactorings safely
              - Quality metrics improve after refactoring
              - All tests pass after refactoring
            domain: mcp
            description: Enable autopilot to improve its own implementation. Refactoring triggers - lesson learned (this pattern causes bugs), metric degradation (test coverage drops), complexity increase (function >50 lines), duplication detected (same code in 3+ places). Refactoring process - identify target, propose refactoring (via AFP work process), execute refactoring, verify improvement (metrics better + tests pass), record lesson. Safety - all changes go through AFP 10-phase process, critics must pass, tests must pass, metrics must improve or stay same. Enables autopilot to evolve its own codebase.
          - id: AFP-LESSON-PERSISTENCE
            title: Persist lessons across autopilot sessions
            status: pending
            dependencies:
              - AFP-MEMORY-CORE-20251117
              - AFP-FEEDBACK-LOOP
            exit_criteria:
              - Lessons survive autopilot restart
              - Lessons retrieved by pattern matching
              - Lesson effectiveness tracked over time
              - Low-effectiveness lessons pruned automatically
            domain: mcp
            description: Ensure lessons learned persist and improve. Storage in state/memory/lessons.jsonl (append-only), indexed by pattern for fast retrieval. Retrieval via pattern matching, context similarity (vector embeddings), effectiveness ranking (sort by success rate). Pruning - lessons with <20% effectiveness deleted, superseded lessons archived, conflicting lessons reconciled. Enables autopilot to accumulate knowledge.
  - id: E-ML-REMEDIATION
    title: ML Model Remediation - From Prototype to Production
    status: pending
    domain: modeling
    milestones:
      - id: M-MLR-0
        title: "Foundation: Truth & Accountability"
        status: pending
        tasks:
          - id: T-MLR-0.1
            title: Create ModelingReality critic with quantitative thresholds
            status: done
            dependencies: []
            exit_criteria:
              - artifact:tools/wvo_mcp/src/critics/modeling_reality_v2.ts
              - test:Critic FAILS when R² < 0.50
              - test:Critic FAILS when no baseline comparison
              - test:Critic FAILS when weather elasticity signs wrong
              - metric:critic_strictness = 1.0
              - critic:tests
            domain: modeling
            description: >
              Create critic that enforces quantitative thresholds: R² > 0.50,
              correct

              elasticity signs, baseline comparison required, no subjective
              judgment.
          - id: T-MLR-0.2
            title: Update all ML task exit criteria with objective metrics
            status: done
            dependencies: []
            exit_criteria:
              - artifact:state/roadmap.yaml (T12.*, T13.* updated)
              - verification:All ML tasks have "metric:r2 > 0.50"
              - verification:All ML tasks have "metric:beats_baseline > 1.10"
              - verification:All ML tasks have "critic:modeling_reality_v2"
            domain: modeling
          - id: T-MLR-0.3
            title: Document world-class quality standards for ML
            status: pending
            dependencies: []
            exit_criteria:
              - artifact:docs/ML_QUALITY_STANDARDS.md
              - verification:Numeric thresholds for all metrics
              - verification:Baseline comparison requirements
              - review:External ML practitioner peer review
            domain: modeling
      - id: M-MLR-1
        title: "Phase 1: Fix Synthetic Data (2 weeks)"
        status: pending
        tasks:
          - id: T-MLR-1.1
            title: Debug and fix weather multiplier logic in data generator
            status: done
            dependencies: []
            exit_criteria:
              - artifact:scripts/weather/generate_synthetic_tenants_v2.py
              - test:Extreme correlation = 0.85 ± 0.05
              - test:High correlation = 0.70 ± 0.05
              - test:Medium correlation = 0.40 ± 0.05
              - test:None correlation < 0.10
              - critic:data_quality
            domain: modeling
          - id: T-MLR-1.2
            title: Generate 3 years of synthetic data for 20 tenants
            status: pending
            dependencies: []
            exit_criteria:
              - artifact:storage/seeds/synthetic_v2/*.parquet (20 files)
              - metric:total_rows = 219000
              - metric:date_range = 2022-01-01 to 2024-12-31
              - metric:weather_correlations_within_target >= 0.90
              - test:pytest tests/data_gen/test_synthetic_v2_quality.py
              - critic:data_quality
            domain: modeling
          - id: T-MLR-1.3
            title: Create validation tests for synthetic data quality
            status: done
            dependencies: []
            exit_criteria:
              - artifact:tests/data_gen/test_synthetic_v2_quality.py
              - test:20/20 tenant tests pass
              - artifact:experiments/data_validation/correlation_plots.pdf
              - critic:tests
            domain: modeling
      - id: M-MLR-2
        title: "Phase 2: Rigorous MMM Training (3 weeks)"
        status: pending
        tasks:
          - id: T-MLR-2.1
            title: Implement proper train/val/test splitting with no leakage
            status: done
            dependencies: []
            exit_criteria:
              - artifact:shared/libs/modeling/time_series_split.py
              - test:Validation after training (no date overlap)
              - test:Test after validation (no date overlap)
              - test:Split percentages 70/15/15
              - critic:leakage
            domain: modeling
          - id: T-MLR-2.2
            title: Implement LightweightMMM with weather features
            status: done
            dependencies: []
            exit_criteria:
              - artifact:apps/model/mmm_lightweight_weather.py
              - test:Adstock transformation applied
              - test:Hill saturation curves applied
              - test:Weather interaction terms included
              - test:pytest tests/model/test_mmm_lightweight.py (12/12 pass)
              - critic:academic_rigor
            domain: modeling
          - id: T-MLR-2.4
            title: Validate model performance against objective thresholds
            status: pending
            dependencies: []
            exit_criteria:
              - artifact:experiments/mmm_v2/validation_report.json
              - metric:weather_sensitive_r2_pass_rate >= 0.80
              - metric:weather_elasticity_sign_correct = 1.0
              - metric:no_overfitting_detected = true
              - critic:modeling_reality_v2
              - critic:academic_rigor
            domain: modeling
          - id: T-MLR-2.5
            title: Compare models to baseline (naive/seasonal/linear)
            status: done
            dependencies: []
            exit_criteria:
              - artifact:experiments/mmm_v2/baseline_comparison.json
              - metric:beats_naive_by >= 1.10
              - metric:beats_seasonal_by >= 1.05
              - metric:beats_linear_by >= 1.05
              - artifact:experiments/mmm_v2/baseline_comparison_plots.pdf
              - critic:modeling_reality_v2
            domain: modeling
          - id: T-MLR-2.6
            title: Run robustness tests (outliers, missing data, edge cases)
            status: pending
            dependencies: []
            exit_criteria:
              - artifact:experiments/mmm_v2/robustness_report.json
              - test:Model handles extreme weather without crash
              - test:Model handles missing data gracefully
              - test:Zero ad spend predicts organic baseline
              - test:pytest tests/model/test_mmm_robustness.py (15/15 pass)
              - critic:modeling_reality_v2
            domain: modeling
      - id: M-MLR-3
        title: "Phase 3: Reproducibility & Documentation (1 week)"
        status: pending
        tasks:
          - id: T-MLR-3.1
            title: Create reproducible validation notebook
            status: pending
            dependencies: []
            exit_criteria:
              - artifact:experiments/mmm_v2/validation_notebook.ipynb
              - test:Notebook runs end-to-end without errors
              - test:Output matches claimed metrics
              - artifact:experiments/mmm_v2/validation_notebook.html
              - critic:academic_rigor
            domain: modeling
          - id: T-MLR-3.2
            title: Write comprehensive ML validation documentation
            status: done
            dependencies: []
            exit_criteria:
              - artifact:docs/ML_VALIDATION_COMPLETE.md
              - verification:Links to reproducible notebook
              - verification:Includes limitations section
              - verification:Includes baseline comparisons
              - review:External ML practitioner peer review
            domain: modeling
  - id: E-PHASE0
    title: "Phase 0: Measurement & Confidence"
    status: done
    domain: product
    milestones:
      - id: M0.1
        title: Measurement & Confidence Foundations
        status: done
        tasks:
          - id: T0.1.1
            title: Implement geo holdout plumbing
            status: done
            dependencies: []
            exit_criteria:
              - artifact:state/analytics/experiments/geo_holdouts/*.json
              - artifact:state/telemetry/experiments/geo_holdout_runs.jsonl
              - critic:data_quality
            domain: product
            description: Wire apps/validation/incrementality.py into ingestion runs with
              nightly job execution
          - id: T0.1.2
            title: Build lift & confidence UI surfaces
            status: done
            dependencies: []
            exit_criteria:
              - artifact:apps/api/schemas/plan.py
              - artifact:apps/web/src/pages/plan.tsx
              - critic:tests
              - critic:design_system
            domain: product
            description: Plan API surfaces experiment payloads; Plan UI renders
              lift/confidence cards with download
          - id: T0.1.3
            title: Generate forecast calibration report
            status: done
            dependencies: []
            exit_criteria:
              - artifact:docs/modeling/forecast_calibration_report.md
              - artifact:state/telemetry/calibration/*.json
              - critic:forecast_stitch
            domain: product
            description: Quantile calibration metrics with summary published to docs
  - id: E-PHASE1
    title: "Phase 1: Experience Delivery"
    status: done
    domain: product
    milestones:
      - id: M1.1
        title: Experience Delivery MVP
        status: done
        tasks:
          - id: T1.1.3
            title: Wire onboarding progress API
            status: done
            dependencies: []
            exit_criteria:
              - artifact:apps/api/routes/onboarding.py
              - artifact:apps/web/src/hooks/useOnboardingProgress.ts
              - critic:tests
            domain: product
            description: Implement GET/POST /onboarding/progress routes with telemetry
              instrumentation
  - id: E-REMEDIATION
    title: "[CRITICAL] Quality Remediation - Audit All Completed Work"
    status: blocked
    domain: product
    milestones:
      - id: M-MLR-2
        title: M-MLR-2
        status: pending
        tasks:
          - id: T-MLR-2.3
            title: Train models on all 20 synthetic tenants with cross-validation
            status: blocked
            dependencies:
              - T-MLR-2.2
            exit_criteria: []
            domain: product
            description: Train models on all 20 synthetic tenants with cross-validation
      - id: M-MLR-3
        title: M-MLR-3
        status: blocked
        tasks:
          - id: T-MLR-3.3
            title: Package all evidence artifacts for review
            status: blocked
            dependencies:
              - T-MLR-3.2
            exit_criteria: []
            domain: product
            description: Package all evidence artifacts for review
      - id: M-MLR-4
        title: M-MLR-4
        status: blocked
        tasks:
          - id: T-MLR-4.1
            title: Deploy ModelingReality_v2 critic to production
            status: pending
            dependencies:
              - T-MLR-0.1
              - T-MLR-3.3
            exit_criteria: []
            domain: product
            description: Deploy ModelingReality_v2 critic to production
          - id: T-MLR-4.2
            title: Update autopilot policy to require critic approval
            status: done
            dependencies:
              - T-MLR-4.1
            exit_criteria: []
            domain: product
            description: Update autopilot policy to require critic approval
          - id: T-MLR-4.3
            title: Create meta-critic to review past completed ML tasks
            status: blocked
            dependencies:
              - T-MLR-4.2
            exit_criteria: []
            domain: product
            description: Create meta-critic to review past completed ML tasks
          - id: T-MLR-4.4
            title: Document lessons learned and update contributor guide
            status: pending
            dependencies:
              - T-MLR-4.3
            exit_criteria: []
            domain: product
            description: Document lessons learned and update contributor guide
      - id: M-REM-1
        title: "[CRITICAL] Core Infrastructure Audit"
        status: blocked
        tasks:
          - id: CRIT-PERF-ACADEMICRIGOR-b0301a
            title: "[Critic:academicrigor] Restore performance"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: >-
              Critic academicrigor is underperforming and needs immediate
              remediation.


              Identity: Academic Rigor (academic_rigor, authority advisory)

              Mission: Safeguard academic_rigor discipline.

              Signature powers: Reports on findings when configuration is
              missing.


              Critic academicrigor failed 10 of the last 11 runs with 0
              consecutive failures.


              Observation window: 11 runs


              Consecutive failures: 0


              Failures: 10 | Successes: 1


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              {
                "epic": "E12",
                "summary": {
                  "done": 15,
                  "in_progress": 0,
                  "pending": 0,
                  "blocked": 0
                },
                "critical_issues": [],
                "high_issues": [],
                "warnings": [],
                "doc_path": "/Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/docs/CRITIQUE_E12_20251022.md"
              }
          - id: CRIT-PERF-ACADEMICRIGOR-f89932
            title: "[Critic:academicrigor] Restore performance"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: >-
              Critic academicrigor is underperforming and needs immediate
              remediation.


              Identity: Academic Rigor (academic_rigor, authority advisory)

              Mission: Safeguard academic_rigor discipline.

              Signature powers: Reports on findings when configuration is
              missing.


              Critic academicrigor failed 8 of the last 10 runs with 0
              consecutive failures.


              Observation window: 10 runs


              Consecutive failures: 0


              Failures: 8 | Successes: 2


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              {
                "epic": "E12",
                "summary": {
                  "done": 15,
                  "in_progress": 0,
                  "pending": 0,
                  "blocked": 0
                },
                "critical_issues": [],
                "high_issues": [],
                "warnings": [],
                "doc_path": "/Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/docs/CRITIQUE_E12_20251022.md"
              }
          - id: CRIT-PERF-ALLOCATOR-bc8604
            title: "[Critic:allocator] Restore performance"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: >-
              Critic allocator is underperforming and needs immediate
              remediation.


              Identity: Allocator Sentinel (operations, authority advisory)

              Mission: Ensure planner allocation and task routing stay optimal.

              Signature powers: Diagnoses misrouted tasks and capacity
              imbalances.; Suggests rebalancing across agents and squads.

              Autonomy guidance: Auto-adjust planner weights when safe; escalate
              persistent misallocations to Autopilot.


              Critic allocator failed 8 of the last 10 runs with 0 consecutive
              failures.


              Observation window: 10 runs


              Consecutive failures: 0


              Failures: 8 | Successes: 2


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              ============================= test session starts
              ==============================

              platform darwin -- Python 3.10.12, pytest-8.4.2, pluggy-1.6.0

              rootdir:
              /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane

              configfile: pyproject.toml

              plugins: anyio-3.7.1, asyncio-1.2.0

              asyncio: mode=strict, debug=False,
              asyncio_default_fixture_loop_scope=None,
              asyncio_default_test_loop_scope=function

              collected 5 items


              tests/test_allocator_routes.py
              ..                                        [ 40%]

              tests/test_creative_route.py
              .                                           [ 60%]

              tests/apps/model/test_cre...
          - id: CRIT-PERF-BUILD-958e1f
            title: "[Critic:build] Restore performance"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: >-
              Critic build is underperforming and needs immediate remediation.


              Identity: Build Sentinel (engineering, authority blocking)

              Mission: Guarantee that core build processes remain reproducible
              and optimized across environments.

              Signature powers: Diagnoses build pipeline regressions and
              unstable toolchains.; Flags missing build artifacts or
              misconfigured dependencies before release.

              Autonomy guidance: Attempt automated patching of build scripts
              when safe; escalate infrastructure escalations beyond local fixes.


              Critic build failed 5 of the last 6 runs with 0 consecutive
              failures.


              Observation window: 6 runs


              Consecutive failures: 0


              Failures: 5 | Successes: 1


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              warning: The top-level linter settings are deprecated in favour of
              their counterparts in the `lint` section. Please update the
              following options in `pyproject.toml`:
                - 'select' -> 'lint.select'
          - id: CRIT-PERF-CAUSAL-070d3d
            title: "[Critic:causal] Restore performance"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: >-
              Critic causal is underperforming and needs immediate remediation.


              Identity: Causal Strategist (ml, authority critical)

              Mission: Guarantee counterfactual validity and causal modeling
              rigor.

              Signature powers: Checks identifying assumptions, invariances, and
              instrumentation.; Constructs mitigation plans for confounding
              risks.

              Autonomy guidance: Partner with Research Orchestrator on complex
              interventions; log learnings for future experiments.


              No successful runs recorded in the last 12 observations; 12
              consecutive failures detected.


              Observation window: 12 runs


              Consecutive failures: 12


              Failures: 12 | Successes: 0


              Assigned to: Director Dana


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              skipped due to capability profile
          - id: CRIT-PERF-CAUSAL-e7682e
            title: "[Critic:causal] Restore performance"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: >-
              Critic causal is underperforming and needs immediate remediation.


              Identity: Causal Strategist (ml, authority critical)

              Mission: Guarantee counterfactual validity and causal modeling
              rigor.

              Signature powers: Checks identifying assumptions, invariances, and
              instrumentation.; Constructs mitigation plans for confounding
              risks.

              Autonomy guidance: Partner with Research Orchestrator on complex
              interventions; log learnings for future experiments.


              Critic causal failed 8 of the last 10 runs with 0 consecutive
              failures.


              Observation window: 10 runs


              Consecutive failures: 0


              Failures: 8 | Successes: 2


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              {
                "status": "passed",
                "level": "medium",
                "findings": [
                  {
                    "severity": "INFO",
                    "message": "Weather shock estimator present.",
                    "details": null
                  },
                  {
                    "severity": "INFO",
                    "message": "Weather shock tests passed.",
                    "details": "  /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/shared/libs/causal/weather_shock.py:194: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n  (Deprecated in version 0.20.5)\n    pl.count().alias(\"pre_count\"),\n\ntests/shared/libs/causal/test_weather_shock.py::test_weather_shock...
          - id: CRIT-PERF-DESIGNSYSTEM-1a886a
            title: "[Critic:designsystem] Restore performance"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: >-
              Critic designsystem is underperforming and needs immediate
              remediation.


              Identity: Design System (design_system, authority advisory)

              Mission: Safeguard design_system discipline.

              Signature powers: Reports on findings when configuration is
              missing.


              No successful runs recorded in the last 9 observations; 5
              consecutive failures detected.


              Observation window: 9 runs


              Consecutive failures: 5


              Failures: 6 | Successes: 3


              Assigned to: Director Dana


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              ./src/pages/dashboard.tsx

              968:14  Error: Parsing error: ')' expected.


              info  - Need to disable some ESLint rules? Learn more here:
              https://nextjs.org/docs/basic-features/eslint#disabling-rules
          - id: CRIT-PERF-EXECREVIEW-ef2384
            title: "[Critic:execreview] Restore performance"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: >-
              Critic execreview is underperforming and needs immediate
              remediation.


              Identity: Exec Review (exec_review, authority advisory)

              Mission: Safeguard exec_review discipline.

              Signature powers: Reports on findings when configuration is
              missing.


              No successful runs recorded in the last 12 observations; 12
              consecutive failures detected.


              Observation window: 12 runs


              Consecutive failures: 12


              Failures: 12 | Successes: 0


              Assigned to: Director Dana


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              skipped due to capability profile
          - id: CRIT-PERF-GLOBAL-9882b7
            title: "[Critics] Systemic performance remediation"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: >-
              Multiple critics are underperforming and require coordinated
              intervention.


              3 critics require director-level intervention after repeated
              failures.


              Affected critics: execreview, integrationfury, managerselfcheck


              Critics evaluated in run: 5


              Reports captured: 3


              Assigned to: Director Dana


              Expectations:

              - Review individual remediation tasks and look for systemic
              issues.

              - Adjust critic configurations, training loops, or staffing mixes.

              - Provide a coordination brief in state/context.md.

              - Close this systemic task once individual critics are back on
              track.
          - id: CRIT-PERF-HEALTHCHECK-0e6b67
            title: "[Critic:healthcheck] Restore performance"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: >-
              Critic healthcheck is underperforming and needs immediate
              remediation.


              Identity: Health Check (health_check, authority advisory)

              Mission: Safeguard health_check discipline.

              Signature powers: Reports on findings when configuration is
              missing.


              Critic healthcheck failed 4 of the last 5 runs with 0 consecutive
              failures.


              Observation window: 5 runs


              Consecutive failures: 0


              Failures: 4 | Successes: 1


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              warning: The top-level linter settings are deprecated in favour of
              their counterparts in the `lint` section. Please update the
              following options in `pyproject.toml`:
                - 'select' -> 'lint.select'
          - id: CRIT-PERF-INTEGRATIONFURY-9401af
            title: "[Critic:integrationfury] Restore performance"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: >-
              Critic integrationfury is underperforming and needs immediate
              remediation.


              Identity: Integration Fury (integration_fury, authority advisory)

              Mission: Safeguard integration_fury discipline.

              Signature powers: Reports on findings when configuration is
              missing.


              No successful runs recorded in the last 6 observations; 6
              consecutive failures detected.


              Observation window: 6 runs


              Consecutive failures: 6


              Failures: 6 | Successes: 0


              Assigned to: Director Dana


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              ============================= test session starts
              ==============================

              platform darwin -- Python 3.10.12, pytest-8.4.2, pluggy-1.6.0

              rootdir:
              /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane

              configfile: pyproject.toml

              plugins: anyio-4.11.0, asyncio-1.2.0

              asyncio: mode=strict, debug=False,
              asyncio_default_fixture_loop_scope=None,
              asyncio_default_test_loop_scope=function

              collected 242 items


              tests/api/onboarding/test_progress.py
              ....                               [  1%]

              tests/api/test_ad_push_routes.py
              ....                                    [  3%]

              tests/api/test_dashboa...
          - id: CRIT-PERF-MANAGERSELFCHECK-61ab48
            title: "[Critic:managerselfcheck] Restore performance"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: >-
              Critic managerselfcheck is underperforming and needs immediate
              remediation.


              Identity: Manager Self Check (manager_self_check, authority
              advisory)

              Mission: Safeguard manager_self_check discipline.

              Signature powers: Reports on findings when configuration is
              missing.


              No successful runs recorded in the last 6 observations; 6
              consecutive failures detected.


              Observation window: 6 runs


              Consecutive failures: 6


              Failures: 6 | Successes: 0


              Assigned to: Director Dana


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              Rollback simulation stale
              (simulated_at=2025-10-15T21:05:00+00:00); rerun executor to
              refresh promotion gate.
          - id: CRIT-PERF-ORGPM-be2140
            title: "[Critic:orgpm] Restore performance"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: >-
              Critic orgpm is underperforming and needs immediate remediation.


              Identity: Org Pm (org_pm, authority advisory)

              Mission: Safeguard org_pm discipline.

              Signature powers: Reports on findings when configuration is
              missing.


              Critic orgpm failed 5 of the last 6 runs with 0 consecutive
              failures.


              Observation window: 6 runs


              Consecutive failures: 0


              Failures: 5 | Successes: 1


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              Org PM charter/state checks passed.
          - id: CRIT-PERF-PROMPTBUDGET-2c30f3
            title: "[Critic:promptbudget] Restore performance"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: >-
              Critic promptbudget is underperforming and needs immediate
              remediation.


              Identity: Prompt Budget (prompt_budget, authority advisory)

              Mission: Safeguard prompt_budget discipline.

              Signature powers: Reports on findings when configuration is
              missing.


              Critic promptbudget failed 5 of the last 6 runs with 0 consecutive
              failures.


              Observation window: 6 runs


              Consecutive failures: 0


              Failures: 5 | Successes: 1


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              {"level":"warning","message":"Code search index rebuild
              failed","timestamp":"2025-10-16T20:39:47.650Z","error":"The
              database connection is not open"}
          - id: CRIT-PERF-SECURITY-645edd
            title: "[Critic:security] Restore performance"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: >-
              Critic security is underperforming and needs immediate
              remediation.


              Identity: Security Sentinel (security, authority critical)

              Mission: Guard secrets, policies, and attack surfaces throughout
              the stack.

              Signature powers: Identifies credential leaks, insecure defaults,
              and policy gaps.; Cross-references security playbooks to recommend
              mitigations.

              Autonomy guidance: Demand sign-off for high-risk findings;
              coordinate with Director Dana and Security Stewards.


              No successful runs recorded in the last 9 observations; 6
              consecutive failures detected.


              Observation window: 9 runs


              Consecutive failures: 6


              Failures: 6 | Successes: 3


              Assigned to: Director Dana


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              skipped due to capability profile
          - id: CRIT-PERF-TESTS-426598
            title: "[Critic:tests] Restore performance"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: "Critic tests is underperforming and needs immediate remediation.


              Identity: Regression Hunter (quality, authority blocking)

              Mission: Keep the test suites healthy and ensure deterministic
              results across flows.

              Signature powers: Surfaces flaky suites and failing assertions
              with reproduction notes.; Synthesizes minimal repro commands for
              Autopilot triage.

              Autonomy guidance: Rerun targeted suites automatically; lean on
              Autopilot only when new failures persist.


              Critic tests failed 5 of the last 6 runs with 0 consecutive
              failures.


              Observation window: 6 runs


              Consecutive failures: 0


              Failures: 5 | Successes: 1


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              \e[33mThe CJS build of Vite's Node API is deprecated. See
              https://vite.dev/guide/troubleshooting.html#vite-cjs-node-api-dep\
              recated for more details.\e[39m

              {\"level\":\"info\",\"message\":\"Subscription limit tracker
              initialized\",\"timestamp\":\"2025-10-16T21:37:34.835Z\",\"provid\
              ers\":[]}

              {\"level\":\"info\",\"message\":\"Provider registered for usage
              tracking\",\"timestamp\":\"2025-10-16T21:37:34.837Z\",\"provider\
              \":\"claude\",\"account\":\"test-account\",\"tier\":\"pro\"}

              {\"level\":\"info\",\"message\":\"Subscription limit tracker
              stopped\",\"timestamp\":\"2025-10-16T21:37:34.840Z\"}

              {\"level\":\"info\",\"message\":\"Subscription limit tracker ..."
          - id: CRIT-PERF-TESTS-c3fce7
            title: "[Critic:tests] Restore performance"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: "Critic tests is underperforming and needs immediate remediation.


              Identity: Regression Hunter (quality, authority blocking)

              Mission: Keep the test suites healthy and ensure deterministic
              results across flows.

              Signature powers: Surfaces flaky suites and failing assertions
              with reproduction notes.; Synthesizes minimal repro commands for
              Autopilot triage.

              Autonomy guidance: Rerun targeted suites automatically; lean on
              Autopilot only when new failures persist.


              Critic tests failed 5 of the last 6 runs with 4 consecutive
              failures.


              Observation window: 6 runs


              Consecutive failures: 4


              Failures: 5 | Successes: 1


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying
              automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              \e[33mThe CJS build of Vite's Node API is deprecated. See
              https://vite.dev/guide/troubleshooting.html#vite-cjs-node-api-dep\
              recated for more details.\e[39m

              ⎯⎯⎯⎯⎯⎯⎯ Failed Tests 6 ⎯⎯⎯⎯⎯⎯⎯


              \ FAIL  ../../tests/web/design_system_acceptance.spec.ts > Design
              system acceptance – Stories page > renders skip navigation, main
              landmark, and story metadata tokens

              AssertionError: expected \"error\" to not be called at all, but
              actually been called 5 times


              Received:\ 


              \  1st error call:


              \    Array [

              \      \"Warning: An update to %s inside a test was not wrapped in
              act(...).

              \   \ 

              \    When testing, code that cau..."
          - id: PHASE-1-HARDENING
            title: "Phase 1: MCP Hardening"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: BLOCKING – must complete before other work (disable YAML writes,
              real usage/cost, correlation IDs, coordinator failover)
          - id: PHASE-2-COMPACT
            title: "Phase 2: Compact Prompts + Selfchecks"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: BLOCKING – must complete before other work (compact context
              assembler, snapshot selfcheck)
          - id: PHASE-3-BATCH
            title: "Phase 3: Batch Queue & Prompt Headers"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: BLOCKING – must complete before other work (batch queue, stable
              headers, token heuristics)
          - id: REM-T-MLR-0.1
            title: "[REM] Verify: Create ModelingReality critic with quantitative
              thresholds"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T-MLR-0.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Create ModelingReality critic
              with quantitative thresholds


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T-MLR-0.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T-MLR-0.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T-MLR-0.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T-MLR-0.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T-MLR-0.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T-MLR-0.1 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T-MLR-0.2
            title: "[REM] Verify: Update all ML task exit criteria with objective metrics"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T-MLR-0.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Update all ML task exit criteria
              with objective metrics


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T-MLR-0.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T-MLR-0.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T-MLR-0.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T-MLR-0.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T-MLR-0.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T-MLR-0.2 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T-MLR-1.1
            title: "[REM] Verify: Debug and fix weather multiplier logic in data generator"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T-MLR-1.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Debug and fix weather multiplier
              logic in data generator


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T-MLR-1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T-MLR-1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T-MLR-1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T-MLR-1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T-MLR-1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T-MLR-1.1 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T-MLR-1.3
            title: "[REM] Verify: Create validation tests for synthetic data quality"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T-MLR-1.3 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Create validation tests for
              synthetic data quality


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T-MLR-1.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T-MLR-1.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T-MLR-1.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T-MLR-1.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T-MLR-1.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T-MLR-1.3 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T-MLR-2.1
            title: "[REM] Verify: Implement proper train/val/test splitting with no leakage"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T-MLR-2.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Implement proper train/val/test
              splitting with no leakage


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T-MLR-2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T-MLR-2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T-MLR-2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T-MLR-2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T-MLR-2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T-MLR-2.1 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T-MLR-2.2
            title: "[REM] Verify: Implement LightweightMMM with weather features"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T-MLR-2.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Implement LightweightMMM with
              weather features


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T-MLR-2.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T-MLR-2.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T-MLR-2.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T-MLR-2.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T-MLR-2.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T-MLR-2.2 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T-MLR-2.5
            title: "[REM] Verify: Compare models to baseline (naive/seasonal/linear)"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T-MLR-2.5 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Compare models to baseline
              (naive/seasonal/linear)


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T-MLR-2.5
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T-MLR-2.5
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T-MLR-2.5
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T-MLR-2.5
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T-MLR-2.5 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T-MLR-2.5 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T-MLR-3.2
            title: "[REM] Verify: Write comprehensive ML validation documentation"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T-MLR-3.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Write comprehensive ML validation
              documentation


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T-MLR-3.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T-MLR-3.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T-MLR-3.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T-MLR-3.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T-MLR-3.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T-MLR-3.2 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T0.1.1
            title: "[REM] Verify: Implement geo holdout plumbing"
            status: blocked
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T0.1.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Implement geo holdout plumbing


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T0.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T0.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T0.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T0.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T0.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T0.1.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T0.1.2
            title: "[REM] Verify: Build lift & confidence UI surfaces"
            status: blocked
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T0.1.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Build lift & confidence UI
              surfaces


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T0.1.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T0.1.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T0.1.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T0.1.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T0.1.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T0.1.2 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T0.1.3
            title: "[REM] Verify: Generate forecast calibration report"
            status: blocked
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T0.1.3 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Generate forecast calibration
              report


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T0.1.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T0.1.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T0.1.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T0.1.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T0.1.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T0.1.3 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T1.1.1
            title: "[REM] Verify: Design Open-Meteo + Shopify connectors and data contracts"
            status: blocked
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T1.1.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Design Open-Meteo + Shopify
              connectors and data contracts


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T1.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T1.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T1.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T1.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T1.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T1.1.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T1.1.2
            title: "[REM] Verify: Implement ingestion Prefect flow with checkpointing"
            status: blocked
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T1.1.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Implement ingestion Prefect flow
              with checkpointing


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T1.1.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T1.1.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T1.1.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T1.1.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T1.1.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T1.1.2 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T1.1.3
            title: "[REM] Verify: Wire onboarding progress API"
            status: needs_improvement
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T1.1.3 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Wire onboarding progress API


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T1.1.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T1.1.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T1.1.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T1.1.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T1.1.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T1.1.3 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T1.2.1
            title: "[REM] Verify: Blend historical + forecast weather, enforce timezone
              alignm"
            status: blocked
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T1.2.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Blend historical + forecast
              weather, enforce timezone alignm


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T1.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T1.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T1.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T1.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T1.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T1.2.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T1.2.2
            title: "[REM] Verify: Add leakage guardrails to feature builder"
            status: blocked
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T1.2.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Add leakage guardrails to feature
              builder


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T1.2.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T1.2.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T1.2.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T1.2.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T1.2.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T1.2.2 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T10.1.1
            title: "[REM] Verify: Cost telemetry and budget alerts"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T10.1.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Cost telemetry and budget alerts


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T10.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T10.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T10.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T10.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T10.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T10.1.1 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T11.1.1
            title: "[REM] Verify: Implement hardware probe & profile persistence"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T11.1.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Implement hardware probe &
              profile persistence


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T11.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T11.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T11.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T11.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T11.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T11.1.1 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T11.1.2
            title: "[REM] Verify: Adaptive scheduling for heavy tasks"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T11.1.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Adaptive scheduling for heavy
              tasks


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T11.1.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T11.1.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T11.1.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T11.1.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T11.1.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T11.1.2 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T11.2.1
            title: "[REM] Verify: Design system elevation (motion, typography, theming)"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T11.2.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Design system elevation (motion,
              typography, theming)


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T11.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T11.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T11.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T11.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T11.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T11.2.1 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T11.2.2
            title: "[REM] Verify: Award-level experience audit & remediation"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T11.2.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Award-level experience audit &
              remediation


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T11.2.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T11.2.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T11.2.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T11.2.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T11.2.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T11.2.2 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T11.2.3
            title: "[REM] Verify: Extend calm/aero theme tokens to Automations and
              Experiments"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T11.2.3 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Extend calm/aero theme tokens to
              Automations and Experiments


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T11.2.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T11.2.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T11.2.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T11.2.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T11.2.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T11.2.3 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T11.2.4
            title: "[REM] Verify: Refactor landing/marketing gradients into reusable tokens"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T11.2.4 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Refactor landing/marketing
              gradients into reusable tokens


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T11.2.4
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T11.2.4
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T11.2.4
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T11.2.4
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T11.2.4 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T11.2.4 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T11.2.5
            title: "[REM] Verify: Centralize retry button styles in shared component once
              App"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T11.2.5 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Centralize retry button styles in
              shared component once App 


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T11.2.5
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T11.2.5
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T11.2.5
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T11.2.5
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T11.2.5 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T11.2.5 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T11.2.6
            title: "[REM] Verify: Formalize shared panel mixin (border + shadow) to reduce
              ove"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T11.2.6 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Formalize shared panel mixin
              (border + shadow) to reduce ove


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T11.2.6
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T11.2.6
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T11.2.6
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T11.2.6
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T11.2.6 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T11.2.6 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T12.0.3
            title: "[REM] Verify: Document synthetic tenant characteristics"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T12.0.3 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Document synthetic tenant
              characteristics


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T12.0.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T12.0.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T12.0.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T12.0.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T12.0.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T12.0.3 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T12.1.2
            title: "[REM] Verify: Validate feature store joins against historical weather
              base"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T12.1.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Validate feature store joins
              against historical weather base


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T12.1.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T12.1.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T12.1.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T12.1.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T12.1.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T12.1.2 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T12.2.1
            title: "[REM] Verify: Backtest weather-aware model vs control across top
              tenants"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T12.2.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Backtest weather-aware model vs
              control across top tenants


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T12.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T12.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T12.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T12.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T12.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T12.2.1 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T12.PoC.3
            title: "[REM] Verify: Create PoC demo results and proof brief"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T12.PoC.3 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Create PoC demo results and proof
              brief


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T12.PoC.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T12.PoC.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T12.PoC.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T12.PoC.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T12.PoC.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T12.PoC.3 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T13.1.1
            title: "[REM] Verify: Validate 90-day tenant data coverage across sales, spend,
              an"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T13.1.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Validate 90-day tenant data
              coverage across sales, spend, an


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T13.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T13.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T13.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T13.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T13.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T13.1.1 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T13.1.3
            title: "[REM] Verify: Implement product taxonomy auto-classification with
              weather"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T13.1.3 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Implement product taxonomy
              auto-classification with weather 


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T13.1.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T13.1.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T13.1.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T13.1.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T13.1.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T13.1.3 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T13.1.4
            title: "[REM] Verify: Data quality validation framework (verify data fitness for
              M"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T13.1.4 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Data quality validation framework
              (verify data fitness for M


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T13.1.4
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T13.1.4
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T13.1.4
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T13.1.4
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T13.1.4 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T13.1.4 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T13.2.1
            title: "[REM] Verify: Replace heuristic MMM with LightweightMMM
              adstock+saturation"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T13.2.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Replace heuristic MMM with
              LightweightMMM adstock+saturation


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T13.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T13.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T13.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T13.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T13.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T13.2.1 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T13.2.3
            title: "[REM] Verify: Replace heuristic allocator with constraint-aware
              optimizer"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T13.2.3 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Replace heuristic allocator with
              constraint-aware optimizer


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T13.2.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T13.2.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T13.2.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T13.2.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T13.2.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T13.2.3 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T13.3.2
            title: "[REM] Verify: Implement DMA-first geographic aggregation with
              hierarchical"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T13.3.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Implement DMA-first geographic
              aggregation with hierarchical


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T13.3.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T13.3.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T13.3.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T13.3.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T13.3.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T13.3.2 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T13.4.1
            title: "[REM] Verify: Add modeling reality critic to Autopilot"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T13.4.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Add modeling reality critic to
              Autopilot


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T13.4.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T13.4.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T13.4.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T13.4.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T13.4.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T13.4.1 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T13.5.1
            title: "[REM] Verify: Train weather-aware allocation model on top of MMM
              baseline"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T13.5.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Train weather-aware allocation
              model on top of MMM baseline


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T13.5.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T13.5.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T13.5.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T13.5.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T13.5.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T13.5.1 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T2.1.1
            title: "[REM] Verify: Build lag/rolling feature generators with deterministic
              seed"
            status: blocked
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T2.1.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Build lag/rolling feature
              generators with deterministic seed


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T2.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T2.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T2.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T2.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T2.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T2.1.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T2.2.1
            title: "[REM] Verify: Train weather-aware GAM baseline and document methodology"
            status: needs_improvement
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T2.2.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Train weather-aware GAM baseline
              and document methodology


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T2.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T2.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T2.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T2.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T2.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T2.2.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T3.1.1
            title: "[REM] Verify: Implement budget allocator stress tests and regret bounds"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T3.1.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Implement budget allocator stress
              tests and regret bounds


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T3.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T3.1.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T3.2.1
            title: "[REM] Verify: Run design system critic and ensure accessibility
              coverage"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T3.2.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Run design system critic and
              ensure accessibility coverage


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T3.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T3.2.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T3.2.2
            title: "[REM] Verify: Elevate dashboard storytelling & UX"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T3.2.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Elevate dashboard storytelling &
              UX


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T3.2.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.2.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.2.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.2.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.2.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T3.2.2 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T3.3.1
            title: "[REM] Verify: Draft multi-agent charter & delegation mesh (AutoGen/Swarm
              p"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T3.3.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Draft multi-agent charter &
              delegation mesh (AutoGen/Swarm p


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T3.3.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.3.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.3.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.3.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.3.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T3.3.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T3.3.2
            title: "[REM] Verify: Implement hierarchical consensus & escalation engine"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T3.3.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Implement hierarchical consensus
              & escalation engine


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T3.3.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.3.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.3.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.3.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.3.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T3.3.2 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T3.3.3
            title: "[REM] Verify: Build closed-loop simulation harness for autonomous teams"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T3.3.3 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Build closed-loop simulation
              harness for autonomous teams


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T3.3.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.3.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.3.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.3.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.3.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T3.3.3 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T3.3.4
            title: "[REM] Verify: Instrument dynamic staffing telemetry & learning pipeline"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T3.3.4 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Instrument dynamic staffing
              telemetry & learning pipeline


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T3.3.4
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.3.4
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.3.4
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.3.4
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.3.4 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T3.3.4 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T3.4.1
            title: "[REM] Verify: Implement Plan overview page with weather-driven insights"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T3.4.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Implement Plan overview page with
              weather-driven insights


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T3.4.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.4.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.4.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.4.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.4.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T3.4.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T3.4.2
            title: "[REM] Verify: Build WeatherOps dashboard with allocator + weather KPIs"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T3.4.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Build WeatherOps dashboard with
              allocator + weather KPIs


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T3.4.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.4.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.4.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.4.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.4.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T3.4.2 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T3.4.3
            title: "[REM] Verify: Ship Experiments hub UI for uplift & incrementality
              reviews"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T3.4.3 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Ship Experiments hub UI for
              uplift & incrementality reviews


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T3.4.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.4.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.4.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.4.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.4.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T3.4.3 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T3.4.4
            title: "[REM] Verify: Deliver storytelling Reports view with weather + spend
              narra"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T3.4.4 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Deliver storytelling Reports view
              with weather + spend narra


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T3.4.4
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.4.4
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.4.4
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.4.4
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.4.4 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T3.4.4 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T3.4.5
            title: "[REM] Verify: Conduct design_system + UX acceptance review across
              implemen"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T3.4.5 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Conduct design_system + UX
              acceptance review across implemen


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T3.4.5
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.4.5
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.4.5
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.4.5
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.4.5 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T3.4.5 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T3.4.6
            title: "[REM] Verify: Rewrite WeatherOps dashboard around plain-language
              decisions"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T3.4.6 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Rewrite WeatherOps dashboard
              around plain-language decisions


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T3.4.6
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.4.6
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.4.6
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.4.6
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.4.6 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T3.4.6 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T3.4.7
            title: "[REM] Verify: Reimagine Automations change log as a trust-first
              narrative"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T3.4.7 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Reimagine Automations change log
              as a trust-first narrative


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T3.4.7
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.4.7
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.4.7
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.4.7
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.4.7 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T3.4.7 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T4.1.10
            title: "[REM] Verify: Cross-market saturation optimization (fairness-aware)"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T4.1.10 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Cross-market saturation
              optimization (fairness-aware)


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T4.1.10
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T4.1.10
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T4.1.10
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T4.1.10
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T4.1.10 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T4.1.10 was marked "done" but needs
              verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T4.1.3
            title: "[REM] Verify: Causal uplift modeling & incremental lift validation"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T4.1.3 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Causal uplift modeling &
              incremental lift validation


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T4.1.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T4.1.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T4.1.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T4.1.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T4.1.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T4.1.3 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T4.1.4
            title: "[REM] Verify: Multi-horizon ensemble forecasting"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T4.1.4 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Multi-horizon ensemble
              forecasting


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T4.1.4
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T4.1.4
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T4.1.4
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T4.1.4
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T4.1.4 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T4.1.4 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T4.1.5
            title: "[REM] Verify: Non-linear allocation optimizer with constraints (ROAS,
              spen"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T4.1.5 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Non-linear allocation optimizer
              with constraints (ROAS, spen


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T4.1.5
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T4.1.5
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T4.1.5
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T4.1.5
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T4.1.5 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T4.1.5 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T4.1.6
            title: "[REM] Verify: High-frequency spend response modeling (intraday
              adjustments"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T4.1.6 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: High-frequency spend response
              modeling (intraday adjustments


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T4.1.6
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T4.1.6
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T4.1.6
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T4.1.6
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T4.1.6 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T4.1.6 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T4.1.7
            title: "[REM] Verify: Marketing mix budget solver (multi-channel,
              weather-aware)"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T4.1.7 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Marketing mix budget solver
              (multi-channel, weather-aware)


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T4.1.7
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T4.1.7
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T4.1.7
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T4.1.7
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T4.1.7 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T4.1.7 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T4.1.8
            title: "[REM] Verify: Reinforcement-learning shadow mode (safe exploration)"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T4.1.8 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Reinforcement-learning shadow
              mode (safe exploration)


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T4.1.8
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T4.1.8
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T4.1.8
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T4.1.8
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T4.1.8 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T4.1.8 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T4.1.9
            title: "[REM] Verify: Creative-level response modeling with brand safety
              guardrail"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T4.1.9 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Creative-level response modeling
              with brand safety guardrail


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T4.1.9
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T4.1.9
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T4.1.9
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T4.1.9
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T4.1.9 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T4.1.9 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T5.1.1
            title: "[REM] Verify: Implement Meta Marketing API client (creative + campaign
              man"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T5.1.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Implement Meta Marketing API
              client (creative + campaign man


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T5.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T5.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T5.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T5.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T5.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T5.1.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T5.1.2
            title: "[REM] Verify: Meta sandbox and dry-run executor with credential
              vaulting"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T5.1.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Meta sandbox and dry-run executor
              with credential vaulting


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T5.1.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T5.1.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T5.1.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T5.1.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T5.1.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T5.1.2 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T5.2.1
            title: "[REM] Verify: Google Ads API integration (campaign create/update, shared
              b"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T5.2.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Google Ads API integration
              (campaign create/update, shared b


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T5.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T5.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T5.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T5.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T5.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T5.2.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T5.2.2
            title: "[REM] Verify: Budget reconciliation & spend guardrails across platforms"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T5.2.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Budget reconciliation & spend
              guardrails across platforms


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T5.2.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T5.2.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T5.2.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T5.2.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T5.2.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T5.2.2 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T5.3.1
            title: "[REM] Verify: Dry-run & diff visualizer for ad pushes (pre-flight
              checks)"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T5.3.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Dry-run & diff visualizer for ad
              pushes (pre-flight checks)


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T5.3.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T5.3.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T5.3.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T5.3.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T5.3.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T5.3.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T5.3.2
            title: "[REM] Verify: Automated rollback + alerting when performance/regression
              de"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T5.3.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Automated rollback + alerting
              when performance/regression de


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T5.3.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T5.3.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T5.3.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T5.3.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T5.3.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T5.3.2 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T6.1.1
            title: "[REM] Verify: MCP server integration tests (all 25 tools across both
              provi"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T6.1.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: MCP server integration tests (all
              25 tools across both provi


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T6.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T6.1.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T6.1.2
            title: "[REM] Verify: Provider failover testing (token limit simulation &
              automati"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T6.1.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Provider failover testing (token
              limit simulation & automati


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T6.1.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.1.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.1.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.1.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.1.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T6.1.2 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T6.1.3
            title: "[REM] Verify: State persistence testing (checkpoint recovery across
              sessio"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T6.1.3 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: State persistence testing
              (checkpoint recovery across sessio


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T6.1.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.1.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.1.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.1.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.1.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T6.1.3 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T6.1.4
            title: "[REM] Verify: Quality framework validation (10 dimensions operational)"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T6.1.4 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Quality framework validation (10
              dimensions operational)


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T6.1.4
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.1.4
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.1.4
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.1.4
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.1.4 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T6.1.4 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T6.2.1
            title: "[REM] Verify: Credentials security audit (auth.json, API keys, token
              rotat"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T6.2.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Credentials security audit
              (auth.json, API keys, token rotat


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T6.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T6.2.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T6.2.2
            title: "[REM] Verify: Error recovery testing (graceful degradation, retry
              logic)"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T6.2.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Error recovery testing (graceful
              degradation, retry logic)


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T6.2.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.2.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.2.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.2.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.2.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T6.2.2 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T6.2.3
            title: "[REM] Verify: Schema validation enforcement (all data contracts
              validated)"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T6.2.3 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Schema validation enforcement
              (all data contracts validated)


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T6.2.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.2.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.2.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.2.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.2.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T6.2.3 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T6.2.4
            title: "[REM] Verify: API rate limiting & exponential backoff (Open-Meteo,
              Shopify"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T6.2.4 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: API rate limiting & exponential
              backoff (Open-Meteo, Shopify


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T6.2.4
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.2.4
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.2.4
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.2.4
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.2.4 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T6.2.4 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T6.3.1
            title: "[REM] Verify: Performance benchmarking (MCP overhead, checkpoint size,
              tok"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T6.3.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Performance benchmarking (MCP
              overhead, checkpoint size, tok


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T6.3.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.3.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.3.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.3.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.3.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T6.3.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T6.3.2
            title: "[REM] Verify: Enhanced observability export (structured logs, metrics
              dash"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T6.3.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Enhanced observability export
              (structured logs, metrics dash


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T6.3.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.3.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.3.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.3.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.3.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T6.3.2 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T6.3.3
            title: "[REM] Verify: Autopilot loop end-to-end testing (full autonomous cycle
              val"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T6.3.3 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Autopilot loop end-to-end testing
              (full autonomous cycle val


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T6.3.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.3.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.3.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.3.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.3.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T6.3.3 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T6.4.0
            title: "[REM] Verify: Upgrade invariants & preflight guardrails"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T6.4.0 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Upgrade invariants & preflight
              guardrails


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T6.4.0
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.4.0
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.4.0
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.4.0
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.4.0 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T6.4.0 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T6.4.1
            title: "[REM] Verify: Live feature flag store with kill switch"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T6.4.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Live feature flag store with kill
              switch


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T6.4.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.4.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.4.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.4.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.4.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T6.4.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T6.4.2
            title: "[REM] Verify: Blue/green worker manager & front-end proxy"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T6.4.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Blue/green worker manager &
              front-end proxy


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T6.4.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.4.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.4.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.4.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.4.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T6.4.2 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T6.4.3
            title: "[REM] Verify: Worker entrypoint with DRY_RUN safeguards"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T6.4.3 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Worker entrypoint with DRY_RUN
              safeguards


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T6.4.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.4.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.4.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.4.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.4.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T6.4.3 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T6.4.4
            title: "[REM] Verify: Canary upgrade harness & shadow validation"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T6.4.4 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Canary upgrade harness & shadow
              validation


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T6.4.4
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.4.4
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.4.4
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.4.4
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.4.4 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T6.4.4 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T6.4.7
            title: "[REM] Verify: Automatic rollback monitors & kill-switch reset"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T6.4.7 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Automatic rollback monitors &
              kill-switch reset


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T6.4.7
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.4.7
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.4.7
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.4.7
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.4.7 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T6.4.7 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T6.4.8
            title: "[REM] Verify: Observability & resource budgets during upgrade"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T6.4.8 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Observability & resource budgets
              during upgrade


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T6.4.8
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.4.8
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.4.8
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.4.8
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.4.8 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T6.4.8 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T7.1.1
            title: "[REM] Verify: Complete geocoding integration (city->lat/lon, cache
              strateg"
            status: needs_improvement
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T7.1.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Complete geocoding integration
              (city->lat/lon, cache strateg


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T7.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T7.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T7.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T7.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T7.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T7.1.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T7.1.2
            title: "[REM] Verify: Weather feature join to model matrix (prevent future
              leakage"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T7.1.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Weather feature join to model
              matrix (prevent future leakage


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T7.1.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T7.1.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T7.1.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T7.1.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T7.1.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T7.1.2 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T7.1.3
            title: "[REM] Verify: Data contract schema validation (Shopify, weather, ads)"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T7.1.3 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Data contract schema validation
              (Shopify, weather, ads)


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T7.1.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T7.1.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T7.1.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T7.1.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T7.1.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T7.1.3 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T7.2.1
            title: "[REM] Verify: Incremental ingestion with deduplication & checkpointing"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T7.2.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Incremental ingestion with
              deduplication & checkpointing


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T7.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T7.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T7.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T7.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T7.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T7.2.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T7.2.2
            title: "[REM] Verify: Data quality monitoring & alerting (anomaly detection)"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T7.2.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Data quality monitoring &
              alerting (anomaly detection)


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T7.2.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T7.2.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T7.2.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T7.2.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T7.2.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T7.2.2 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T8.1.1
            title: "[REM] Verify: Lock MCP schemas to Zod shapes (SAFE: guardrail)"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T8.1.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Lock MCP schemas to Zod shapes
              (SAFE: guardrail)


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T8.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T8.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T8.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T8.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T8.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T8.1.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T8.1.2
            title: "[REM] Verify: Implement command allow-list in guardrails (SAFE: additive
              s"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T8.1.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Implement command allow-list in
              guardrails (SAFE: additive s


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T8.1.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T8.1.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T8.1.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T8.1.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T8.1.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T8.1.2 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T8.1.3
            title: "[REM] Verify: Thread correlation IDs through state transitions (SAFE:
              obse"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T8.1.3 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Thread correlation IDs through
              state transitions (SAFE: obse


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T8.1.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T8.1.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T8.1.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T8.1.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T8.1.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T8.1.3 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T8.2.1
            title: "[REM] Verify: Implement compact evidence-pack prompt mode (SAFE: new
              funct"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T8.2.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Implement compact evidence-pack
              prompt mode (SAFE: new funct


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T8.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T8.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T8.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T8.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T8.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T8.2.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T8.2.2
            title: "[REM] Verify: Finalize Claude↔Codex coordinator failover (SAFE: expose
              exi"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T8.2.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Finalize Claude↔Codex coordinator
              failover (SAFE: expose exi


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T8.2.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T8.2.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T8.2.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T8.2.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T8.2.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T8.2.2 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T9.1.1
            title: "[REM] Verify: Stable prompt headers with provider caching (SAFE:
              additive"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T9.1.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Stable prompt headers with
              provider caching (SAFE: additive 


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T9.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T9.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T9.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T9.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T9.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T9.1.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T9.1.2
            title: "[REM] Verify: Batch queue for non-urgent prompts (SAFE: new queueing
              syste"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T9.1.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Batch queue for non-urgent
              prompts (SAFE: new queueing syste


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T9.1.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T9.1.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T9.1.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T9.1.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T9.1.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T9.1.2 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T9.2.1
            title: "[REM] Verify: Strict output DSL validation (SAFE: validation layer
              only)"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T9.2.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Strict output DSL validation
              (SAFE: validation layer only)


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T9.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T9.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T9.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T9.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T9.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T9.2.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T9.2.2
            title: "[REM] Verify: Idempotency keys for mutating tools (SAFE: caching layer)"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T9.2.2 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: Idempotency keys for mutating
              tools (SAFE: caching layer)


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T9.2.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T9.2.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T9.2.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T9.2.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T9.2.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T9.2.2 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T9.3.1
            title: "[REM] Verify: OpenTelemetry spans for all operations (SAFE: tracing
              wrappe"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T9.3.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: OpenTelemetry spans for all
              operations (SAFE: tracing wrappe


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T9.3.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T9.3.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T9.3.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T9.3.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T9.3.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T9.3.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REM-T9.4.1
            title: "[REM] Verify: SQLite FTS5 index for code search (SAFE: new index)"
            status: pending
            dependencies: []
            exit_criteria:
              - Implementation verified
              - Tests verified
              - Quality gate APPROVED
              - Runtime verification PASSED
              - Critical issues fixed
            domain: product
            description: >
              VERIFY task T9.4.1 was completed correctly with quality.


              **ORIGINAL TASK**: [REM] Verify: SQLite FTS5 index for code search
              (SAFE: new index)


              **VERIFICATION CHECKLIST**:


              1. **Code Exists**:
                 - Locate all files modified/created for task T9.4.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T9.4.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T9.4.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T9.4.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T9.4.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:

              - Missing code → Implement it

              - Failing tests → Fix the bugs OR fix the tests (if tests are
              wrong)

              - Build errors → Fix compilation issues

              - Doc mismatches → Update docs to match code OR implement missing
              features

              - No runtime evidence → Actually run it and capture evidence

              - Superficial completion → Do the real work


              **EXIT CRITERIA**:

              - ✅ Code exists and is complete

              - ✅ All tests pass (100%)

              - ✅ Build passes (0 errors)

              - ✅ Documentation matches implementation

              - ✅ Runtime evidence provided (screenshot/logs)

              - ✅ Adversarial detector: APPROVED

              - ✅ Integration verified (feature works in context)

              - ✅ No critical issues found


              **SEVERITY**: Task T9.4.1 was marked "done" but needs verification

              **PRIORITY**: Must verify before claiming remediation complete


              **MANDATORY EVIDENCE COLLECTION** (for quality gates):


              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:

              - [ ] Build output collected (0 errors required)

              - [ ] Test output collected (all passing required)

              - [ ] Audit output collected (0 vulnerabilities required)

              - [ ] Runtime evidence provided (artifacts/logs/screenshots)

              - [ ] Documentation verified (no mismatches)


              **NOTE**: Without ALL evidence above, quality gates will
              AUTOMATICALLY REJECT.

              Do NOT skip evidence collection. Do NOT assume quality gates will
              pass without proof.
          - id: REMEDIATION-ALL-MCP-SERVER
            title: "[CRITICAL] Audit ALL MCP server code for quality issues"
            status: done
            dependencies: []
            exit_criteria:
              - Build passes with 0 errors
              - ALL tests pass (currently 865 tests)
              - npm audit shows 0 vulnerabilities
              - Quality gate adversarial detector APPROVED
              - Runtime evidence provided for each major system
              - No superficial completion detected
              - No documentation-code mismatches
              - Decision log shows APPROVED status
            domain: product
            description: >-
              AUDIT all MCP server implementation for quality issues.


              **SCOPE**: tools/wvo_mcp/src/ (all TypeScript code)


              **WHAT TO AUDIT**:

              1. Orchestrator: unified_orchestrator.ts,
              quality_gate_orchestrator.ts

              2. State Management: state_machine.ts, roadmap_tracker.ts

              3. Model Routing: model_router.ts, capacity tracking

              4. Telemetry: logging, metrics, analytics

              5. Critics: All critic implementations

              6. Resource Management: agent_pool.ts,
              resource_lifecycle_manager.ts


              **VERIFICATION STEPS**:

              1. Build: cd tools/wvo_mcp && npm run build (0 errors required)

              2. Tests: npm test (ALL must pass, currently 985+)

              3. Audit: npm audit (0 vulnerabilities required)

              4. Coverage: npm run test:coverage (80%+ on orchestrator code)

              5. Adversarial detector: Run on all modules

              6. Runtime: Start orchestrator, run 1 task end-to-end, collect
              logs

              7. Decision log: Verify
              state/analytics/quality_gate_decisions.jsonl has real decisions


              **EXIT CRITERIA**:

              - ✅ Build: 0 errors

              - ✅ Tests: 985/985+ passing

              - ✅ Audit: 0 vulnerabilities

              - ✅ Coverage: 80%+ on critical code

              - ✅ Runtime evidence: Screenshots/logs of orchestrator running

              - ✅ Decision log: Real decisions from autopilot (not demos)

              - ✅ No superficial completion detected
          - id: REMEDIATION-ALL-QUALITY-GATES-DOGFOOD
            title: "[CRITICAL] Integrate multi-domain genius-level reviews (GATE 5)"
            status: done
            dependencies: []
            exit_criteria:
              - unknown
              - Decision log shows decisions from REAL tasks (not demos)
              - Post-task verification confirmed in autopilot logs
              - unknown
              - unknown
            domain: product
            description: >-
              INTEGRATE multi-domain genius-level reviews as GATE 5.


              Transform quality gates from checkbox thinking to expert-level
              domain analysis.


              **WHAT TO BUILD**:

              1. Import DomainExpertReviewer into quality_gate_orchestrator.ts

              2. Add GATE 5: Multi-domain expert review (after GATE 4)

              3. Update QualityGateDecision to include domainExpert results

              4. Extend makeConsensusDecision() to check domain expert approval

              5. Update TaskEvidence to include title + description


              **IMPLEMENTATION**:

              - quality_gate_orchestrator.ts:20-25: Add imports

              - quality_gate_orchestrator.ts:71: Update interface

              - quality_gate_orchestrator.ts:105: Instantiate reviewer

              - quality_gate_orchestrator.ts:255-258: Execute GATE 5

              - quality_gate_orchestrator.ts:397-421: Update consensus

              - adversarial_bullshit_detector.ts:26-27: Add title/description to
              TaskEvidence


              **VERIFICATION**:

              1. Build: cd tools/wvo_mcp && npm run build (0 errors)

              2. Tests: npm test (985+ passing)

              3. Audit: npm audit (0 vulnerabilities)

              4. Run orchestrator: See GATE 5 execute with 3+ domain experts

              5. Check logs: Domain expert reviews appear in
              quality_gate_decisions.jsonl

              6. Test rejection: Verify tasks rejected when experts find issues


              **EXIT CRITERIA**:

              - ✅ GATE 5 integrated and executing

              - ✅ Build: 0 errors

              - ✅ Tests: 985/985+ passing

              - ✅ Audit: 0 vulnerabilities

              - ✅ Logs show domain expert reviews (3+ experts per task)

              - ✅ Evidence document created
              (docs/DOMAIN_EXPERT_INTEGRATION_EVIDENCE.md)
          - id: REMEDIATION-ALL-TESTING-INFRASTRUCTURE
            title: "[CRITICAL] Verify testing infrastructure quality"
            status: done
            dependencies: []
            exit_criteria:
              - npm test shows 865/865 passing (100%)
              - Test quality validation passes on ALL test files
              - No unconditional success mocks detected
              - Integration tests exist and pass
              - Edge case coverage verified
            domain: product
            description: >-
              VERIFY testing infrastructure is high-quality and tests are
              meaningful.


              **WHAT TO VERIFY**:

              - Test QUALITY: Do tests verify behavior or just run code?

              - Test COVERAGE: Is critical code actually tested?

              - Test ASSERTIONS: Are assertions checking real conditions?

              - Integration tests: Do they test actual integration or just
              mocks?

              - Runtime tests: Do critical systems have end-to-end tests?


              **SPECIFIC FILES TO AUDIT**:

              1. adversarial_bullshit_detector.test.ts: 15+ tests, all 6
              detection categories

              2. quality_gate_orchestrator.test.ts: All 5 gates + consensus

              3. unified_orchestrator.test.ts: End-to-end task execution

              4. domain_expert_reviewer.test.ts: Multi-domain reviews

              5. state_machine.test.ts: State transitions, concurrent access

              6. model_router.test.ts: Model selection, capacity tracking


              **VERIFICATION STEPS**:

              1. Run tests: npm test (must ALL pass)

              2. Coverage: npm run test:coverage (generate report)

              3. Break test: Intentionally break critical code, verify tests
              FAIL

              4. Fix test: Fix the break, verify tests PASS

              5. Review assertions: Check every test's assertions

              6. Add missing tests: For untested critical code


              **QUALITY CHECKS**:

              - Are tests checking behavior (GOOD) or just running code (BAD)?

              - Do tests use meaningful assertions (GOOD) or just
              expect(x).toBeDefined() (BAD)?

              - Do integration tests actually integrate (GOOD) or mock
              everything (BAD)?

              - Do tests fail when code breaks (GOOD) or always pass (BAD)?


              **EXIT CRITERIA**:

              - ✅ All tests passing (985+)

              - ✅ Coverage: 80%+ on critical code

              - ✅ Tests demonstrate they catch bugs (tested by breaking code)

              - ✅ No superficial tests (all verify behavior)

              - ✅ Integration tests run orchestrator end-to-end

              - ✅ Evidence: Coverage report + test quality analysis
          - id: REMEDIATION-T1.1.2-PREFECT-FLOW
            title: "[URGENT] Convert ingestion to actual Prefect flow"
            status: done
            dependencies: []
            exit_criteria:
              - Code uses @flow and @task decorators from Prefect
              - Flow can be registered with Prefect server
              - Flow execution produces Prefect UI artifacts
              - Checkpointing uses Prefect state management
              - "Runtime evidence: Prefect UI screenshot showing flow run"
              - Tests validate Prefect integration
            domain: product
            description: >-
              REMEDIATION: Task T1.1.2 was marked "done" but WRONG framework
              used.


              **Audit Finding**: Code exists but doesn't use Prefect.

              - Found: shared/libs/ingestion/ contains code

              - Problem: No @flow or @task decorators found

              - Problem: Not using Prefect framework despite task requirement


              **Required Work**:

              1. Convert ingestion pipeline to use Prefect decorators

              2. Define flow with @flow decorator

              3. Define tasks with @task decorator

              4. Integrate with Prefect state/checkpoint system

              5. Test flow registration and execution

              6. Document Prefect-specific features used


              **Verification Requirements**:

              - grep -r "@flow|@task" shared/libs/ingestion/ returns matches

              - Can run: prefect deployment build (or equivalent)

              - Flow appears in Prefect UI (screenshot required)

              - Tests cover Prefect integration


              **Severity**: HIGH - Wrong technology implementation

              **Priority**: HIGH - Must use correct framework
          - id: REMEDIATION-T2.2.1-GAM-BASELINE
            title: "[URGENT] Implement missing Weather-aware GAM baseline"
            status: done
            dependencies: []
            exit_criteria:
              - Training script apps/modeling/train_weather_gam.py exists and
                runs
              - Script produces model artifacts in expected location
              - Documentation in WEATHER_PROOF_OF_CONCEPT.md references actual
                implementation
              - "Runtime evidence: screenshot of training run with metrics"
              - Tests exist and pass for GAM training pipeline
            domain: product
            description: >-
              REMEDIATION: Task T2.2.1 was marked "done" but implementation is
              missing.


              **Audit Finding**: Documentation exists but NO code found.

              - Expected: apps/modeling/train_weather_gam.py or
              apps/modeling/weather_gam.py

              - Found: Nothing

              - Documentation references features that don't exist


              **Required Work**:

              1. Implement weather-aware GAM baseline training script

              2. Integrate with existing feature pipeline

              3. Produce model artifacts matching documentation claims

              4. Add tests covering training, prediction, and validation

              5. Run end-to-end and provide runtime evidence


              **Verification Requirements**:

              - Build passes

              - Tests pass

              - Script actually runs: python apps/modeling/train_weather_gam.py
              --dry-run

              - Produces expected outputs

              - Documentation matches implementation


              **Severity**: HIGH - Claimed feature completely missing

              **Priority**: CRITICAL - Must fix before claiming task complete
          - id: REMEDIATION-T6.3.1-PERF-BENCHMARKING
            title: "[URGENT] Fix empty performance benchmarking system"
            status: done
            dependencies: []
            exit_criteria:
              - state/analytics/orchestration_metrics.json contains >0 decision
                entries
              - Performance benchmarks exist in docs with REAL data
              - MCP overhead measured and documented
              - Checkpoint size limits validated
              - Token usage tracked over time
              - "Runtime evidence: metrics collection in action"
            domain: product
            description: >-
              REMEDIATION: Task T6.3.1 was marked "done" but system is EMPTY.


              **Audit Finding**: Infrastructure exists but unused (0 metrics
              recorded).

              - Found: state/analytics/orchestration_metrics.json (empty: 0
              decisions)

              - Found: docs/MODEL_PERFORMANCE_THRESHOLDS.md (generic thresholds,
              no real data)

              - Problem: System built but never actually used


              **Required Work**:

              1. Actually USE the performance monitoring system

              2. Collect real performance data from autopilot runs

              3. Measure MCP overhead vs direct calls

              4. Measure checkpoint sizes over time

              5. Track token efficiency metrics

              6. Update docs with ACTUAL measured data


              **Verification Requirements**:

              - Run autopilot for at least 10 iterations

              - Verify metrics file grows (check file size before/after)

              - Extract sample metrics: jq '.decisions | length'
              state/analytics/orchestration_metrics.json

              - Document shows real numbers from real runs


              **Severity**: MEDIUM - Feature exists but superficially completed

              **Priority**: HIGH - Must demonstrate system actually works
          - id: TASK-RESEARCH-AD-AUTOMATION
            title: Research Meta/Google ads automation constraints
            status: done
            dependencies: []
            exit_criteria:
              - doc:docs/api/ads_capability_matrix.md
              - doc:docs/security/ads_automation_sop.md
              - artifact:state/artifacts/research/ads_api_compliance.json
            domain: product
            description: Consolidate API capabilities, credential flows, and compliance
              requirements so E5 automation tasks can launch with allocator and
              security critics satisfied.
          - id: TASK-RESEARCH-CONSENSUS-BENCHMARKS
            title: Research staffing heuristics for consensus engine rollout
            status: done
            dependencies: []
            exit_criteria:
              - artifact:state/analytics/consensus_workload.json
              - doc:docs/research/consensus_staffing_playbook.md
              - artifact:state/analytics/orchestration_metrics.json
            domain: product
            description: Collect decision workload traces, benchmark quorum cost, and codify
              staffing heuristics so T3.3.x tasks can wire consensus + telemetry
              with real evidence.
          - id: TASK-RESEARCH-DATA-GUARDRAILS
            title: Research ingestion data-quality guardrails
            status: done
            dependencies: []
            exit_criteria:
              - doc:docs/research/data_quality_guardrails.md
              - artifact:state/analytics/data_quality_baselines.json
              - artifact:state/artifacts/research/geocoding_coverage_report.json
            domain: product
            description: Define geocoding coverage thresholds, schema validation rules, and
              incremental dedupe checks so E7 and E12/E13 work inherit trusted
              data.
          - id: TASK-RESEARCH-DEMO
            title: "Research: evaluate cache warming pattern"
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: Investigate academic cache warming strategies and summarize
              findings for orchestration upgrades.
          - id: TASK-RESEARCH-EXPERIENCE-VALIDATION
            title: Research Experiments/Reports validation with core personas
            status: done
            dependencies: []
            exit_criteria:
              - doc:docs/research/experiments_reports_validation.md
              - artifact:state/artifacts/research/experiments_sessions
              - doc:docs/UX_CRITIQUE.md
            domain: product
            description: Run moderated sessions across Sarah, Leo, and Priya personas,
              produce evidence-backed acceptance metrics, and update UX briefs
              so T3.4.x implementation unblocks without rework.
          - id: TASK-RESEARCH-SWEEP
            title: Research backlog sweep
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: Identify high-impact roadmap areas lacking research coverage and
              propose follow-up research tasks.
          - id: TEST-1
            title: Run build and verify no errors
            status: blocked
            dependencies: []
            exit_criteria: []
            domain: product
            description: Execute npm run build in tools/wvo_mcp and verify 0 TypeScript
              errors
          - id: TEST-2
            title: Run npm audit and verify no vulnerabilities
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: Execute npm audit and verify 0 vulnerabilities found
          - id: TEST-3
            title: Review CLAUDE.md for completeness
            status: done
            dependencies: []
            exit_criteria: []
            domain: product
            description: Read CLAUDE.md and verify all verification loop documentation is
              present
    description: >
      CRITICAL PRIORITY: Comprehensive quality audit of ALL work completed
      before quality gates.


      Assumption: ALL "done" tasks have quality issues until proven otherwise.


      This epic contains remediation tasks for every major system.
  - id: E1
    title: Epic 1 — Ingest & Weather Foundations
    status: done
    domain: product
    milestones:
      - id: M1.1
        title: Connector scaffolding
        status: done
        tasks:
          - id: T1.1.1
            title: Design Open-Meteo + Shopify connectors and data contracts
            status: done
            dependencies: []
            exit_criteria:
              - critic:build
              - critic:tests
              - doc:docs/INGESTION.md
            domain: product
            description: Interactive scenario flows with API endpoints for scenario
              snapshots and storybook coverage
          - id: T1.1.2
            title: Implement ingestion Prefect flow with checkpointing
            status: done
            dependencies: []
            exit_criteria:
              - critic:data_quality
              - critic:org_pm
              - artifact:experiments/ingest/dq_report.json
            domain: product
            description: Map + chart overlays with export service (PPT/CSV)
      - id: M1.2
        title: Weather harmonisation
        status: done
        tasks:
          - id: T1.2.1
            title: Blend historical + forecast weather, enforce timezone alignment
            status: done
            dependencies: []
            exit_criteria:
              - critic:forecast_stitch
              - doc:docs/weather/blending.md
            domain: product
          - id: T1.2.2
            title: Add leakage guardrails to feature builder
            status: done
            dependencies: []
            exit_criteria:
              - critic:leakage
              - critic:tests
            domain: product
    description: Stand up weather + marketing ingestion, harmonise geo/time, and
      validate data quality.
  - id: E10
    title: PHASE-6-COST — Usage-Based Optimisations
    status: done
    domain: mcp
    milestones:
      - id: M10.1
        title: Usage telemetry & guardrails
        status: done
        tasks:
          - id: T10.1.1
            title: Cost telemetry and budget alerts
            status: done
            dependencies: []
            exit_criteria:
              - Provider cost telemetry recorded in
                state/telemetry/operations.jsonl
              - Budget thresholds configurable per environment
              - Alert surfaced via state/context.md and orchestration logs
            domain: mcp
  - id: E11
    title: Resource-Aware Intelligence & Personalisation
    status: done
    domain: product
    milestones:
      - id: M11.1
        title: Capability Detection
        status: done
        tasks:
          - id: T11.1.1
            title: Implement hardware probe & profile persistence
            status: done
            dependencies: []
            exit_criteria:
              - critic:build
              - doc:docs/ROADMAP.md
            domain: product
          - id: T11.1.2
            title: Adaptive scheduling for heavy tasks
            status: done
            dependencies: []
            exit_criteria:
              - critic:tests
              - artifact:state/device_profiles.json
            domain: product
      - id: M11.2
        title: Falcon Design System & Award-ready UX
        status: done
        tasks:
          - id: T11.2.1
            title: Design system elevation (motion, typography, theming)
            status: done
            dependencies: []
            exit_criteria:
              - critic:design_system
              - doc:docs/WEB_DESIGN_SYSTEM.md
            domain: product
          - id: T11.2.2
            title: Award-level experience audit & remediation
            status: done
            dependencies: []
            exit_criteria:
              - critic:exec_review
              - artifact:docs/UX_CRITIQUE.md
            domain: product
          - id: T11.2.3
            title: Extend calm/aero theme tokens to Automations and Experiments surfaces
            status: done
            dependencies: []
            exit_criteria:
              - artifact:apps/web/styles/themes/calm.ts
              - artifact:apps/web/styles/themes/aero.ts
              - critic:design_system
            domain: product
          - id: T11.2.4
            title: Refactor landing/marketing gradients into reusable tokens
            status: done
            dependencies: []
            exit_criteria:
              - artifact:apps/web/styles/tokens/gradients.md
              - critic:design_system
            domain: product
          - id: T11.2.5
            title: Centralize retry button styles in shared component once App Router lands
            status: done
            dependencies: []
            exit_criteria:
              - artifact:apps/web/components/buttons/RetryButton.tsx
              - unknown
              - critic:design_system
            domain: product
          - id: T11.2.6
            title: Formalize shared panel mixin (border + shadow) to reduce overrides
            status: done
            dependencies: []
            exit_criteria:
              - artifact:apps/web/styles/mixins/panel.css
              - critic:design_system
            domain: product
    description: Auto-detect hardware, adapt workloads, and guarantee great
      performance on constrained machines.
  - id: E12
    title: Epic 12 — Weather Model Production Validation
    status: blocked
    domain: product
    milestones:
      - id: M12.0
        title: Synthetic Multi-Tenant Dataset Generation
        status: blocked
        tasks:
          - id: T12.0.1
            title: Generate synthetic multi-tenant dataset with weather-sensitive products
            status: blocked
            dependencies: []
            exit_criteria:
              - artifact:storage/seeds/synthetic/*.parquet
              - artifact:state/analytics/synthetic_tenant_profiles.json
              - critic:data_quality
            domain: product
            description: Create 4 simulated tenants with Shopify products, Meta/Google ads
              spend, Klaviyo data, and weather-driven demand patterns
          - id: T12.0.2
            title: Validate synthetic data quality and weather correlation
            status: pending
            dependencies: []
            exit_criteria:
              - artifact:state/analytics/synthetic_data_validation.json
              - artifact:docs/DATA_GENERATION.md
              - critic:data_quality
            domain: product
            description: Confirm data volume, coverage, completeness; measure weather
              elasticity for each tenant
          - id: T12.0.3
            title: Document synthetic tenant characteristics
            status: done
            dependencies: []
            exit_criteria:
              - artifact:docs/SYNTHETIC_TENANTS.md
              - artifact:state/analytics/tenant_weather_profiles.json
              - critic:data_quality
            domain: product
            description: Create data dictionary with tenant profiles, weather sensitivity,
              expected model behaviors
      - id: M12.1
        title: Weather ingestion + feature QA
        status: pending
        tasks:
          - id: T12.1.1
            title: Run smoke-context and weather ingestion regression suite nightly
            status: pending
            dependencies: []
            exit_criteria:
              - artifact:state/telemetry/weather_ingestion.json
              - critic:data_quality
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:causal
            domain: product
          - id: T12.1.2
            title: Validate feature store joins against historical weather baselines
            status: done
            dependencies: []
            exit_criteria:
              - artifact:experiments/weather/feature_backfill_report.md
              - critic:forecast_stitch
              - critic:tests
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:data_quality
              - critic:causal
            domain: product
      - id: M12.2
        title: Weather model capability sign-off
        status: pending
        tasks:
          - id: T12.2.1
            title: Backtest weather-aware model vs control across top tenants
            status: done
            dependencies: []
            exit_criteria:
              - artifact:experiments/weather/model_backtest_summary.md
              - critic:causal
              - critic:allocator
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
            domain: product
          - id: T12.2.2
            title: Publish weather capability runbook and monitoring dashboards
            status: pending
            dependencies: []
            exit_criteria:
              - artifact:apps/web/pages/ops/weather-capabilities.tsx
              - critic:org_pm
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:causal
            domain: product
      - id: M12.3
        title: Weather-Aware MMM Model Training
        status: pending
        tasks:
          - id: T12.3.1
            title: Train weather-aware MMM on validated 90-day tenant data
            status: pending
            dependencies: []
            exit_criteria:
              - artifact:experiments/mcp/mmm_weather_model.json
              - critic:causal
              - critic:academic_rigor
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
            domain: product
            description: Train multi-channel MMM using validated 90-day data with weather
              features integrated
          - id: T12.3.2
            title: Implement weather sensitivity elasticity estimation
            status: pending
            dependencies: []
            exit_criteria:
              - artifact:docs/models/weather_elasticity_analysis.md
              - critic:causal
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
            domain: product
            description: Quantify how demand elasticity varies by weather (temperature
              sensitivity, rain impact, seasonal patterns)
          - id: T12.3.3
            title: Ship production MMM inference service with real-time weather scoring
            status: pending
            dependencies: []
            exit_criteria:
              - artifact:apps/api/routes/models/weather_mixin.py
              - artifact:apps/worker/models/mmm_weather_inference.py
              - critic:tests
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:causal
            domain: product
            description: Deploy MMM with weather features as production inference service
      - id: M12.Demo
        title: Executive Demo & Stakeholder Sign-Off
        status: pending
        tasks:
          - id: T12.Demo.1
            title: Build interactive demo UI showing weather impact on ROAS
            status: pending
            dependencies:
              - T12.UX.1
              - T12.UX.2
              - T12.UX.3
              - T12.UX.4
            exit_criteria:
              - artifact:apps/web/src/pages/demo-weather-analysis.tsx
              - critic:design_system
              - artifact:state/artifacts/screenshots/demo_ui_iteration/
            domain: product
            description: Create web UI that lets stakeholders toggle weather on/off and see
              impact on predicted revenue and ROAS for each synthetic tenant.
              Interactive proof that weather matters for demand forecasting.
              DESIGN EXCELLENCE REQUIRED - Follow M12.UXExcellence standards
              (Playwright iteration, world-class inspiration, surprise &
              delight, zero AI aesthetic). Use screenshot_session for visual QA.
              Validate with design_system critic.
          - id: T12.Demo.2
            title: Record demo video and create stakeholder brief
            status: pending
            dependencies: []
            exit_criteria:
              - artifact:docs/WEATHER_DEMO_BRIEF.md
              - artifact:state/artifacts/stakeholder/weather_demo_script.md
            domain: product
            description: Record 5-min demo video showing weather-aware model in action.
              Create 1-page brief for executives explaining business impact
              (revenue upside, forecast accuracy improvement, ROAS optimization
              potential).
      - id: M12.PoC
        title: Proof of Concept & Model Testing
        status: pending
        tasks:
          - id: T12.PoC.1
            title: Train weather-aware model on synthetic tenant data
            status: pending
            dependencies: []
            exit_criteria:
              - artifact:experiments/mcp/weather_poc_model.pkl
              - artifact:experiments/mcp/weather_poc_metrics.json
              - critic:academic_rigor
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
            domain: product
            description: Train a baseline weather-aware regression model on each synthetic
              tenant (high/extreme/medium/none sensitivity) to validate the
              weather correlation detection works across different product types
              and sensitivity profiles.
          - id: T12.PoC.2
            title: Validate PoC model predictions on hold-out data
            status: pending
            dependencies: []
            exit_criteria:
              - artifact:experiments/mcp/weather_poc_validation.json
              - critic:causal
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
            domain: product
            description: "Test PoC model on final 30 days of each synthetic tenant. Verify
              that: (1) High/Extreme tenants show strong weather effects, (2)
              None tenant shows no weather effect, (3) Model R² > 0.6 on
              validation set"
          - id: T12.PoC.3
            title: Create PoC demo results and proof brief
            status: done
            dependencies: []
            exit_criteria:
              - artifact:docs/WEATHER_PROOF_OF_CONCEPT.md
              - artifact:experiments/mcp/poc_demo_charts.json
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
            domain: product
            description: "Create executive brief demonstrating that weather-aware modeling
              works: show before/after model performance, weather elasticity
              coefficients, and prediction examples. Use this to get stakeholder
              buy-in before full MMM training."
      - id: M12.UXExcellence
        title: Design Excellence Infrastructure (Lower Priority - After Data Gen)
        status: pending
        tasks:
          - id: T12.UX.1
            title: Setup Playwright screenshot workflow for design iteration
            status: pending
            dependencies:
              - T12.0.1
            exit_criteria:
              - artifact:state/screenshot_config.yaml
              - artifact:docs/DESIGN_WORKFLOW.md
              - critic:tests
            domain: product
            description: Configure screenshot_session, screenshot_capture_multiple tools for
              visual design iteration. Document workflow for
              Build→Screenshot→Critique→Refine loop. PRIORITY NOTE - Execute
              only after T12.0.1 (data generation) completes.
          - id: T12.UX.2
            title: Create design research process and inspiration library
            status: pending
            dependencies:
              - T12.0.1
            exit_criteria:
              - artifact:docs/DESIGN_INSPIRATION.md
              - artifact:state/design_references.json
            domain: product
            description: Document process for researching award-winning UIs (Awwwards, FWA,
              Stripe, Linear, Observable). Create reference library of
              world-class design patterns to inform WeatherVane UI decisions.
              Extract principles from Dieter Rams, Müller-Brockmann, Vignelli,
              Paul Rand.
          - id: T12.UX.3
            title: Establish surprise & delight checklist and validation criteria
            status: pending
            dependencies:
              - T12.0.1
            exit_criteria:
              - artifact:docs/SURPRISE_DELIGHT_CHECKLIST.md
              - artifact:tools/wvo_mcp/src/critics/ux_delight_scorer.ts
            domain: product
            description: Create checklist for trivial delights (micro-interactions, loading
              states, empty states, witty copy) and non-trivial delights
              (anticipatory UX, intelligent recovery, time-saving magic).
              Validation - Would user screenshot and share? Would Don
              Norman/Kathy Sierra/Julie Zhuo approve?
          - id: T12.UX.4
            title: Configure design_system critic for zero-AI-aesthetic enforcement
            status: pending
            dependencies:
              - T12.UX.1
              - T12.UX.2
              - T12.UX.3
            exit_criteria:
              - artifact:tools/wvo_mcp/src/critics/design_system_enhanced.ts
              - critic:design_system
            domain: product
            description: Enhance design_system critic to detect generic gradients, stock
              layouts, template components. Enforce heritage design principles.
              Integrate with screenshot workflow to compare against world-class
              references. Block merge if AI aesthetic detected.
    description: Prioritise end-to-end weather ingestion QA, model backtests, and
      operational readiness so WeatherVane shiproom can demo weather insights
      with confidence.
  - id: E13
    title: Epic 13 — Weather-Aware Modeling Reality
    status: pending
    domain: product
    milestones:
      - id: M13.1
        title: Data Backbone Verified
        status: pending
        tasks:
          - id: T13.1.1
            title: Validate 90-day tenant data coverage across sales, spend, and weather
            status: done
            dependencies: []
            exit_criteria:
              - artifact:experiments/features/weather_join_validation.json
              - critic:data_quality passes with weather join metrics captured
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:data_quality
              - critic:causal
            domain: product
            description: Ensure the Shopify/ads/weather ingestion flows actually populate
              product_daily with geocoded spend and 90+ days of history so MMM
              inputs are real, not theoretical.
          - id: T13.1.2
            title: Autopilot guardrail for ingestion + weather drift
            status: pending
            dependencies: []
            exit_criteria:
              - critic:weather_coverage autop-run nightly with failure
                escalation to Atlas
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:data_quality
              - critic:causal
            domain: product
            description: Bake data completeness checks into Autopilot so future
              weather-awareness regressions trigger automated investigations.
          - id: T13.1.3
            title: Implement product taxonomy auto-classification with weather affinity
            status: done
            dependencies: []
            exit_criteria:
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:data_quality
              - critic:causal
            domain: product
            description: >
              Auto-classify products from Shopify/Meta/Google using LLM
              (Claude/GPT-4) to tag products with weather affinity and category
              hierarchy. This enables product-level modeling (not just
              brand-level) and cold-start for new brands.
          - id: T13.1.4
            title: Data quality validation framework (verify data fitness for ML)
            status: done
            dependencies: []
            exit_criteria:
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:causal
            domain: product
            description: >
              Implement data quality checks to verify data is ready for ML
              training. Prevents training models on insufficient/corrupted data.
      - id: M13.2
        title: MMM Upgrade & Backtests
        status: pending
        tasks:
          - id: T13.2.1
            title: Replace heuristic MMM with LightweightMMM adstock+saturation fit
            status: done
            dependencies: []
            exit_criteria:
              - critic:model_fit passes with synthetic recovery tests
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:causal
            domain: product
            description: Integrate the existing LightweightMMM wrapper so allocations use
              Bayesian adstock/saturation estimates instead of covariance
              heuristics.
          - id: T13.2.2
            title: Build MMM backtesting + regression suite
            status: pending
            dependencies: []
            exit_criteria:
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:causal
            domain: product
            description: Establish out-of-sample evaluation so MMM recommendations are
              validated continuously.
          - id: T13.2.3
            title: Replace heuristic allocator with constraint-aware optimizer
            status: done
            dependencies: []
            exit_criteria:
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:causal
            domain: product
            description: >
              Replace heuristic allocation rules (±10% budget adjustments) with
              proper constrained optimization. Use cvxpy or OR-Tools to maximize
              ROAS subject to budget, inventory, and platform constraints.
      - id: M13.3
        title: Causal & Geography Alignment
        status: pending
        tasks:
          - id: T13.3.1
            title: Swap uplift propensity scoring with DID/synthetic control for weather
              shocks
            status: pending
            dependencies: []
            exit_criteria:
              - critic:causal passes with new methodology notes in
                docs/CAUSAL_LIMITATIONS.md
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:causal
            domain: product
            description: Adopt causal estimators appropriate for non-manipulable treatments
              so weather impact claims are statistically defensible.
          - id: T13.3.2
            title: Implement DMA-first geographic aggregation with hierarchical fallback
            status: done
            dependencies: []
            exit_criteria:
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:causal
            domain: product
            description: Resolve the open question on geographic granularity by codifying
              DMA-first modeling with automatic fallback.
      - id: M13.4
        title: Autopilot Meta-Critique Loop
        status: pending
        tasks:
          - id: T13.4.1
            title: Add modeling reality critic to Autopilot
            status: done
            dependencies: []
            exit_criteria:
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
            domain: product
            description: Teach Autopilot to generate the sort of gap analysis we just
              performed so future discrepancies surface automatically.
          - id: T13.4.2
            title: Meta-evaluation playbook for modeling roadmap
            status: pending
            dependencies: []
            exit_criteria:
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
            domain: product
            description: Provide a repeatable process—documentation, scheduling, and
              telemetry—for leadership to review modeling execution against
              strategy.
      - id: M13.5
        title: Weather-Aware Allocation Model Deployment
        status: pending
        tasks:
          - id: T13.5.1
            title: Train weather-aware allocation model on top of MMM baseline
            status: done
            dependencies: []
            exit_criteria:
              - artifact:experiments/allocation/weather_aware_model.json
              - critic:allocator
              - critic:causal
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
            domain: product
            description: Build allocation optimization model that incorporates
              weather-driven demand elasticity from MMM training
          - id: T13.5.2
            title: Implement weather-responsive budget allocation constraints
            status: pending
            dependencies: []
            exit_criteria:
              - artifact:experiments/allocation/constraint_validation.json
              - critic:tests
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
              - critic:causal
            domain: product
            description: Add constraints that adjust budget allocation based on weather
              forecasts (e.g., reduce spend on low-demand weather days)
          - id: T13.5.3
            title: Deploy weather-aware allocator to production
            status: pending
            dependencies: []
            exit_criteria:
              - artifact:apps/api/routes/allocate.py updated with weather model
              - artifact:apps/worker/allocation_service.py deployed
              - critic:tests
              - critic:allocator
              - metric:r2 > 0.50
              - metric:beats_baseline > 1.10
              - critic:modeling_reality_v2
            domain: product
            description: Ship weather-aware allocation as the primary recommendation engine
    description: Close the execution gap between sophisticated modeling plans and
      the current codebase, while embedding Autopilot self-critique so these
      regressions cannot hide in the future.
  - id: E13.1
    title: Research and design for Epic 13 — Weather-Aware Modeling Reality
    status: pending
    domain: product
    milestones: []
    description: "Research phase: Understand requirements and design approach for
      Epic 13 — Weather-Aware Modeling Reality"
  - id: E13.2
    title: Implement Epic 13 — Weather-Aware Modeling Reality
    status: pending
    domain: product
    milestones: []
    description: "Implementation phase: Execute the plan for Epic 13 — Weather-Aware
      Modeling Reality"
  - id: E13.3
    title: Validate and test Epic 13 — Weather-Aware Modeling Reality
    status: pending
    domain: product
    milestones: []
    description: "Validation phase: Test and verify Epic 13 — Weather-Aware Modeling
      Reality"
  - id: E2
    title: Epic 2 — Features & Modeling Baseline
    status: done
    domain: product
    milestones:
      - id: M2.1
        title: Feature pipeline
        status: done
        tasks:
          - id: T2.1.1
            title: Build lag/rolling feature generators with deterministic seeds
            status: done
            dependencies: []
            exit_criteria:
              - critic:build
              - critic:tests
              - critic:data_quality
            domain: product
      - id: M2.2
        title: Baseline modeling
        status: done
        tasks:
          - id: T2.2.1
            title: Train weather-aware GAM baseline and document methodology
            status: done
            dependencies: []
            exit_criteria:
              - critic:causal
              - critic:academic_rigor
              - doc:docs/models/baseline.md
            domain: product
    description: Ship lagged features, baseline models, and evaluation harness.
  - id: E3
    title: Epic 3 — Allocation & UX
    status: done
    domain: product
    milestones:
      - id: M3.1
        title: Allocator guardrails
        status: done
        tasks:
          - id: T3.1.1
            title: Implement budget allocator stress tests and regret bounds
            status: done
            dependencies: []
            exit_criteria:
              - critic:allocator
              - critic:cost_perf
              - artifact:experiments/policy/regret.json
            domain: product
      - id: M3.2
        title: Dashboard + UX review
        status: done
        tasks:
          - id: T3.2.1
            title: Run design system critic and ensure accessibility coverage
            status: done
            dependencies: []
            exit_criteria:
              - critic:design_system
            domain: product
          - id: T3.2.2
            title: Elevate dashboard storytelling & UX
            status: done
            dependencies: []
            exit_criteria:
              - critic:design_system
              - critic:exec_review
              - doc:docs/UX_CRITIQUE.md
              - artifact:docs/product/UX_CRITIQUE.md
            domain: product
      - id: M3.3
        title: Autonomous Orchestration Blueprints
        status: done
        tasks:
          - id: T3.3.1
            title: Draft multi-agent charter & delegation mesh (AutoGen/Swarm patterns)
            status: done
            dependencies: []
            exit_criteria:
              - artifact:docs/orchestration/multi_agent_charter.md
              - critic:manager_self_check
              - critic:org_pm
            domain: product
          - id: T3.3.2
            title: Implement hierarchical consensus & escalation engine
            status: done
            dependencies: []
            exit_criteria:
              - critic:integration_fury
              - critic:manager_self_check
              - doc:docs/orchestration/consensus_engine.md
            domain: product
          - id: T3.3.3
            title: Build closed-loop simulation harness for autonomous teams
            status: done
            dependencies: []
            exit_criteria:
              - artifact:experiments/orchestration/simulation_report.md
              - critic:tests
              - critic:health_check
            domain: product
          - id: T3.3.4
            title: Instrument dynamic staffing telemetry & learning pipeline
            status: done
            dependencies: []
            exit_criteria:
              - critic:prompt_budget
              - critic:exec_review
              - artifact:state/analytics/orchestration_metrics.json
            domain: product
      - id: M3.4
        title: Experience Implementation
        status: done
        tasks:
          - id: T3.4.1
            title: Implement Plan overview page with weather-driven insights
            status: done
            dependencies: []
            exit_criteria:
              - artifact:apps/web/pages/plan.tsx
              - unknown
              - critic:design_system
              - unknown
            domain: product
          - id: T3.4.2
            title: Build WeatherOps dashboard with allocator + weather KPIs
            status: done
            dependencies: []
            exit_criteria:
              - artifact:apps/web/pages/dashboard.tsx
              - unknown
              - critic:design_system
              - unknown
            domain: product
          - id: T3.4.3
            title: Ship Experiments hub UI for uplift & incrementality reviews
            status: done
            dependencies: []
            exit_criteria:
              - artifact:apps/web/pages/experiments.tsx
              - unknown
              - critic:design_system
              - unknown
            domain: product
          - id: T3.4.4
            title: Deliver storytelling Reports view with weather + spend narratives
            status: done
            dependencies: []
            exit_criteria:
              - artifact:apps/web/pages/reports.tsx
              - unknown
              - critic:design_system
              - unknown
            domain: product
          - id: T3.4.5
            title: Conduct design_system + UX acceptance review across implemented pages
            status: done
            dependencies: []
            exit_criteria:
              - critic:design_system
              - doc:docs/product/acceptance_report.md
              - artifact:state/critics/designsystem.json
            domain: product
          - id: T3.4.6
            title: Rewrite WeatherOps dashboard around plain-language decisions
            status: done
            dependencies: []
            exit_criteria:
              - unknown
              - unknown
              - critic:design_system
              - critic:exec_review
              - unknown
            domain: product
            description: >-
              Requirements:
                - Show operators exactly what WeatherVane changed or recommends changing, with clear “what/why/next” messaging.
                - Remove jargon (“guardrail”, “triage”) in favour of user-facing language (e.g., “Overspend alert”, “Weather action”).
                - Keep first view scannable: one hero recommendation, secondary cards, optional detail drill-down.
              Standards:
                - Copy: conversational, action-oriented, no internal terminology.
                - UX: minimalist layout, responsive to desktop/tablet, accessible (WCAG AA).
                - Engineering: Playwright smoke must pass; analytics instrumentation preserved; Vitest coverage for helpers.
              Implementation Plan:
                - Draft/record design brief in docs/UX_CRITIQUE.md.
                - Refactor hero + summary components around new copy and layout.
                - Update analytics helpers/tests, run Vitest + Playwright.
                - Capture iteration in state/context.md with screenshots and critic notes.
              Deliverables:
                - Updated React/CSS modules under apps/web/src/pages/dashboard.tsx and styles.
                - Revised helper libraries/tests (apps/web/src/lib/**, tests/web/**).
                - Playwright report + screenshots stored under state/artifacts/ui/weatherops.
              Integration Points:
                - API suggestion telemetry (shared/services/dashboard_analytics_ingestion.py) ensuring copy aligns with payload fields.
                - Analytics events (`trackDashboardEvent`) and downstream dashboards; coordinate with data/ML owners if field names change.
                - Worker-generated suggestion summaries (apps/worker/flows/poc_pipeline.py) to maintain consistency across channels.
              Evidence:
                - Playwright run ID + html report.
                - design_system + exec_review critic outputs (once available).
                - Context entry summarising decisions, open questions, next iteration.
          - id: T3.4.7
            title: Reimagine Automations change log as a trust-first narrative
            status: done
            dependencies: []
            exit_criteria:
              - unknown
              - unknown
              - critic:design_system
              - critic:exec_review
              - unknown
            domain: product
            description: >-
              Requirements:
                - Explain every autonomous change in plain language (what changed, when, why, impact).
                - Provide explicit approval/rollback affordances for humans and highlight pending reviews.
                - Surface evidence (metrics, weather context, spend forecasts) inline or a click away.
              Standards:
                - Copy: transparent, confidence-building, avoids “audit/guardrail” jargon.
                - UX: timeline or table must prioritise newest changes, support filtering; accessible controls for approvals.
                - Engineering: tests updated (Vitest, Playwright), telemetry preserved, change log data schema documented.
                - ML context (if applicable): explain model confidence/reason codes clearly.
              Implementation Plan:
                - Extend docs/UX_CRITIQUE.md with Automations brief, list user questions + acceptance metrics.
                - Redesign components/layout in apps/web/src/pages/automations.tsx; integrate evidence panels.
                - Update helpers/tests, run Vitest + Playwright, capture critics.
                - Log iterations in state/context.md with before/after screenshots and open questions.
              Deliverables:
                - Updated Automations page/components/styles.
                - Supporting helper modules/tests (automationInsights, validation, etc.).
                - Evidence artifacts (Playwright report, screenshots, context notes).
              Integration Points:
                - Automation audit APIs/events (apps/api/services/dashboard_service.py, shared schemas) so reason codes remain synchronized.
                - Worker automation execution logs (`apps/worker/flows/**`) and telemetry exports consumed by directors/Dana.
                - Notification channels or forthcoming approval workflows (e.g., Slack/email) to ensure new statuses map correctly.
              Evidence:
                - Playwright run + report stored under state/artifacts/ui/automations.
                - design_system + exec_review critic confirmation.
                - Context log summarising decisions, trade-offs, next steps.
    description: Allocator robustness checks, dashboards, and UI polish.
  - id: E4
    title: Epic 4 — Operational Excellence
    status: done
    domain: product
    milestones:
      - id: M4.1
        title: Optimization sprint
        status: done
        tasks:
          - id: T4.1.10
            title: Cross-market saturation optimization (fairness-aware)
            status: done
            dependencies: []
            exit_criteria:
              - critic:allocator
              - artifact:experiments/allocator/saturation_report.json
            domain: product
          - id: T4.1.3
            title: Causal uplift modeling & incremental lift validation
            status: done
            dependencies: []
            exit_criteria:
              - critic:causal
              - artifact:experiments/causal/uplift_report.json
            domain: product
          - id: T4.1.4
            title: Multi-horizon ensemble forecasting
            status: done
            dependencies: []
            exit_criteria:
              - critic:forecast_stitch
              - artifact:experiments/forecast/ensemble_metrics.json
            domain: product
          - id: T4.1.5
            title: Non-linear allocation optimizer with constraints (ROAS, spend caps)
            status: done
            dependencies: []
            exit_criteria:
              - critic:allocator
              - unknown
            domain: product
          - id: T4.1.6
            title: High-frequency spend response modeling (intraday adjustments)
            status: done
            dependencies: []
            exit_criteria:
              - critic:allocator
              - artifact:experiments/allocator/hf_response.json
            domain: product
          - id: T4.1.7
            title: Marketing mix budget solver (multi-channel, weather-aware)
            status: done
            dependencies: []
            exit_criteria:
              - critic:allocator
              - unknown
            domain: product
          - id: T4.1.8
            title: Reinforcement-learning shadow mode (safe exploration)
            status: done
            dependencies: []
            exit_criteria:
              - critic:allocator
              - artifact:experiments/rl/shadow_mode.json
            domain: product
          - id: T4.1.9
            title: Creative-level response modeling with brand safety guardrails
            status: done
            dependencies: []
            exit_criteria:
              - critic:design_system
              - artifact:experiments/creative/response_scores.json
            domain: product
    description: Maintain velocity while hardening performance and delivery processes.
  - id: E5
    title: Ad Platform Execution & Automation
    status: done
    domain: product
    milestones:
      - id: M5.1
        title: Meta Ads Command Pipeline
        status: done
        tasks:
          - id: T5.1.1
            title: Implement Meta Marketing API client (creative + campaign management)
            status: done
            dependencies: []
            exit_criteria:
              - critic:allocator
              - unknown
            domain: product
          - id: T5.1.2
            title: Meta sandbox and dry-run executor with credential vaulting
            status: done
            dependencies: []
            exit_criteria:
              - critic:security
              - artifact:experiments/meta/sandbox_run.json
            domain: product
      - id: M5.2
        title: Google Ads Execution & Budget Sync
        status: done
        tasks:
          - id: T5.2.1
            title: Google Ads API integration (campaign create/update, shared budgets)
            status: done
            dependencies: []
            exit_criteria:
              - critic:allocator
              - unknown
            domain: product
          - id: T5.2.2
            title: Budget reconciliation & spend guardrails across platforms
            status: done
            dependencies: []
            exit_criteria:
              - critic:allocator
              - artifact:experiments/allocator/spend_guardrails.json
            domain: product
      - id: M5.3
        title: QA, Rollback & Safety Harness
        status: done
        tasks:
          - id: T5.3.1
            title: Dry-run & diff visualizer for ad pushes (pre-flight checks)
            status: done
            dependencies: []
            exit_criteria:
              - critic:tests
              - artifact:state/ad_push_diffs.json
            domain: product
          - id: T5.3.2
            title: Automated rollback + alerting when performance/regression detected
            status: done
            dependencies: []
            exit_criteria:
              - critic:manager_self_check
              - artifact:experiments/allocator/rollback_sim.json
            domain: product
    description: Enable WeatherVane to programmatically create, update, monitor, and
      rollback ads across major platforms.
  - id: E6
    title: MCP Orchestrator Production Readiness
    status: done
    domain: mcp
    milestones:
      - id: M6.1
        title: Core Infrastructure Validation
        status: done
        tasks:
          - id: T6.1.1
            title: MCP server integration tests (all 25 tools across both providers)
            status: done
            dependencies: []
            exit_criteria:
              - critic:tests
              - artifact:tests/test_mcp_tools.py
              - "Guardrail: integration suite enforces blue/green safety
                invariants (no unhandled throws, DRY_RUN parity)"
            domain: mcp
          - id: T6.1.2
            title: Provider failover testing (token limit simulation & automatic switching)
            status: done
            dependencies: []
            exit_criteria:
              - critic:manager_self_check
              - artifact:experiments/mcp/failover_test.json
              - "Guardrail: circuit-breaker rollback and DISABLE_NEW kill switch
                verified under simulated failures"
            domain: mcp
          - id: T6.1.3
            title: State persistence testing (checkpoint recovery across sessions)
            status: done
            dependencies: []
            exit_criteria:
              - critic:tests
              - artifact:tests/test_state_persistence.py
              - "Guardrail: recovery flow preserves upgrade locks and safety
                state without manual intervention"
            domain: mcp
          - id: T6.1.4
            title: Quality framework validation (10 dimensions operational)
            status: done
            dependencies: []
            exit_criteria:
              - critic:manager_self_check
              - artifact:state/quality/assessment_log.json
              - "Guardrail: quality checks confirm run-safety metrics from
                blue/green playbook remain green"
            domain: mcp
      - id: M6.2
        title: Security & Reliability Hardening
        status: done
        tasks:
          - id: T6.2.1
            title: Credentials security audit (auth.json, API keys, token rotation)
            status: done
            dependencies: []
            exit_criteria:
              - critic:security
              - doc:docs/SECURITY_AUDIT.md
              - "Guardrail: audit verifies secrets handling inside blue/green
                upgrade flow and DRY_RUN constraints"
            domain: mcp
          - id: T6.2.2
            title: Error recovery testing (graceful degradation, retry logic)
            status: done
            dependencies: []
            exit_criteria:
              - critic:tests
              - artifact:experiments/mcp/error_recovery.json
              - "Guardrail: automated rollback path exercised with observation
                window + DISABLE_NEW reset"
            domain: mcp
          - id: T6.2.3
            title: Schema validation enforcement (all data contracts validated)
            status: done
            dependencies: []
            exit_criteria:
              - critic:data_quality
              - artifact:shared/contracts/*.schema.json
              - "Guardrail: dual-write / expand-cutover-contract workflow logged
                for 100% safe migrations"
            domain: mcp
          - id: T6.2.4
            title: API rate limiting & exponential backoff (Open-Meteo, Shopify, Ads APIs)
            status: done
            dependencies: []
            exit_criteria:
              - critic:allocator
              - unknown
              - "Guardrail: rate-limit handling respects worker timeouts and
                prevents cascading failures during upgrades"
            domain: mcp
      - id: M6.3
        title: Observability & Performance
        status: done
        tasks:
          - id: T6.3.1
            title: Performance benchmarking (MCP overhead, checkpoint size, token usage)
            status: done
            dependencies: []
            exit_criteria:
              - critic:cost_perf
              - artifact:experiments/mcp/performance_benchmarks.json
              - "Guardrail: benchmarks include worker swap scenarios and confirm
                resource limits (timeouts, RSS) hold"
            domain: mcp
          - id: T6.3.2
            title: Enhanced observability export (structured logs, metrics dashboards)
            status: done
            dependencies: []
            exit_criteria:
              - critic:manager_self_check
              - artifact:state/telemetry/metrics_summary.json
              - "Guardrail: telemetry captures Step 0–15 safety signals with
                alerting on breaches"
            domain: mcp
          - id: T6.3.3
            title: Autopilot loop end-to-end testing (full autonomous cycle validation)
            status: done
            dependencies: []
            exit_criteria:
              - critic:manager_self_check
              - artifact:experiments/mcp/autopilot_e2e.json
              - "Guardrail: autonomous loop validates automatic promotion +
                rollback without manual resets"
            domain: mcp
      - id: M6.4
        title: Zero-downtime self-upgrade
        status: pending
        tasks:
          - id: T6.4.0
            title: Upgrade invariants & preflight guardrails
            status: done
            dependencies: []
            exit_criteria:
              - state/upgrade.lock created before work and removed on exit
              - Preflight script validates git status, Node/npm versions, disk
                ≥500MB, sandbox availability
              - Four-step gate recorded in logs; any failure returns
                {error:"upgrade_aborted"}
            domain: mcp
            description: 'Define upgrade preflight: clean git, version sanity, ≥500MB disk,
              SQLite lock probe, and single-flight upgrade.lock. Gate promotion
              through build → unit → selfchecks → canary, aborting with
              {error:"upgrade_aborted"} on any failure.'
          - id: T6.4.1
            title: Live feature flag store with kill switch
            status: done
            dependencies: []
            exit_criteria:
              - settings table created with defaults + DISABLE_NEW
              - LiveFlags poller refreshes in-memory cache during runtime
              - Integration test flips PROMPT_MODE without restart
            domain: mcp
            description: >
              Replace environment toggles with a SQLite-backed `settings` table,
              seed

              defaults, and hot-refresh cached flags (≤500 ms poll). Include a

              `DISABLE_NEW` global kill switch that forces legacy behaviour
              instantly.
          - id: T6.4.2
            title: Blue/green worker manager & front-end proxy
            status: done
            dependencies: []
            exit_criteria:
              - WorkerManager exposes startActive/startCanary/switchToCanary
              - Front-end tool handlers call workers.getActive().call(...)
              - RPC protocol enforces ready handshake, 30s timeouts, and
                structured {ok,error} results
              - Test demonstrates zero-downtime swap between worker binaries
            domain: mcp
            description: >
              Keep the MCP front-end process stable while managing active and
              canary

              worker children over IPC. Ensure requests route through a proxy
              that can

              atomically switch to the validated canary without disconnecting
              clients.
          - id: T6.4.3
            title: Worker entrypoint with DRY_RUN safeguards
            status: done
            dependencies: []
            exit_criteria:
              - Route function covers
                health/plan/dispatch/runTool/verify/report.mo
              - SQLite opened via file:state/state.db?mode=ro when DRY_RUN=1
              - applyPatch/mutate operations rejected while DRY_RUN=1
              - tests/test_worker_dry_run.py captures read-only guarantees
            domain: mcp
            description: >
              Implement a dedicated worker entry that routes RPCs, enforces
              DRY_RUN=1

              by opening the state DB read-only, refuses mutating calls, and
              confirms

              legacy behaviour when DRY_RUN=0.
          - id: T6.4.4
            title: Canary upgrade harness & shadow validation
            status: done
            dependencies: []
            exit_criteria:
              - scripts/mcp_safe_upgrade.sh orchestrates worktree build + tests
              - Shadow checks compare active vs canary outputs in logs
              - Promotion flow documents gate order and staged routing (DRY →
                live) with metrics snapshots
              - experiments/mcp/upgrade/<ts>/report.json recorded for each run
            domain: mcp
            description: >
              Automate the upgrade flow: create a separate git worktree,
              build/test new

              code, spawn a DRY_RUN canary, run shadow health/plan/report
              checks, then

              promote only if outputs match expectations.
          - id: T6.4.5
            title: Feature flag gating for compact prompts & sandbox pool
            status: pending
            dependencies: []
            exit_criteria:
              - PROMPT_MODE, SANDBOX_MODE, SCHEDULER_MODE, SELECTIVE_TESTS,
                DANGER_GATES, MO_ENGINE read from LiveFlags
              - Regression fixtures cover legacy vs new mode per feature
              - docs/MCP_ORCHESTRATOR.md updated with flag toggle order
            domain: mcp
            description: >
              Gate compact prompt headers, sandbox pooling, scheduler WSJF mode,

              selective tests, danger gates, and MO engine behind live flags so
              they

              only activate after successful canary validation.
          - id: T6.4.6
            title: Runtime tool registration & admin flag controls
            status: pending
            dependencies: []
            exit_criteria:
              - Tool handlers return 'disabled' until corresponding flag enabled
              - settings.update, upgrade.applyPatch, route.switch commands
                exposed with structured errors
              - Operator guide added under docs/MCP_AUTOMATION.md#live-flags
            domain: mcp
            description: >
              Ensure tool surfaces remain stable while routing to v1/v2 handlers
              based

              on flags. Provide an MCP admin tool or CLI to update settings
              atomically

              without restarts.
          - id: T6.4.7
            title: Automatic rollback monitors & kill-switch reset
            status: done
            dependencies: []
            exit_criteria:
              - Heartbeat every 2s with 3-strike circuit breaker routes back to
                standby
              - Error budget (5%/2min) and SLO monitors trigger automatic
                rollback
              - DISABLE_NEW flag automatically flipped during rollback
              - docs/MCP_ORCHESTRATOR.md includes rollback playbook
            domain: mcp
            description: >
              Add health monitoring that reverts to the previous worker and
              resets

              flags when error rates spike post-promotion. Document on-call
              rollback

              steps and ensure DISABLE_NEW restores legacy behaviour.
          - id: T6.4.8
            title: Observability & resource budgets during upgrade
            status: done
            dependencies: []
            exit_criteria:
              - Span/log attributes include method, lane, ok/error, duration,
                task.id
              - runTool/plan timeouts (30s/120s) & lane concurrency limits
                enforced
              - RSS watchdog throttles batch lane when >1.5x baseline
            domain: mcp
            description: >
              Emit OTel spans (or structured JSON logs) for every worker call
              with

              timing, lane, task, and outcome metadata. Enforce concurrency,
              timeout,

              and RSS guards to prevent runaway resource usage.
    description: Validate and harden the dual-provider MCP orchestrator for
      autonomous operation while preserving 100% run safety. All milestones
      under this epic must enforce the blue/green upgrade guardrails defined in
      docs/MCP_ORCHESTRATOR.md#1113-tight-integration-playbook-steps-0-15.
  - id: E7
    title: Data Pipeline Hardening
    status: done
    domain: product
    milestones:
      - id: M7.1
        title: Geocoding & Weather Integration
        status: done
        tasks:
          - id: T7.1.1
            title: Complete geocoding integration (city->lat/lon, cache strategy)
            status: done
            dependencies: []
            exit_criteria:
              - critic:data_quality
              - unknown
            domain: product
          - id: T7.1.2
            title: Weather feature join to model matrix (prevent future leakage)
            status: done
            dependencies: []
            exit_criteria:
              - critic:leakage
              - artifact:experiments/features/weather_join_validation.json
            domain: product
          - id: T7.1.3
            title: Data contract schema validation (Shopify, weather, ads)
            status: done
            dependencies: []
            exit_criteria:
              - critic:data_quality
              - unknown
            domain: product
      - id: M7.2
        title: Pipeline Robustness
        status: done
        tasks:
          - id: T7.2.1
            title: Incremental ingestion with deduplication & checkpointing
            status: done
            dependencies: []
            exit_criteria:
              - critic:data_quality
              - unknown
            domain: product
          - id: T7.2.2
            title: Data quality monitoring & alerting (anomaly detection)
            status: done
            dependencies: []
            exit_criteria:
              - critic:data_quality
              - artifact:state/dq_monitoring.json
            domain: product
    description: Complete geocoding integration, weather feature joins, and data
      quality validation.
  - id: E8
    title: PHASE-4-POLISH — MCP Production Hardening
    status: done
    domain: mcp
    milestones:
      - id: M8.1
        title: MCP Compliance & Security
        status: done
        tasks:
          - id: T8.1.1
            title: "Lock MCP schemas to Zod shapes (SAFE: guardrail)"
            status: done
            dependencies: []
            exit_criteria:
              - utils/schema.ts returns schema.shape with guardrail comment
              - MCP entrypoints register raw shapes only
              - Autopilot documentation updated to reflect guardrail
              - critic:build passes
              - "Guardrail: validation confirms schema handling does not weaken
                blue/green safety gates"
            domain: mcp
          - id: T8.1.2
            title: "Implement command allow-list in guardrails (SAFE: additive security)"
            status: done
            dependencies: []
            exit_criteria:
              - ALLOWED_COMMANDS constant defined
              - isCommandAllowed() enforced before execution
              - Deny-list kept as secondary check
              - critic:tests passes with new test_command_allowlist.py
              - critic:manager_self_check passes
              - "Guardrail: allow-list integration verified against blue/green
                upgrade scenarios"
            domain: mcp
          - id: T8.1.3
            title: "Thread correlation IDs through state transitions (SAFE: observability
              only)"
            status: done
            dependencies: []
            exit_criteria:
              - All tool handlers generate correlationId
              - All state transitions include correlationId
              - Events in SQLite include correlation_id column populated
              - critic:manager_self_check passes
              - End-to-end trace visible in state/orchestrator.db
              - "Guardrail: correlation IDs trace compliance with Step 0–15
                safety checks"
            domain: mcp
      - id: M8.2
        title: Context & Performance Optimization
        status: done
        tasks:
          - id: T8.2.1
            title: "Implement compact evidence-pack prompt mode (SAFE: new function,
              backward compatible)"
            status: done
            dependencies: []
            exit_criteria:
              - formatForPromptCompact() returns JSON evidence pack
              - unknown
              - All coordinator calls use compact mode
              - critic:build passes
              - critic:manager_self_check passes
              - unknown
              - "Guardrail: compact mode flip integrated with Step 15 staged
                flag process"
            domain: mcp
          - id: T8.2.2
            title: "Finalize Claude↔Codex coordinator failover (SAFE: expose existing
              functionality)"
            status: done
            dependencies: []
            exit_criteria:
              - orchestrator_status tool shows coordinator type and availability
              - Telemetry includes coordinator field in execution logs
              - Documentation updated in IMPLEMENTATION_STATUS.md
              - critic:manager_self_check passes
              - Failover behavior visible and logged
              - "Guardrail: failover reporting feeds SLO/error budget monitors
                for auto rollback"
            domain: mcp
    description: Critical production readiness tasks for MCP orchestrator. Complete
      before WeatherVane v1 launch while maintaining the Step 0–15 run-safety
      guardrails
      (docs/MCP_ORCHESTRATOR.md#1113-tight-integration-playbook-steps-0-15).
  - id: E9
    title: PHASE-5-OPTIMIZATION — Performance & Observability
    status: pending
    domain: mcp
    milestones:
      - id: M9.1
        title: Cost Optimization & Caching
        status: done
        tasks:
          - id: T9.1.1
            title: "Stable prompt headers with provider caching (SAFE: additive
              optimization)"
            status: done
            dependencies: []
            exit_criteria:
              - standardPromptHeader() returns deterministic header
              - All prompts include standard header
              - Header enables provider caching (verified with API logs)
              - critic:cost_perf shows token cache hit rate
              - critic:manager_self_check passes
              - "Guardrail: caching rollout assessed via Step 0–15 safety checks
                before staying live"
            domain: mcp
          - id: T9.1.2
            title: "Batch queue for non-urgent prompts (SAFE: new queueing system)"
            status: done
            dependencies: []
            exit_criteria:
              - Priority queue with 3 lanes operational
              - Semaphore limits enforced per lane
              - Interactive tasks always get priority
              - critic:tests passes
              - critic:manager_self_check passes
              - "Guardrail: queue respects worker concurrency caps from
                blue/green playbook"
            domain: mcp
      - id: M9.2
        title: Reliability & Quality Improvements
        status: done
        tasks:
          - id: T9.2.1
            title: "Strict output DSL validation (SAFE: validation layer only)"
            status: done
            dependencies: []
            exit_criteria:
              - validateDiff() rejects non-diff outputs
              - validateJSON() rejects invalid JSON
              - Retry rate reduction measured
              - critic:tests passes
              - "Guardrail: validation enforced in canary shadow runs before
                live promotion"
            domain: mcp
          - id: T9.2.2
            title: "Idempotency keys for mutating tools (SAFE: caching layer)"
            status: done
            dependencies: []
            exit_criteria:
              - Idempotency cache operational
              - Duplicate operations return cached results
              - 1-hour TTL enforced
              - critic:tests passes
              - "Guardrail: cache respects DRY_RUN mode and avoids side effects
                during canary runs"
            domain: mcp
      - id: M9.3
        title: Production Observability
        status: pending
        tasks:
          - id: T9.3.1
            title: "OpenTelemetry spans for all operations (SAFE: tracing wrapper)"
            status: done
            dependencies: []
            exit_criteria:
              - All tool handlers instrumented
              - Spans exported to tracing backend
              - End-to-end traces visible
              - Performance insights available
              - critic:manager_self_check passes
              - "Guardrail: telemetry alerts on Step 0–15 safety breaches"
            domain: mcp
          - id: T9.3.2
            title: "Sandbox pooling for test execution (SAFE: new executor)"
            status: pending
            dependencies: []
            exit_criteria:
              - Sandbox pool with 3 pre-warmed containers
              - Test execution uses pooled sandboxes
              - 10x speedup measured
              - Fallback to non-pooled works
              - critic:tests passes
              - "Guardrail: pool enforces DRY_RUN read-only mode during canary
                validation"
            domain: mcp
      - id: M9.4
        title: Advanced Context & Search
        status: pending
        tasks:
          - id: T9.4.1
            title: "SQLite FTS5 index for code search (SAFE: new index)"
            status: done
            dependencies: []
            exit_criteria:
              - code_fts virtual table created
              - Index populated on repo sync
              - Search performance <50ms
              - critic:tests passes
            domain: mcp
          - id: T9.4.2
            title: "LSP proxy tools for symbol-aware context (SAFE: new tools)"
            status: pending
            dependencies: []
            exit_criteria:
              - tsserver and pyright proxies running
              - lsp.definition and lsp.references tools work
              - Context assembler uses LSP for code slices
              - Context relevance measured and improved
              - critic:tests passes
              - "Guardrail: LSP tools routed through worker proxy with Step 0–15
                safety enforcement"
            domain: mcp
    description: Post-v1 performance improvements and production observability. High
      ROI optimizations that still honour the blue/green guardrail contract.
  - id: E9.1
    title: Research and design for PHASE-5-OPTIMIZATION — Performance & Observability
    status: pending
    domain: product
    milestones: []
    description: "Research phase: Understand requirements and design approach for
      PHASE-5-OPTIMIZATION — Performance & Observability"
  - id: E9.2
    title: Implement PHASE-5-OPTIMIZATION — Performance & Observability
    status: pending
    domain: product
    milestones: []
    description: "Implementation phase: Execute the plan for PHASE-5-OPTIMIZATION —
      Performance & Observability"
  - id: E9.3
    title: Validate and test PHASE-5-OPTIMIZATION — Performance & Observability
    status: pending
    domain: product
    milestones: []
    description: "Validation phase: Test and verify PHASE-5-OPTIMIZATION —
      Performance & Observability"
  - id: E-AFP-ROADMAP
    title: AFP Antifragile Roadmap
    status: pending
    domain: mcp
    description: Stage-based roadmap aligning AFP/SCAS controls toward antifragile
      autonomous operations and full autonomy trials.
    milestones:
      - id: E-AFP-ROADMAP-stage1
        title: Stage 1 — Deterministic Core
        status: pending
        description: Stand up a deterministic autopilot loop with smoke evidence,
          telemetry, and ops visibility.
        tasks:
          - id: AFP-S1-BOOTSTRAP
            title: Establish Autopilot Baseline
            status: in_progress
            description: Bring MCP orchestration online with deterministic bootstrap scripts
              and health logging.
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Bootstrap executes cleanly on a fresh clone and captures restart
                  evidence.
              - prose: Health check entry appended in state/analytics/health_checks.jsonl.
            domain: mcp
            complexity_score: 5
            effort_hours: 6
            required_tools:
              - bash
              - npm
              - node
            evidence_path: state/evidence/AFP-S1-BOOTSTRAP
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S1-README-BOOTSTRAP
            title: Bootstrap README automation & directory health
            status: pending
            description: Promote docsync to the canonical local knowledge base with
              hierarchy awareness, AFP scoring, and automatic
              ancestor/descendant refresh.
            dependencies:
              depends_on:
                - AFP-S1-OPS-DASHBOARD
            exit_criteria:
              - prose: Docsync bootstrap regenerates READMEs (including parents/children) with
                  hierarchy, dependency, and AFP critical evaluation sections
                  recorded in state/analytics/readme_manifest.json.
              - prose: README automation handbook + template published; workflow instructs
                  agents to run staged updates during IMPLEMENT/VERIFY/REVIEW.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S1-README-BOOTSTRAP
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S1-LEDGER
            title: Implement Phase Ledger & Enforcer
            status: done
            description: Create immutable work-process ledger enforcing STRATEGIZE→MONITOR
              sequencing.
            dependencies:
              depends_on:
                - AFP-S1-BOOTSTRAP
            exit_criteria:
              - prose: Ledger prevents phase skips and records backtracks with hash chain
                  validation.
              - prose: Unit tests cover success and rejection paths for WorkProcessEnforcer.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S1-LEDGER
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S1-GUARDRAILS
            title: Seed Guardrail Catalog & Policy Controller
            status: done
            description: Define baseline security, quality, and governance guardrails with
              policy enforcement hooks.
            dependencies:
              depends_on:
                - AFP-S1-BOOTSTRAP
                - AFP-S1-LEDGER
            exit_criteria:
              - prose: Guardrail catalog defined and validated against policy controller.
              - prose: Guardrail check fails on intentionally bad configuration and passes on
                  baseline.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S1-GUARDRAILS
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S1-STATE-GRAPH
            title: Build State Graph & Lease Manager
            status: pending
            description: Provide orchestration backbone for leases, retries, and checkpoints
              across phases.
            dependencies:
              depends_on:
                - AFP-S1-BOOTSTRAP
                - AFP-S1-LEDGER
                - AFP-S1-GUARDRAILS
            exit_criteria:
              - prose: State graph prevents duplicate executions and supports resume after
                  crash.
              - prose: Lease and retry unit tests cover happy path and expiry scenarios.
            domain: mcp
            complexity_score: 7
            effort_hours: 10
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S1-STATE-GRAPH
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S1-TOOLS-CATALOG
            title: Normalize Tooling Metadata & Access Controls
            status: pending
            description: Enumerate tools with schema, cost, and permissions for
              deterministic routing.
            dependencies:
              depends_on:
                - AFP-S1-BOOTSTRAP
                - AFP-S1-STATE-GRAPH
            exit_criteria:
              - prose: Tool catalog published and validated against schema checks.
              - prose: Router consumes catalog without warnings and enforces allowlists.
            domain: mcp
            complexity_score: 5
            effort_hours: 6
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S1-TOOLS-CATALOG
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S1-EVIDENCE-SCHEMA
            title: Standardize Evidence Bundles & Storage
            status: pending
            description: Define schemas and validation tooling for state/evidence/<TASK_ID>
              artifacts.
            dependencies:
              depends_on:
                - AFP-S1-BOOTSTRAP
                - AFP-S1-TOOLS-CATALOG
            exit_criteria:
              - prose: Evidence schema documented and validator CLI passes on canonical bundles.
              - prose: Invalid evidence sample fails with actionable error messaging.
            domain: mcp
            complexity_score: 5
            effort_hours: 6
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S1-EVIDENCE-SCHEMA
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S1-SMOKE-SUITE
            title: Create End-to-End Work Process Smoke Tests
            status: pending
            description: Build synthetic tasks proving the 9-phase loop records required
              evidence.
            dependencies:
              depends_on:
                - AFP-S1-LEDGER
                - AFP-S1-STATE-GRAPH
                - AFP-S1-EVIDENCE-SCHEMA
            exit_criteria:
              - prose: Smoke suite fails when any phase evidence is missing.
              - prose: Happy-path run exercises STRATEGIZE→MONITOR with recorded artifacts.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - npm
              - node
              - bash
            evidence_path: state/evidence/AFP-S1-SMOKE-SUITE
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S1-SANDBOX
            title: Harden Workspace & Secrets Handling
            status: pending
            description: Provide sandboxed execution and secrets isolation for agent
              operations.
            dependencies:
              depends_on:
                - AFP-S1-STATE-GRAPH
                - AFP-S1-SMOKE-SUITE
            exit_criteria:
              - prose: Sandbox denies forbidden filesystem paths in acceptance tests.
              - prose: Secrets configuration stored outside repo with documented usage.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - bash
              - npm
              - node
            evidence_path: state/evidence/AFP-S1-SANDBOX
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S1-PROMPT-HASHES
            title: Baseline Prompts & Attestation Hashes
            status: pending
            description: Capture core prompt families, hash them, and wire attestation into
              router.
            dependencies:
              depends_on:
                - AFP-S1-TOOLS-CATALOG
                - AFP-S1-SANDBOX
            exit_criteria:
              - prose: Prompt hashes generated and stored with attestation metadata.
              - prose: Router blocks prompt drift without explicit steward approval.
            domain: mcp
            complexity_score: 5
            effort_hours: 6
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S1-PROMPT-HASHES
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S1-OBSERVABILITY
            title: Instrument OTel GenAI Spans
            status: pending
            description: Emit spans for tool calls, phases, and token usage with success
              metrics.
            dependencies:
              depends_on:
                - AFP-S1-STATE-GRAPH
                - AFP-S1-PROMPT-HASHES
            exit_criteria:
              - prose: Telemetry spans emitted with success/failure attribution.
              - prose: Snapshot stored in state/evidence/AFP-S1-OBSERVABILITY/verify/.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S1-OBSERVABILITY
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S1-OPS-DASHBOARD
            title: Minimal Operations Dashboard
            status: pending
            description: Build dashboard surfacing smoke, guardrail, and telemetry signals.
            dependencies:
              depends_on:
                - AFP-S1-SMOKE-SUITE
                - AFP-S1-OBSERVABILITY
            exit_criteria:
              - prose: Dashboard renders key widgets with live data feed.
              - prose: UI smoke test passes for primary views.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - npm
              - node
              - bash
            evidence_path: state/evidence/AFP-S1-OPS-DASHBOARD
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S1-HANDBOOK
            title: Author Agent Handbook & Workflow Contracts
            status: pending
            description: Document enforced work process, guardrails, and onboarding
              expectations.
            dependencies:
              depends_on:
                - AFP-S1-BOOTSTRAP
                - AFP-S1-OPS-DASHBOARD
            exit_criteria:
              - prose: Handbook published with STRATEGIZE→MONITOR guidance and guardrail
                  references.
              - prose: Link checker passes and stewards sign off on content accuracy.
            domain: mcp
            complexity_score: 5
            effort_hours: 6
            required_tools:
              - bash
            evidence_path: state/evidence/AFP-S1-HANDBOOK
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
      - id: E-AFP-ROADMAP-stage2
        title: Stage 2 — Reliability & Guardrails
        status: pending
        description: Make regressions impossible to ignore and keep AFP/SCAS constraints
          self-enforcing.
        tasks:
          - id: AFP-S2-FAILURE-CATALOG
            title: Classify Failure Modes & Recovery Playbooks
            status: pending
            description: Create taxonomy for task failures with remediation playbooks.
            dependencies:
              depends_on:
                - AFP-S1-HANDBOOK
            exit_criteria:
              - prose: Failure taxonomy published with remediation playbooks.
              - prose: CLI returns remediation steps for sampled failure types.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S2-FAILURE-CATALOG
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S2-EVIDENCE-LINTER
            title: Enforce Evidence Completeness
            status: pending
            description: Lint evidence bundles for required artifacts per phase.
            dependencies:
              depends_on:
                - AFP-S1-EVIDENCE-SCHEMA
                - AFP-S2-FAILURE-CATALOG
            exit_criteria:
              - prose: Evidence completeness check fails when required artifact missing.
              - prose: CI integration blocks merge on missing evidence.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S2-EVIDENCE-LINTER
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S2-PROMPT-DRIFT
            title: Real-Time Prompt Drift Monitoring
            status: pending
            description: Continuously monitor prompt usage and detect unauthorized changes.
            dependencies:
              depends_on:
                - AFP-S1-PROMPT-HASHES
                - AFP-S2-EVIDENCE-LINTER
            exit_criteria:
              - prose: Simulated prompt drift triggers alert within one loop.
              - prose: Hashed log stored for steward review.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S2-PROMPT-DRIFT
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S2-BIAS-SIGNAL
            title: Capture Agent Bias Indicators
            status: pending
            description: Log bias signals (recency, confirmation, anchoring) from agent
              decisions.
            dependencies:
              depends_on:
                - AFP-S1-OPS-DASHBOARD
                - AFP-S2-EVIDENCE-LINTER
            exit_criteria:
              - prose: Bias detector logs severity-tagged events during simulation.
              - prose: Escalations routed to reviewers with evidence bundle.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S2-BIAS-SIGNAL
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S2-TOOL-HEALTH
            title: Monitor Tool Failures & Latency
            status: pending
            description: Track tool availability, latency histograms, and failure bursts.
            dependencies:
              depends_on:
                - AFP-S1-OBSERVABILITY
                - AFP-S1-OPS-DASHBOARD
            exit_criteria:
              - prose: Tool health dashboard shows latency histogram and uptime trends.
              - prose: Simulated outage triggers escalation workflow.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S2-TOOL-HEALTH
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S2-HEALTH-REPORTER
            title: Automated Health Check Summaries
            status: pending
            description: Summarize system health daily with metrics, guardrails, and entropy
              trend.
            dependencies:
              depends_on:
                - AFP-S2-FAILURE-CATALOG
                - AFP-S2-TOOL-HEALTH
                - AFP-S2-BIAS-SIGNAL
            exit_criteria:
              - prose: Daily health report generated with key metrics and guardrail summaries.
              - prose: Report archived under state/analytics with retention policy.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S2-HEALTH-REPORTER
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S2-RELIABILITY-ORACLE
            title: Reliability Scorecard & MTTR Tracking
            status: pending
            description: Aggregate Stage 2 signals into a reliability oracle with MTTR/SLO
              tracking and gating.
            dependencies:
              depends_on:
                - AFP-S2-FAILURE-CATALOG
                - AFP-S2-EVIDENCE-LINTER
                - AFP-S2-PROMPT-DRIFT
                - AFP-S2-BIAS-SIGNAL
                - AFP-S2-TOOL-HEALTH
                - AFP-S2-HEALTH-REPORTER
            exit_criteria:
              - prose: Reliability scorecard report generated with MTTR and SLO metrics.
              - prose: Scorecard gating integrated with WorkProcessEnforcer to block failing
                  runs.
            domain: mcp
            complexity_score: 7
            effort_hours: 10
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S2-RELIABILITY-ORACLE
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S2-CHAOS-LEDGER
            title: Guardrail Deviation Ledger
            status: pending
            description: Maintain immutable ledger of guardrail deviations, remediation
              status, and follow-up tasks.
            dependencies:
              depends_on:
                - AFP-S2-RELIABILITY-ORACLE
            exit_criteria:
              - prose: Deviation ledger CLI records sample deviation with owner and due date.
              - prose: Ledger shows zero outstanding deviations before advancing to Stage 3.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S2-CHAOS-LEDGER
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S2-README-GUARDRAIL
            title: Enforce README automation guardrail
            status: pending
            description: Make README automation a hard guardrail across
              IMPLEMENT/REVIEW/MONITOR phases; structural drift must block
              merges and open remediation tasks.
            dependencies:
              depends_on:
                - AFP-S1-README-BOOTSTRAP
                - AFP-S2-RELIABILITY-ORACLE
            exit_criteria:
              - prose: Pre-commit hook and CI workflow block merges when READMEs or structural
                  metrics are stale.
              - prose: Structural warnings logged to state/analytics/readme_sync.jsonl,
                  automatically triaged into Stage 2 remediation backlog with
                  escalation guidance.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S2-README-GUARDRAIL
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
      - id: E-AFP-ROADMAP-stage3
        title: Stage 3 — Autonomous Delivery
        status: pending
        description: Enable autopilot to spec, implement, and verify new work with
          risk-tuned guardrails.
        tasks:
          - id: AFP-S3-INTENT-PARSER
            title: Classify Task Intent & Mode Selection
            status: pending
            description: Automatically infer task mode and AFP implications for planning.
            dependencies:
              depends_on:
                - AFP-S2-RELIABILITY-ORACLE
                - AFP-S2-CHAOS-LEDGER
            exit_criteria:
              - prose: Intent classifier hits ≥90% precision/recall on curated dataset.
              - prose: Misclassification review loop documented with evidence.
            domain: mcp
            complexity_score: 7
            effort_hours: 10
            required_tools:
              - python
              - node
              - npm
            evidence_path: state/evidence/AFP-S3-INTENT-PARSER
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S3-RESOURCE-BUDGETER
            title: Adaptive Resource Allocation Engine
            status: pending
            description: Allocate tokens/time per phase based on complexity, risk, and
              history.
            dependencies:
              depends_on:
                - AFP-S3-INTENT-PARSER
            exit_criteria:
              - prose: Budget model adjusts allocations in simulation scenarios.
              - prose: Override policy captured with steward sign-off.
            domain: mcp
            complexity_score: 7
            effort_hours: 10
            required_tools:
              - python
              - node
              - npm
            evidence_path: state/evidence/AFP-S3-RESOURCE-BUDGETER
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S3-AGENT-PROFILE
            title: Agent Profiling Analytics
            status: pending
            description: Track agent strengths/weaknesses to inform staffing and review
              pairings.
            dependencies:
              depends_on:
                - AFP-S3-INTENT-PARSER
                - AFP-S3-RESOURCE-BUDGETER
            exit_criteria:
              - prose: Profiling accuracy exceeds 80% compared to steward assessment.
              - prose: Dashboard screenshot stored with analysis narrative.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - python
              - node
              - npm
            evidence_path: state/evidence/AFP-S3-AGENT-PROFILE
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S3-SCENARIO-SIM
            title: Scenario Simulator for Planner Strategies
            status: pending
            description: Simulate strategy decisions using historical data to inform
              STRATEGIZE.
            dependencies:
              depends_on:
                - AFP-S3-INTENT-PARSER
                - AFP-S3-RESOURCE-BUDGETER
            exit_criteria:
              - prose: Scenario simulator accuracy exceeds 75% vs actual outcomes.
              - prose: Simulation outputs archived with evaluation report.
            domain: mcp
            complexity_score: 7
            effort_hours: 10
            required_tools:
              - python
              - node
              - npm
            evidence_path: state/evidence/AFP-S3-SCENARIO-SIM
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S3-ADAPTIVE-REVIEWS
            title: Reviewer Assignment Engine
            status: pending
            description: Match reviewers to tasks based on risk, expertise, availability.
            dependencies:
              depends_on:
                - AFP-S3-AGENT-PROFILE
                - AFP-S3-SCENARIO-SIM
            exit_criteria:
              - prose: Reviewer assignments logged with rationale and reduction in latency ≥15%.
              - prose: Integration with WorkProcessEnforcer validated by tests.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - python
              - node
              - npm
            evidence_path: state/evidence/AFP-S3-ADAPTIVE-REVIEWS
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S3-WORKFLOW-SANDBOX
            title: Mode-Specific Sandbox Environments
            status: pending
            description: Provide dedicated sandboxes per mode with tailored guard sets.
            dependencies:
              depends_on:
                - AFP-S1-SANDBOX
                - AFP-S3-INTENT-PARSER
                - AFP-S3-RESOURCE-BUDGETER
            exit_criteria:
              - prose: Mode-specific sandbox smoke tests pass for normal/investigation/delivery
                  modes.
              - prose: Configuration bundles reviewed by SRE steward.
            domain: mcp
            complexity_score: 7
            effort_hours: 10
            required_tools:
              - bash
              - python
              - node
              - npm
            evidence_path: state/evidence/AFP-S3-WORKFLOW-SANDBOX
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S3-AUTONOMOUS-DELIVERY-LADDER
            title: End-to-End Autonomy Ladder
            status: pending
            description: Define ladder of progressively harder internal delivery tasks with
              explicit oracles and scoring.
            dependencies:
              depends_on:
                - AFP-S3-INTENT-PARSER
                - AFP-S3-RESOURCE-BUDGETER
                - AFP-S3-AGENT-PROFILE
                - AFP-S3-SCENARIO-SIM
                - AFP-S3-ADAPTIVE-REVIEWS
                - AFP-S3-WORKFLOW-SANDBOX
            exit_criteria:
              - prose: Autonomy ladder spec approved with scoring rubric.
              - prose: Harness run results stored for baseline tiers.
            domain: mcp
            complexity_score: 7
            effort_hours: 10
            required_tools:
              - node
              - npm
              - python
            evidence_path: state/evidence/AFP-S3-AUTONOMOUS-DELIVERY-LADDER
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S3-SPEC-TEST-SYNTH
            title: Spec/Test Synthesis Harness
            status: pending
            description: Automate generation and verification of design/spec/test artifacts
              for autopilot work.
            dependencies:
              depends_on:
                - AFP-S3-AUTONOMOUS-DELIVERY-LADDER
            exit_criteria:
              - prose: Harness produces spec/test bundles meeting coverage targets.
              - prose: Oracle comparator highlights discrepancies with steward approval.
            domain: mcp
            complexity_score: 7
            effort_hours: 10
            required_tools:
              - python
              - node
              - npm
            evidence_path: state/evidence/AFP-S3-SPEC-TEST-SYNTH
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S3-KNOWLEDGE-SYNTH
            title: Organizational Memory Rehydration
            status: pending
            description: Persist distilled learnings (including README analytics) into a
              retrieval pipeline that feeds STRATEGIZE/THINK for autonomous
              agents.
            dependencies:
              depends_on:
                - AFP-S3-AUTONOMOUS-DELIVERY-LADDER
                - AFP-S3-SPEC-TEST-SYNTH
            exit_criteria:
              - prose: Memory queries return high-signal notes + README critical evaluations for
                  sampled tasks during STRATEGIZE reviews.
              - prose: Decay policy implementation audited; demonstrates retention of
                  autonomy-critical knowledge while pruning stale noise.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - python
              - node
              - npm
            evidence_path: state/evidence/AFP-S3-KNOWLEDGE-SYNTH
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
      - id: E-AFP-ROADMAP-stage4
        title: Stage 4 — Self-Govern & Scale
        status: pending
        description: Preserve AFP/SCAS integrity while scaling concurrency, governance,
          and incident response.
        tasks:
          - id: AFP-S4-MULTI-TENANT
            title: Multi-Tenant Orchestrator Support
            status: pending
            description: Allow isolated tenants with shared infrastructure honoring
              guardrails per tenant.
            dependencies:
              depends_on:
                - AFP-S3-AUTONOMOUS-DELIVERY-LADDER
                - AFP-S3-KNOWLEDGE-SYNTH
            exit_criteria:
              - prose: Multi-tenant integration tests pass without cross-tenant leakage.
              - prose: Security review sign-off documented.
            domain: mcp
            complexity_score: 7
            effort_hours: 10
            required_tools:
              - python
              - node
              - npm
            evidence_path: state/evidence/AFP-S4-MULTI-TENANT
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S4-QUEUE-ADAPT
            title: Dynamic Task Queue Prioritization
            status: pending
            description: Prioritize tasks based on severity, deadlines, and entropy impact.
            dependencies:
              depends_on:
                - AFP-S3-AUTONOMOUS-DELIVERY-LADDER
                - AFP-S3-KNOWLEDGE-SYNTH
            exit_criteria:
              - prose: Priority scheduler simulation demonstrates SLA improvements.
              - prose: Dashboard shows reprioritization metrics.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - python
              - node
              - npm
            evidence_path: state/evidence/AFP-S4-QUEUE-ADAPT
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S4-SWARM-COORD
            title: Coordinated Multi-Agent Workflows
            status: pending
            description: Enable multi-agent collaboration with managed interfaces across
              phases.
            dependencies:
              depends_on:
                - AFP-S4-MULTI-TENANT
                - AFP-S4-QUEUE-ADAPT
            exit_criteria:
              - prose: Swarm coordination protocol validated by integration test with conflict
                  log.
              - prose: Handoff artifacts reviewed by stewardship committee.
            domain: mcp
            complexity_score: 7
            effort_hours: 10
            required_tools:
              - python
              - node
              - npm
            evidence_path: state/evidence/AFP-S4-SWARM-COORD
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S4-ADVERSARIAL-TESTS
            title: Autonomous Adversarial Test Harness
            status: pending
            description: Continuously attack the system to find weaknesses.
            dependencies:
              depends_on:
                - AFP-S2-PROMPT-DRIFT
                - AFP-S3-AUTONOMOUS-DELIVERY-LADDER
            exit_criteria:
              - prose: Scheduled adversarial run logs stored with findings.
              - prose: Vulnerabilities triaged with remediation tasks linked.
            domain: mcp
            complexity_score: 7
            effort_hours: 10
            required_tools:
              - python
              - node
              - npm
            evidence_path: state/evidence/AFP-S4-ADVERSARIAL-TESTS
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S4-LOAD-CHAOS
            title: High-Load & Chaos Resilience Tests
            status: pending
            description: Validate orchestrator under peak load and chaotic failures.
            dependencies:
              depends_on:
                - AFP-S4-MULTI-TENANT
                - AFP-S4-SWARM-COORD
            exit_criteria:
              - prose: Stress test results demonstrate stability at 5× baseline load.
              - prose: Chaos drills record recovery time under target.
            domain: mcp
            complexity_score: 7
            effort_hours: 10
            required_tools:
              - python
              - node
              - npm
            evidence_path: state/evidence/AFP-S4-LOAD-CHAOS
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S4-SUCCESSION-ORCH
            title: Steward Succession Orchestrator
            status: pending
            description: Automate stewardship succession with readiness scoring.
            dependencies:
              depends_on:
                - AFP-S3-KNOWLEDGE-SYNTH
            exit_criteria:
              - prose: Succession registry lists steward + apprentice for every directory.
              - prose: Quarterly drill evidence stored with readiness metrics.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S4-SUCCESSION-ORCH
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S4-POLICY-CHANGE
            title: Policy Change Process Automation
            status: pending
            description: Automate policy proposals, reviews, and rollouts with traceable
              approvals.
            dependencies:
              depends_on:
                - AFP-S2-RELIABILITY-ORACLE
                - AFP-S4-SUCCESSION-ORCH
            exit_criteria:
              - prose: Policy change workflow exercised end-to-end with recorded approvals.
              - prose: CI gating prevents policy merges without recorded approvals.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S4-POLICY-CHANGE
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S4-COMMS-ROUTER
            title: Adaptive Communication Router & Incident Escalation
            status: pending
            description: Route alerts via appropriate channels while preventing alert
              fatigue.
            dependencies:
              depends_on:
                - AFP-S2-TOOL-HEALTH
                - AFP-S4-ADVERSARIAL-TESTS
            exit_criteria:
              - prose: Simulated incidents show correct routing with no duplicates.
              - prose: False positive rate <5% documented in evidence.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S4-COMMS-ROUTER
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S4-VIA-NEGATIVA
            title: Via Negativa Principle Engine
            status: pending
            description: Identify and remove harmful practices, unused features, and
              redundant policies.
            dependencies:
              depends_on:
                - AFP-S2-CHAOS-LEDGER
                - AFP-S4-QUEUE-ADAPT
            exit_criteria:
              - prose: Removal candidate report logged with harm/benefit scoring.
              - prose: At least three removals executed with improvement metrics.
            domain: mcp
            complexity_score: 6
            effort_hours: 8
            required_tools:
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S4-VIA-NEGATIVA
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
      - id: E-AFP-ROADMAP-stage5
        title: Stage 5 — Full Autonomy Trials
        status: pending
        description: Prove autopilot can program anything with audited reliability
          across diverse domains.
        tasks:
          - id: AFP-S5-AUTONOMY-FRAMEWORK
            title: Autonomy Trial Framework
            status: pending
            description: Design autonomy trial rubric, oracles, and scoring automation
              covering core repo changes, internal tooling, and greenfield
              creative builds.
            dependencies:
              depends_on:
                - AFP-S4-LOAD-CHAOS
                - AFP-S4-POLICY-CHANGE
                - AFP-S4-VIA-NEGATIVA
            exit_criteria:
              - prose: Trial rubric (core/tooling/greenfield) approved by stewardship board with
                  explicit oracles + rollback policies.
              - prose: Scoring harness executes against seed corpus with stored results and
                  publishes success/failure dashboards.
            domain: mcp
            complexity_score: 7
            effort_hours: 10
            required_tools:
              - python
              - node
              - npm
            evidence_path: state/evidence/AFP-S5-AUTONOMY-FRAMEWORK
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S5-AUTOPILOT-CORE
            title: Core WeatherVane Autonomy Trial
            status: pending
            description: Run Autopilot on a WeatherVane production-grade change
              (API/worker/web) end-to-end with zero human edits.
            dependencies:
              depends_on:
                - AFP-S5-AUTONOMY-FRAMEWORK
            exit_criteria:
              - prose: Trial execution archives design/spec/plan artifacts plus README updates
                  proving knowledge propagation.
              - prose: Telemetry shows ≥99% phase compliance, zero failed guardrails, and
                  post-merge monitors green.
            domain: mcp
            complexity_score: 8
            effort_hours: 12
            required_tools:
              - python
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S5-AUTOPILOT-CORE
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S5-AUTOPILOT-TOOLING
            title: Internal Tool Autonomy Trial
            status: pending
            description: Have Autopilot build a new internal tool/service (e.g., CLI,
              critic, dashboard) end-to-end with deployment readiness.
            dependencies:
              depends_on:
                - AFP-S5-AUTONOMY-FRAMEWORK
            exit_criteria:
              - prose: Tool spec, implementation, deployment scripts, and README coverage
                  recorded with tests passing.
              - prose: Ops handoff checklist signed by stewards; monitors configured for the new
                  tool.
            domain: mcp
            complexity_score: 8
            effort_hours: 12
            required_tools:
              - python
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S5-AUTOPILOT-TOOLING
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S5-AUTOPILOT-GREENFIELD
            title: Greenfield Creative Trial
            status: pending
            description: Autopilot produces a novel artifact (game prototype, external app,
              creative experience) from prompt to deployment with human sign-off
              only at audit.
            dependencies:
              depends_on:
                - AFP-S5-AUTONOMY-FRAMEWORK
                - AFP-S5-AUTOPILOT-CORE
                - AFP-S5-AUTOPILOT-TOOLING
            exit_criteria:
              - prose: Acceptance script passes (functional + UX) for greenfield artifact with
                  replayable harness.
              - prose: Resilience tests + README updates recorded with human red-team signoff
                  documenting residual risks.
            domain: mcp
            complexity_score: 8
            effort_hours: 12
            required_tools:
              - python
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S5-AUTOPILOT-GREENFIELD
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
          - id: AFP-S5-AUTONOMY-AUDIT
            title: Autonomous Operations Audit
            status: pending
            description: Aggregate analytics, guardrail triggers, README health, and trial
              outcomes into the final autonomy certification packet.
            dependencies:
              depends_on:
                - AFP-S5-AUTOPILOT-CORE
                - AFP-S5-AUTOPILOT-TOOLING
                - AFP-S5-AUTOPILOT-GREENFIELD
            exit_criteria:
              - prose: Autonomy scorecard covers MTTR distribution, guardrail triggers, README
                  warning closure rate, and trial scores.
              - prose: Audit committee sign-off recorded; residual gaps logged as remediation
                  tasks with owners and due dates.
            domain: mcp
            complexity_score: 7
            effort_hours: 10
            required_tools:
              - python
              - node
              - npm
              - bash
            evidence_path: state/evidence/AFP-S5-AUTONOMY-AUDIT
            work_process_phases:
              - strategize
              - spec
              - plan
              - think
              - implement
              - verify
              - review
              - pr
              - monitor
            evidence_enforcement: enforce
  - id: E-GENERAL
    title: Legacy Critic Remediation Backlog
    status: on_hold
    domain: product
    description: >
      Historic critic remediation tasks retained for reference. These are
      deprioritized while Autopilot Wave milestones execute; reactivate only if
      critic health degrades after Wave delivery.
    milestones:
      - id: E-GENERAL-backlog
        title: Backlog (On Hold)
        status: on_hold
        tasks:
          - id: CRIT-PERF-BUILD-4254a2
            title: "[Critic:build] Restore performance"
            status: on_hold
            dependencies: []
            exit_criteria: []
            domain: product
            description: Critic build remediation plan archived pending Wave completion.
              Re-enable if build critic regresses after Autopilot Stage tasks.
          - id: CRIT-PERF-BUILD-4254a2.1
            title: Research and design for [Critic:build] Restore performance
            status: on_hold
            dependencies: []
            exit_criteria: []
            domain: product
            description: Research phase (paused) for [Critic:build] Restore performance
          - id: CRIT-PERF-BUILD-4254a2.2
            title: Implement [Critic:build] Restore performance
            status: on_hold
            dependencies:
              - CRIT-PERF-BUILD-4254a2.1
            exit_criteria: []
            domain: product
            description: Implementation phase (paused) for [Critic:build] Restore
              performance
          - id: CRIT-PERF-BUILD-4254a2.3
            title: Validate and test [Critic:build] Restore performance
            status: on_hold
            dependencies:
              - CRIT-PERF-BUILD-4254a2.2
            exit_criteria: []
            domain: product
            description: Validation phase (paused) for [Critic:build] Restore performance
          - id: CRIT-PERF-GLOBAL-9dfa06
            title: "[Critics] Systemic performance remediation"
            status: on_hold
            dependencies: []
            exit_criteria: []
            domain: product
            description: Systemic critic remediation parked while Autopilot Wave execution
              proceeds. Resume once Stage milestones complete or if new systemic
              failures emerge.
          - id: CRIT-PERF-GLOBAL-9dfa06.1
            title: Research and design for [Critics] Systemic performance remediation
            status: on_hold
            dependencies: []
            exit_criteria: []
            domain: product
            description: Research phase (paused) for systemic critic remediation
          - id: CRIT-PERF-GLOBAL-9dfa06.2
            title: Implement [Critics] Systemic performance remediation
            status: on_hold
            dependencies:
              - CRIT-PERF-GLOBAL-9dfa06.1
            exit_criteria: []
            domain: product
            description: Implementation phase (paused) for systemic critic remediation
          - id: CRIT-PERF-GLOBAL-9dfa06.3
            title: Validate and test [Critics] Systemic performance remediation
            status: on_hold
            dependencies:
              - CRIT-PERF-GLOBAL-9dfa06.2
            exit_criteria: []
            domain: product
            description: Validation phase (paused) for systemic critic remediation
