schema_version: '2.0'
epics:
  - id: E-AUTOPILOT-GUARDRAILS
    title: Autopilot Guardrails Completion
    status: pending
    domain: product
    milestones:
      - id: E-AUTOPILOT-PHASE-NEG1
        title: Phase -1 Enforcement Completion
        status: pending
        tasks:
          - id: AT-GUARD-STRATEGIZE
            title: 'STRATEGIZE: Define enforcement scope and link to Autopilot functionality'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Purpose, impacted Autopilot workflows, and success metrics documented in strategy artifact
            domain: product
            description: >
              Capture the strategic framing for Phase -1: which Autopilot behaviours (state transitions, tool
              executions, evidence gates) must be protected, why enforcement failed previously, and what success looks
              like. Produce a strategy.md artifact and ledger entry.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: AT-GUARD-SPEC
            title: 'SPEC: Evidence-gated enforcement requirements'
            status: pending
            dependencies:
              depends_on:
                - AT-GUARD-STRATEGIZE
            exit_criteria:
              - prose: >-
                  Acceptance criteria for enforcement, evidence collection, ledger, prompt attestation, and metrics
                  defined and approved
            domain: product
            description: >
              Write spec.md enumerating acceptance criteria for WorkProcessEnforcer wiring, phase ledger persistence,
              prompt signature checks, artifact validation, leases, and integrity script expectations.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: AT-GUARD-PLAN
            title: 'PLAN: Implementation breakdown for Phase -1'
            status: pending
            dependencies:
              depends_on:
                - AT-GUARD-SPEC
            exit_criteria:
              - prose: Plan.md lists work items, owners, estimates, dependencies, and verification strategy
            domain: product
            description: >
              Create plan.md with sequenced tasks (code, tests, telemetry, docs) required to ship Phase -1 enforcement
              and to gather Strategy→Monitor evidence.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: AT-GUARD-THINK
            title: 'THINK: Risk analysis for enforcement rollout'
            status: pending
            dependencies:
              depends_on:
                - AT-GUARD-PLAN
            exit_criteria:
              - prose: Edge cases and mitigation strategies captured in edge_cases.md with ledger reference
            domain: product
            description: >
              Document risks (multi-agent contention, ledger corruption, false positives, integrity flakiness) and
              mitigation steps before coding.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: AT-GUARD-IMPLEMENT
            title: 'IMPLEMENT: Wire WorkProcessEnforcer, ledger, attestation, leases'
            status: pending
            dependencies:
              depends_on:
                - AT-GUARD-THINK
            exit_criteria:
              - prose: Code merged with unit/integration tests; ledger entries created; prompt signature enforced
            domain: product
            description: >
              Implement startCycle/advancePhase wiring, state graph and tool router guards, phase ledger, prompt
              attestation, artifact gating, lease handling, and metrics emission.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: AT-GUARD-VERIFY
            title: 'VERIFY: Integrity suite and enforcement telemetry'
            status: pending
            dependencies:
              depends_on:
                - AT-GUARD-IMPLEMENT
            exit_criteria:
              - prose: run_integrity_tests.sh passes; telemetry shows process.validation spans & counters
            domain: product
            description: >
              Run full integrity script, capture logs, coverage, ledger verification, and ensure metrics/traces are
              written.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: AT-GUARD-REVIEW
            title: 'REVIEW: Confirm Autopilot functionality post-enforcement'
            status: pending
            dependencies:
              depends_on:
                - AT-GUARD-VERIFY
            exit_criteria:
              - prose: Reviewer rubric completed; functionality smoke evidence attached; ledger/journal updated
            domain: product
            description: >
              Conduct structured review validating Autopilot workflows still operate (planner dispatch, tool execution,
              state transitions) with evidence attached.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: AT-GUARD-PR
            title: 'PR: Document Phase -1 completion'
            status: pending
            dependencies:
              depends_on:
                - AT-GUARD-REVIEW
            exit_criteria:
              - prose: PR summary with evidence bundle, ledger hash, prompt signature verification
            domain: product
            description: |
              Assemble PR summary, evidence, and ledger/prompt attestation outputs proving Phase -1 completion.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: AT-GUARD-MONITOR
            title: 'MONITOR: Post-deployment telemetry for enforcement'
            status: pending
            dependencies:
              depends_on:
                - AT-GUARD-PR
            exit_criteria:
              - prose: Monitoring plan executed; metrics dashboard updated; discrepancies addressed or logged
            domain: product
            description: |
              Run monitoring plan, ensure enforcement metrics stay stable, capture results in monitoring plan doc.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: ROADMAP-STRUCT
            title: Roadmap Structure Enhancement for Autopilot Efficiency
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Typed roadmap schemas defined with validation
              - prose: Machine-readable dependency graph implemented
              - prose: Success criteria structured for automated verification
              - prose: Autopilot metadata added (complexity, effort, tool requirements)
              - prose: plan_next tool enhanced to use new structure
            domain: product
            description: >
              Enhance roadmap.yaml structure to be more machine-readable and autopilot-friendly. Current structure is
              human-optimized (YAML prose); autopilot needs typed schemas, explicit dependencies, machine-readable
              success criteria, and metadata for intelligent task selection.

              Improvements: 1. Define TypeScript schemas (TaskSchema, EpicSchema, MilestoneSchema) 2. Add typed
              dependency relationships (depends_on, blocks, related_to, produces, consumes) 3. Structure acceptance
              criteria as testable predicates (not prose) 4. Add autopilot metadata (complexity: 1-10, effort_hours,
              risk_level, tool_allowlist) 5. Enhance task status (add: blocked, needs_review, ready_for_rollout,
              archived) 6. Add cross-item integration metadata (related items, contract versions)

              Benefits: - Better plan_next intelligence (dependency resolution, effort estimation) - Automated
              validation (no missing dependencies, no circular deps) - Better progress tracking (granular status beyond
              pending/in_progress/done) - Future-proof for multi-agent coordination - System compatibility (structured
              data for tools/APIs)

              Implementation: - Create schemas in tools/wvo_mcp/src/roadmap/schemas.ts - Add roadmap validation script
              (scripts/validate_roadmap.ts) - Migrate existing roadmap.yaml to new structure (backwards compatible) -
              Update plan_next MCP tool to use typed structure - Add roadmap linter to CI
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: ROADMAP-HYBRID
            title: 'Roadmap Hybrid Storage: YAML Source + SQLite Runtime Index'
            status: pending
            dependencies:
              depends_on:
                - ROADMAP-STRUCT
            exit_criteria:
              - prose: SQLite schema defined for roadmap entities (tasks, dependencies, metadata)
              - prose: YAML→SQLite loader implemented with incremental updates
              - prose: Query performance benchmarked (>100x faster than YAML parsing)
              - prose: MCP tools migrated to use SQLite queries (plan_next, plan_update)
              - prose: YAML remains source of truth (git-friendly, human-editable)
              - prose: Hot reload on YAML changes (file watcher → reload SQLite)
              - test: 'npm run test:roadmap-hybrid'
                expect: 'all tests pass'
              - metric: 'plan_next query time'
                expect: '< 5ms (vs ~50ms YAML)'
            domain: mcp
            description: >
              Implement hybrid storage for roadmap: keep YAML as git-friendly source of truth, but build
              SQLite runtime index for fast queries. Best of both worlds.

              Problem: YAML parsing is slow (~50ms for 500 tasks), no indexing, no complex queries. But
              YAML is human-readable, git-friendly, and easy to edit. SQLite is fast (<5ms queries), has
              indexes and SQL, but poor git diffs and not human-editable.

              Solution: Hybrid approach - YAML as source, SQLite as runtime index. On MCP server startup,
              load roadmap.yaml → build SQLite database in memory or on disk. File watcher detects YAML
              changes → reload SQLite. All queries use SQLite, all edits go through YAML.

              Architecture:
              1. Schema: SQLite tables (tasks, dependencies, exit_criteria, metadata, tags)
              2. Loader: YAML parser → populate SQLite (tools/wvo_mcp/src/roadmap/loader.ts)
              3. Query Interface: Replace direct YAML access with SQL queries
              4. Hot Reload: File watcher on state/roadmap.yaml → trigger reload
              5. Validation: Both YAML (structure) and SQLite (referential integrity)

              Migration Path:
              - Phase 1: Build loader + SQLite schema (no breaking changes)
              - Phase 2: Add parallel queries (YAML + SQLite, verify consistency)
              - Phase 3: Migrate MCP tools to SQLite-only
              - Phase 4: Remove YAML parsing from query path (keep for edits)

              Benefits:
              - Fast queries: O(1) lookups vs O(n) YAML scan
              - Complex queries: SQL joins vs manual YAML traversal
              - Indexes: task_id, status, dependencies, tags
              - Scalability: Handle 10,000+ tasks efficiently
              - Git-friendly: YAML source of truth remains unchanged
              - Transactions: Atomic updates to roadmap state

              Example Queries:
              - "Find all ready tasks" → SELECT * FROM tasks WHERE status='ready' AND all_blockers_done=1
              - "Calculate WSJF ranking" → SELECT *, (readiness * value) / effort AS wsjf FROM tasks ORDER BY wsjf DESC
              - "Find blockers for task X" → SELECT * FROM dependencies WHERE dependent_id='X' AND type='depends_on'

              Trade-offs:
              - Complexity: Two representations to keep in sync
              - Memory: SQLite database (~2-5MB for 500 tasks)
              - Startup: Initial load takes ~100ms (vs ~50ms pure YAML)
              - Benefit: Ongoing queries 10-100x faster

              Future Extensions:
              - Vector embeddings for task similarity search
              - Full-text search on descriptions
              - Historical state tracking (task status over time)
              - Multi-roadmap federation (query across multiple roadmaps)
            complexity_score: 7
            effort_hours: 8
            required_tools:
              - fs_read
              - cmd_run
          - id: ROADMAP-PROMPTS
            title: 'Update Work Process Prompts for Roadmap v2.0 Structure'
            status: pending
            dependencies:
              depends_on:
                - ROADMAP-STRUCT
            exit_criteria:
              - prose: CLAUDE.md updated to reference v2.0 structure
              - prose: AGENTS.md updated with v2.0 examples
              - prose: MCP tool descriptions updated (plan_next, plan_update)
              - test: 'grep -r "dependencies: \\[\\]" docs/autopilot/'
                expect: 'no matches'
            domain: mcp
            description: >
              Update work process docs and system prompts to reflect roadmap v2.0 structure. Replace v1
              examples (flat dependencies, prose criteria) with v2.0 (typed dependencies, structured criteria).
            complexity_score: 3
            effort_hours: 2
            required_tools:
              - fs_read
              - fs_write
          - id: TOOL-EVAL-META
            title: MCP Tool Ecosystem Evaluation and Enhancement
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Current tool inventory documented with usage metrics
              - prose: Gap analysis complete (missing tools for autopilot workflows)
              - prose: Tool improvement proposals prioritized
              - prose: New tool designs specified for critical gaps
              - prose: Tool deprecation/consolidation recommendations
            domain: mcp
            description: >
              Evaluate the current state of MCP tools available to autopilot (both current and post-improvement) to
              identify gaps, redundancies, and opportunities for enhancement. Consider evolving work processes and
              autopilot capabilities to anticipate future tool needs.

              Current State Assessment: 1. Inventory all MCP tools (wvo_status, plan_next, fs_read, cmd_run, etc.) 2.
              Measure usage frequency and context (which workflows use which tools) 3. Identify tool gaps (workflows
              that lack appropriate tools) 4. Analyze tool redundancy (overlapping functionality) 5. Evaluate tool
              ergonomics (ease of use, clarity, error handling)

              Future State Analysis: 1. Map tools to future autopilot capabilities (post AT-GUARD, post instrumentation)
              2. Identify tools needed for new workflows (meta-evaluation, self-improvement) 3. Design tools for
              emerging patterns (e.g., quality graph queries, roadmap intelligence) 4. Consider multi-agent coordination
              needs (locks, leases, handoffs) 5. Evaluate cross-cutting concerns (observability, error recovery,
              rollback)

              Tool Categories to Evaluate: - State Management (plan_next, plan_update, context_write) - File System
              (fs_read, fs_write) - Execution (cmd_run, heavy_queue_enqueue) - Quality (critics_run, quality_checklist)
              - Meta (wvo_status, provider_status, state_metrics) - Observability (screenshot_session, artifact_record)
              - LSP (lsp_definition, lsp_references, lsp_hover) - Flags (mcp_admin_flags)

              Outputs: 1. Tool Usage Matrix (tool × workflow × frequency) 2. Gap Analysis Report (missing tools,
              priority ranking) 3. Tool Enhancement Proposals (5-10 high-priority improvements) 4. New Tool
              Specifications (3-5 critical gaps) 5. Deprecation Recommendations (redundant/unused tools)

              Meta Consideration: Roadmap structure improvements (ROADMAP-STRUCT) apply to both autopilot and
              WeatherVane roadmaps. Tool evaluation should consider how roadmap enhancements enable new tool
              capabilities (e.g., tools that query roadmap metadata, suggest next tasks, estimate effort).

              Benefits: - More efficient autopilot workflows (right tools for each job) - Better developer experience
              (ergonomic, discoverable tools) - Reduced tool sprawl (consolidate redundant functionality) -
              Future-proofed tool ecosystem (anticipate emerging needs) - Better cross-cutting concerns (observability,
              error handling)
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: E-AUTOPILOT-PHASE0
        title: Phase 0 Instrumentation
        status: pending
        tasks:
          - id: AT-INST-STRATEGIZE
            title: 'STRATEGIZE: Define instrumentation outcomes'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Strategy artifact links instrumentation goals to Autopilot observability needs
            domain: product
            description: >
              Document why OTEL spans, metrics counters, and dashboards are required, including impacted Autopilot flows
              and success metrics.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: AT-INST-SPEC
            title: 'SPEC: Telemetry & dashboard requirements'
            status: pending
            dependencies:
              depends_on:
                - AT-INST-STRATEGIZE
            exit_criteria:
              - prose: Spec covers span names, attributes, counters, JSONL outputs, dashboards/alerts
            domain: product
            description: |
              Capture detailed specs for instrumentation and visualization deliverables.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: AT-INST-PLAN
            title: 'PLAN: Implementation breakdown for instrumentation'
            status: pending
            dependencies:
              depends_on:
                - AT-INST-SPEC
            exit_criteria:
              - prose: Plan with tasks, estimates, dependencies, verification steps
            domain: product
            description: |
              Outline implementation steps for spans, metrics, telemetry writers, dashboards, evidence capture.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: AT-INST-THINK
            title: 'THINK: Risks & mitigations for instrumentation rollout'
            status: pending
            dependencies:
              depends_on:
                - AT-INST-PLAN
            exit_criteria:
              - prose: Edge cases logged (overhead, missing traces, noisy counters) with mitigations
            domain: product
            description: |
              Analyze risks (performance, data volume, false positives) before coding.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: AT-INST-IMPLEMENT
            title: 'IMPLEMENT: Add spans, counters, telemetry outputs'
            status: pending
            dependencies:
              depends_on:
                - AT-INST-THINK
            exit_criteria:
              - prose: Code merged with tests; traces/metrics JSONL generated in local run
            domain: product
            description: |
              Implement OTEL spans, counters, telemetry writers, and ensure they integrate with enforcement.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: AT-INST-VERIFY
            title: 'VERIFY: Demonstrate instrumentation under load'
            status: pending
            dependencies:
              depends_on:
                - AT-INST-IMPLEMENT
            exit_criteria:
              - prose: Tests and manual run produce traces/metrics; artefacts stored and linked
            domain: product
            description: |
              Run validation suite and manual scenario to confirm instrumentation outputs.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: AT-INST-REVIEW
            title: 'REVIEW: Evaluate instrumentation quality'
            status: pending
            dependencies:
              depends_on:
                - AT-INST-VERIFY
            exit_criteria:
              - prose: Reviewer confirms coverage, evidence linked, documentation updated
            domain: product
            description: |
              Conduct review ensuring instrumentation meets quality bar and Autopilot functionality preserved.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: AT-INST-PR
            title: 'PR: Summarize Phase 0 instrumentation'
            status: pending
            dependencies:
              depends_on:
                - AT-INST-REVIEW
            exit_criteria:
              - prose: PR summary includes spans/metrics evidence, dashboards, ledger references
            domain: product
            description: |
              Document instrumentation deliverables and evidence for PR.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: AT-INST-MONITOR
            title: 'MONITOR: Ongoing telemetry validation'
            status: pending
            dependencies:
              depends_on:
                - AT-INST-PR
            exit_criteria:
              - prose: Monitoring plan executed, dashboards reviewed, follow-ups logged
            domain: product
            description: |
              Operate the instrumentation post-deploy, verify dashboards, and log observations.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
  - id: E-GENERAL
    title: E-GENERAL
    status: pending
    domain: product
    milestones:
      - id: E-GENERAL-backlog
        title: Backlog
        status: pending
        tasks:
          - id: CRIT-PERF-BUILD-4254a2
            title: '[Critic:build] Restore performance'
            status: blocked
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: >-
              Critic build is underperforming and needs immediate remediation.


              Identity: Build Sentinel (engineering, authority blocking)

              Mission: Guarantee that core build processes remain reproducible and optimized across environments.

              Signature powers: Diagnoses build pipeline regressions and unstable toolchains.; Flags missing build
              artifacts or misconfigured dependencies before release.

              Autonomy guidance: Attempt automated patching of build scripts when safe; escalate infrastructure
              escalations beyond local fixes.


              No successful runs recorded in the last 6 observations; 6 consecutive failures detected.


              Observation window: 6 runs


              Consecutive failures: 6


              Failures: 6 | Successes: 0


              Assigned to: Director Dana


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              make[1]: *** [lint] Error 1
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-BUILD-4254a2.1
            title: Research and design for [Critic:build] Restore performance
            status: pending
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: 'Research phase: Understand requirements and design approach for [Critic:build] Restore performance'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-BUILD-4254a2.2
            title: Implement [Critic:build] Restore performance
            status: pending
            dependencies:
              depends_on:
                - CRIT-PERF-BUILD-4254a2.1
            exit_criteria: []
            domain: product
            description: 'Implementation phase: Execute the plan for [Critic:build] Restore performance'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-BUILD-4254a2.3
            title: Validate and test [Critic:build] Restore performance
            status: pending
            dependencies:
              depends_on:
                - CRIT-PERF-BUILD-4254a2.2
            exit_criteria: []
            domain: product
            description: 'Validation phase: Test and verify [Critic:build] Restore performance'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-GLOBAL-9dfa06
            title: '[Critics] Systemic performance remediation'
            status: blocked
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: |-
              Multiple critics are underperforming and require coordinated intervention.

              2 critics require director-level intervention after repeated failures.

              Affected critics: build, tests

              Critics evaluated in run: 3

              Reports captured: 2

              Assigned to: Director Dana

              Expectations:
              - Review individual remediation tasks and look for systemic issues.
              - Adjust critic configurations, training loops, or staffing mixes.
              - Provide a coordination brief in state/context.md.
              - Close this systemic task once individual critics are back on track.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-GLOBAL-9dfa06.1
            title: Research and design for [Critics] Systemic performance remediation
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: 'Research phase: Understand requirements and design approach for [Critics] Systemic performance remediation'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-GLOBAL-9dfa06.2
            title: Implement [Critics] Systemic performance remediation
            status: in_progress
            dependencies:
              depends_on:
                - CRIT-PERF-GLOBAL-9dfa06.1
            exit_criteria: []
            domain: product
            description: 'Implementation phase: Execute the plan for [Critics] Systemic performance remediation'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-GLOBAL-9dfa06.3
            title: Validate and test [Critics] Systemic performance remediation
            status: pending
            dependencies:
              depends_on:
                - CRIT-PERF-GLOBAL-9dfa06.2
            exit_criteria: []
            domain: product
            description: 'Validation phase: Test and verify [Critics] Systemic performance remediation'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-TESTS-fcee61
            title: '[Critic:tests] Restore performance'
            status: needs_review
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: >-
              Critic tests is underperforming and needs immediate remediation.


              Identity: Regression Hunter (quality, authority blocking)

              Mission: Keep the test suites healthy and ensure deterministic results across flows.

              Signature powers: Surfaces flaky suites and failing assertions with reproduction notes.; Synthesizes
              minimal repro commands for Autopilot triage.

              Autonomy guidance: Rerun targeted suites automatically; lean on Autopilot only when new failures persist.


              No successful runs recorded in the last 6 observations; 6 consecutive failures detected.


              Observation window: 6 runs


              Consecutive failures: 6


              Failures: 6 | Successes: 0


              Assigned to: Director Dana


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              == Python test suite ==

              ============================= test session starts ==============================

              platform darwin -- Python 3.10.12, pytest-8.4.2, pluggy-1.6.0

              rootdir: /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane

              configfile: pyproject.toml

              plugins: anyio-4.11.0, asyncio-1.2.0

              asyncio: mode=auto, debug=False, asyncio_default_fixture_loop_scope=None,
              asyncio_default_test_loop_scope=function

              collected 1134 items / 19 errors


              ==================================== ERRORS ====================================

              ____________ ERROR collecting tests/api/onboarding/test_progress.p...
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-TESTS-fcee61.1
            title: Research and design for [Critic:tests] Restore performance
            status: pending
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: 'Research phase: Understand requirements and design approach for [Critic:tests] Restore performance'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-TESTS-fcee61.2
            title: Implement [Critic:tests] Restore performance
            status: pending
            dependencies:
              depends_on:
                - CRIT-PERF-TESTS-fcee61.1
            exit_criteria: []
            domain: product
            description: 'Implementation phase: Execute the plan for [Critic:tests] Restore performance'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-TESTS-fcee61.3
            title: Validate and test [Critic:tests] Restore performance
            status: pending
            dependencies:
              depends_on:
                - CRIT-PERF-TESTS-fcee61.2
            exit_criteria: []
            domain: product
            description: 'Validation phase: Test and verify [Critic:tests] Restore performance'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: E-GENERAL.1
            title: Research and design for E-GENERAL
            status: pending
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: 'Research phase: Understand requirements and design approach for E-GENERAL'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: E-GENERAL.2
            title: Implement E-GENERAL
            status: pending
            dependencies:
              depends_on:
                - E-GENERAL.1
            exit_criteria: []
            domain: product
            description: 'Implementation phase: Execute the plan for E-GENERAL'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: E-GENERAL.3
            title: Validate and test E-GENERAL
            status: pending
            dependencies:
              depends_on:
                - E-GENERAL.2
            exit_criteria: []
            domain: product
            description: 'Validation phase: Test and verify E-GENERAL'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: E-ML-REMEDIATION.1
            title: Research and design for ML Model Remediation - From Prototype to Production
            status: pending
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: >-
              Research phase: Understand requirements and design approach for ML Model Remediation - From Prototype to
              Production
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: E-ML-REMEDIATION.2
            title: Implement ML Model Remediation - From Prototype to Production
            status: pending
            dependencies:
              depends_on:
                - E-ML-REMEDIATION.1
            exit_criteria: []
            domain: product
            description: 'Implementation phase: Execute the plan for ML Model Remediation - From Prototype to Production'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: E-ML-REMEDIATION.3
            title: Validate and test ML Model Remediation - From Prototype to Production
            status: pending
            dependencies:
              depends_on:
                - E-ML-REMEDIATION.2
            exit_criteria: []
            domain: product
            description: 'Validation phase: Test and verify ML Model Remediation - From Prototype to Production'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-0.1.1
            title: Implementation verified
            status: blocked
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Create ModelingReality critic with quantitative thresholds: Implementation verified'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-0.1.2
            title: Tests verified
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-0.1.1
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Create ModelingReality critic with quantitative thresholds: Tests verified'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-0.1.3
            title: Quality gate APPROVED
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-0.1.2
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Create ModelingReality critic with quantitative thresholds: Quality gate APPROVED'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-0.1.4
            title: Runtime verification PASSED
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-0.1.3
            exit_criteria: []
            domain: product
            description: >-
              Part of [REM] Verify: Create ModelingReality critic with quantitative thresholds: Runtime verification
              PASSED
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-0.1.5
            title: Critical issues fixed
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-0.1.4
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Create ModelingReality critic with quantitative thresholds: Critical issues fixed'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-0.2.1
            title: Implementation verified
            status: needs_review
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Update all ML task exit criteria with objective metrics: Implementation verified'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-0.2.2
            title: Tests verified
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-0.2.1
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Update all ML task exit criteria with objective metrics: Tests verified'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-0.2.3
            title: Quality gate APPROVED
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-0.2.2
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Update all ML task exit criteria with objective metrics: Quality gate APPROVED'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-0.2.4
            title: Runtime verification PASSED
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-0.2.3
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Update all ML task exit criteria with objective metrics: Runtime verification PASSED'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-0.2.5
            title: Critical issues fixed
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-0.2.4
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Update all ML task exit criteria with objective metrics: Critical issues fixed'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-1.1.1
            title: Implementation verified
            status: needs_review
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Debug and fix weather multiplier logic in data generator: Implementation verified'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-1.1.2
            title: Tests verified
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-1.1.1
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Debug and fix weather multiplier logic in data generator: Tests verified'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-1.1.3
            title: Quality gate APPROVED
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-1.1.2
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Debug and fix weather multiplier logic in data generator: Quality gate APPROVED'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-1.1.4
            title: Runtime verification PASSED
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-1.1.3
            exit_criteria: []
            domain: product
            description: >-
              Part of [REM] Verify: Debug and fix weather multiplier logic in data generator: Runtime verification
              PASSED
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-1.1.5
            title: Critical issues fixed
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-1.1.4
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Debug and fix weather multiplier logic in data generator: Critical issues fixed'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-1.3.1
            title: Implementation verified
            status: pending
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Create validation tests for synthetic data quality: Implementation verified'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-1.3.2
            title: Tests verified
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-1.3.1
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Create validation tests for synthetic data quality: Tests verified'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-1.3.3
            title: Quality gate APPROVED
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-1.3.2
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Create validation tests for synthetic data quality: Quality gate APPROVED'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-1.3.4
            title: Runtime verification PASSED
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-1.3.3
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Create validation tests for synthetic data quality: Runtime verification PASSED'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-1.3.5
            title: Critical issues fixed
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-1.3.4
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Create validation tests for synthetic data quality: Critical issues fixed'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-2.1.1
            title: Implementation verified
            status: pending
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Implement proper train/val/test splitting with no leakage: Implementation verified'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-2.1.2
            title: Tests verified
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-2.1.1
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Implement proper train/val/test splitting with no leakage: Tests verified'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-2.1.3
            title: Quality gate APPROVED
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-2.1.2
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Implement proper train/val/test splitting with no leakage: Quality gate APPROVED'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-2.1.4
            title: Runtime verification PASSED
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-2.1.3
            exit_criteria: []
            domain: product
            description: >-
              Part of [REM] Verify: Implement proper train/val/test splitting with no leakage: Runtime verification
              PASSED
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-2.1.5
            title: Critical issues fixed
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-2.1.4
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Implement proper train/val/test splitting with no leakage: Critical issues fixed'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-2.2.1
            title: Implementation verified
            status: pending
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Implement LightweightMMM with weather features: Implementation verified'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-2.2.2
            title: Tests verified
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-2.2.1
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Implement LightweightMMM with weather features: Tests verified'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-2.2.3
            title: Quality gate APPROVED
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-2.2.2
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Implement LightweightMMM with weather features: Quality gate APPROVED'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-2.2.4
            title: Runtime verification PASSED
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-2.2.3
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Implement LightweightMMM with weather features: Runtime verification PASSED'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-2.2.5
            title: Critical issues fixed
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-2.2.4
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Implement LightweightMMM with weather features: Critical issues fixed'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-2.5.1
            title: Implementation verified
            status: pending
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Compare models to baseline (naive/seasonal/linear): Implementation verified'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-2.5.2
            title: Tests verified
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-2.5.1
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Compare models to baseline (naive/seasonal/linear): Tests verified'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-2.5.3
            title: Quality gate APPROVED
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-2.5.2
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Compare models to baseline (naive/seasonal/linear): Quality gate APPROVED'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-2.5.4
            title: Runtime verification PASSED
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-2.5.3
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Compare models to baseline (naive/seasonal/linear): Runtime verification PASSED'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-2.5.5
            title: Critical issues fixed
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-2.5.4
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Compare models to baseline (naive/seasonal/linear): Critical issues fixed'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-3.2.1
            title: Implementation verified
            status: pending
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Write comprehensive ML validation documentation: Implementation verified'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-3.2.2
            title: Tests verified
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-3.2.1
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Write comprehensive ML validation documentation: Tests verified'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-3.2.3
            title: Quality gate APPROVED
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-3.2.2
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Write comprehensive ML validation documentation: Quality gate APPROVED'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-3.2.4
            title: Runtime verification PASSED
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-3.2.3
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Write comprehensive ML validation documentation: Runtime verification PASSED'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-3.2.5
            title: Critical issues fixed
            status: pending
            dependencies:
              depends_on:
                - REM-T-MLR-3.2.4
            exit_criteria: []
            domain: product
            description: 'Part of [REM] Verify: Write comprehensive ML validation documentation: Critical issues fixed'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-0.3.1
            title: artifact:docs/ML_QUALITY_STANDARDS.md
            status: pending
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: 'Part of Document world-class quality standards for ML: artifact:docs/ML_QUALITY_STANDARDS.md'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-0.3.2
            title: verification:Numeric thresholds for all metrics
            status: pending
            dependencies:
              depends_on:
                - T-MLR-0.3.1
            exit_criteria: []
            domain: product
            description: 'Part of Document world-class quality standards for ML: verification:Numeric thresholds for all metrics'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-0.3.3
            title: verification:Baseline comparison requirements
            status: pending
            dependencies:
              depends_on:
                - T-MLR-0.3.2
            exit_criteria: []
            domain: product
            description: 'Part of Document world-class quality standards for ML: verification:Baseline comparison requirements'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-0.3.4
            title: review:External ML practitioner peer review
            status: pending
            dependencies:
              depends_on:
                - T-MLR-0.3.3
            exit_criteria: []
            domain: product
            description: 'Part of Document world-class quality standards for ML: review:External ML practitioner peer review'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-1.2.1
            title: artifact:storage/seeds/synthetic_v2/*.parquet (20 files)
            status: pending
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: >-
              Part of Generate 3 years of synthetic data for 20 tenants: artifact:storage/seeds/synthetic_v2/*.parquet
              (20 files)
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-1.2.2
            title: metric:total_rows = 219000
            status: pending
            dependencies:
              depends_on:
                - T-MLR-1.2.1
            exit_criteria: []
            domain: product
            description: 'Part of Generate 3 years of synthetic data for 20 tenants: metric:total_rows = 219000'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-1.2.3
            title: metric:date_range = 2022-01-01 to 2024-12-31
            status: pending
            dependencies:
              depends_on:
                - T-MLR-1.2.2
            exit_criteria: []
            domain: product
            description: 'Part of Generate 3 years of synthetic data for 20 tenants: metric:date_range = 2022-01-01 to 2024-12-31'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-1.2.4
            title: metric:weather_correlations_within_target >= 0.90
            status: pending
            dependencies:
              depends_on:
                - T-MLR-1.2.3
            exit_criteria: []
            domain: product
            description: >-
              Part of Generate 3 years of synthetic data for 20 tenants: metric:weather_correlations_within_target >=
              0.90
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-1.2.5
            title: test:pytest tests/data_gen/test_synthetic_v2_quality.py
            status: pending
            dependencies:
              depends_on:
                - T-MLR-1.2.4
            exit_criteria: []
            domain: product
            description: >-
              Part of Generate 3 years of synthetic data for 20 tenants: test:pytest
              tests/data_gen/test_synthetic_v2_quality.py
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-1.2.6
            title: critic:data_quality
            status: pending
            dependencies:
              depends_on:
                - T-MLR-1.2.5
            exit_criteria: []
            domain: product
            description: 'Part of Generate 3 years of synthetic data for 20 tenants: critic:data_quality'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-2.4.1
            title: artifact:experiments/mmm_v2/validation_report.json
            status: pending
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: >-
              Part of Validate model performance against objective thresholds:
              artifact:experiments/mmm_v2/validation_report.json
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-2.4.2
            title: metric:weather_sensitive_r2_pass_rate >= 0.80
            status: pending
            dependencies:
              depends_on:
                - T-MLR-2.4.1
            exit_criteria: []
            domain: product
            description: >-
              Part of Validate model performance against objective thresholds: metric:weather_sensitive_r2_pass_rate >=
              0.80
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-2.4.3
            title: metric:weather_elasticity_sign_correct = 1.0
            status: pending
            dependencies:
              depends_on:
                - T-MLR-2.4.2
            exit_criteria: []
            domain: product
            description: >-
              Part of Validate model performance against objective thresholds: metric:weather_elasticity_sign_correct =
              1.0
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-2.4.4
            title: metric:no_overfitting_detected = true
            status: pending
            dependencies:
              depends_on:
                - T-MLR-2.4.3
            exit_criteria: []
            domain: product
            description: 'Part of Validate model performance against objective thresholds: metric:no_overfitting_detected = true'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-2.4.5
            title: critic:modeling_reality_v2
            status: pending
            dependencies:
              depends_on:
                - T-MLR-2.4.4
            exit_criteria: []
            domain: product
            description: 'Part of Validate model performance against objective thresholds: critic:modeling_reality_v2'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-2.4.6
            title: critic:academic_rigor
            status: pending
            dependencies:
              depends_on:
                - T-MLR-2.4.5
            exit_criteria: []
            domain: product
            description: 'Part of Validate model performance against objective thresholds: critic:academic_rigor'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-2.6.1
            title: artifact:experiments/mmm_v2/robustness_report.json
            status: pending
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: >-
              Part of Run robustness tests (outliers, missing data, edge cases):
              artifact:experiments/mmm_v2/robustness_report.json
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-2.6.2
            title: test:Model handles extreme weather without crash
            status: pending
            dependencies:
              depends_on:
                - T-MLR-2.6.1
            exit_criteria: []
            domain: product
            description: >-
              Part of Run robustness tests (outliers, missing data, edge cases): test:Model handles extreme weather
              without crash
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-2.6.3
            title: test:Model handles missing data gracefully
            status: pending
            dependencies:
              depends_on:
                - T-MLR-2.6.2
            exit_criteria: []
            domain: product
            description: >-
              Part of Run robustness tests (outliers, missing data, edge cases): test:Model handles missing data
              gracefully
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-2.6.4
            title: test:Zero ad spend predicts organic baseline
            status: pending
            dependencies:
              depends_on:
                - T-MLR-2.6.3
            exit_criteria: []
            domain: product
            description: >-
              Part of Run robustness tests (outliers, missing data, edge cases): test:Zero ad spend predicts organic
              baseline
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-2.6.5
            title: test:pytest tests/model/test_mmm_robustness.py (15/15 pass)
            status: pending
            dependencies:
              depends_on:
                - T-MLR-2.6.4
            exit_criteria: []
            domain: product
            description: >-
              Part of Run robustness tests (outliers, missing data, edge cases): test:pytest
              tests/model/test_mmm_robustness.py (15/15 pass)
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-2.6.6
            title: critic:modeling_reality_v2
            status: pending
            dependencies:
              depends_on:
                - T-MLR-2.6.5
            exit_criteria: []
            domain: product
            description: 'Part of Run robustness tests (outliers, missing data, edge cases): critic:modeling_reality_v2'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-3.1.1
            title: artifact:experiments/mmm_v2/validation_notebook.ipynb
            status: pending
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: 'Part of Create reproducible validation notebook: artifact:experiments/mmm_v2/validation_notebook.ipynb'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-3.1.2
            title: test:Notebook runs end-to-end without errors
            status: pending
            dependencies:
              depends_on:
                - T-MLR-3.1.1
            exit_criteria: []
            domain: product
            description: 'Part of Create reproducible validation notebook: test:Notebook runs end-to-end without errors'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-3.1.3
            title: test:Output matches claimed metrics
            status: pending
            dependencies:
              depends_on:
                - T-MLR-3.1.2
            exit_criteria: []
            domain: product
            description: 'Part of Create reproducible validation notebook: test:Output matches claimed metrics'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-3.1.4
            title: artifact:experiments/mmm_v2/validation_notebook.html
            status: pending
            dependencies:
              depends_on:
                - T-MLR-3.1.3
            exit_criteria: []
            domain: product
            description: 'Part of Create reproducible validation notebook: artifact:experiments/mmm_v2/validation_notebook.html'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-3.1.5
            title: critic:academic_rigor
            status: pending
            dependencies:
              depends_on:
                - T-MLR-3.1.4
            exit_criteria: []
            domain: product
            description: 'Part of Create reproducible validation notebook: critic:academic_rigor'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
  - id: E-ML-REMEDIATION
    title: ML Model Remediation - From Prototype to Production
    status: pending
    domain: product
    milestones:
      - id: M-MLR-0
        title: 'Foundation: Truth & Accountability'
        status: pending
        tasks:
          - id: T-MLR-0.1
            title: Create ModelingReality critic with quantitative thresholds
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:tools/wvo_mcp/src/critics/modeling_reality_v2.ts
              - prose: test:Critic FAILS when R² < 0.50
              - prose: test:Critic FAILS when no baseline comparison
              - prose: test:Critic FAILS when weather elasticity signs wrong
              - prose: metric:critic_strictness = 1.0
              - prose: critic:tests
            domain: product
            description: |
              Create critic that enforces quantitative thresholds: R² > 0.50, correct
              elasticity signs, baseline comparison required, no subjective judgment.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-0.2
            title: Update all ML task exit criteria with objective metrics
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:state/roadmap.yaml (T12.*, T13.* updated)
              - prose: verification:All ML tasks have "metric:r2 > 0.50"
              - prose: verification:All ML tasks have "metric:beats_baseline > 1.10"
              - prose: verification:All ML tasks have "critic:modeling_reality_v2"
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-0.3
            title: Document world-class quality standards for ML
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:docs/ML_QUALITY_STANDARDS.md
              - prose: verification:Numeric thresholds for all metrics
              - prose: verification:Baseline comparison requirements
              - prose: review:External ML practitioner peer review
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M-MLR-1
        title: 'Phase 1: Fix Synthetic Data (2 weeks)'
        status: pending
        tasks:
          - id: T-MLR-1.1
            title: Debug and fix weather multiplier logic in data generator
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:scripts/weather/generate_synthetic_tenants_v2.py
              - prose: test:Extreme correlation = 0.85 ± 0.05
              - prose: test:High correlation = 0.70 ± 0.05
              - prose: test:Medium correlation = 0.40 ± 0.05
              - prose: test:None correlation < 0.10
              - prose: critic:data_quality
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-1.2
            title: Generate 3 years of synthetic data for 20 tenants
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:storage/seeds/synthetic_v2/*.parquet (20 files)
              - prose: metric:total_rows = 219000
              - prose: metric:date_range = 2022-01-01 to 2024-12-31
              - prose: metric:weather_correlations_within_target >= 0.90
              - prose: test:pytest tests/data_gen/test_synthetic_v2_quality.py
              - prose: critic:data_quality
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-1.3
            title: Create validation tests for synthetic data quality
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:tests/data_gen/test_synthetic_v2_quality.py
              - prose: test:20/20 tenant tests pass
              - prose: artifact:experiments/data_validation/correlation_plots.pdf
              - prose: critic:tests
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M-MLR-2
        title: 'Phase 2: Rigorous MMM Training (3 weeks)'
        status: pending
        tasks:
          - id: T-MLR-2.1
            title: Implement proper train/val/test splitting with no leakage
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:shared/libs/modeling/time_series_split.py
              - prose: test:Validation after training (no date overlap)
              - prose: test:Test after validation (no date overlap)
              - prose: test:Split percentages 70/15/15
              - prose: critic:leakage
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-2.2
            title: Implement LightweightMMM with weather features
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:apps/model/mmm_lightweight_weather.py
              - prose: test:Adstock transformation applied
              - prose: test:Hill saturation curves applied
              - prose: test:Weather interaction terms included
              - prose: test:pytest tests/model/test_mmm_lightweight.py (12/12 pass)
              - prose: critic:academic_rigor
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-2.4
            title: Validate model performance against objective thresholds
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/mmm_v2/validation_report.json
              - prose: metric:weather_sensitive_r2_pass_rate >= 0.80
              - prose: metric:weather_elasticity_sign_correct = 1.0
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-2.5
            title: Compare models to baseline (naive/seasonal/linear)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/mmm_v2/baseline_comparison.json
              - prose: metric:beats_naive_by >= 1.10
              - prose: metric:beats_seasonal_by >= 1.05
              - prose: metric:beats_linear_by >= 1.05
              - prose: artifact:experiments/mmm_v2/baseline_comparison_plots.pdf
              - prose: critic:modeling_reality_v2
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-2.6
            title: Run robustness tests (outliers, missing data, edge cases)
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/mmm_v2/robustness_report.json
              - prose: test:Model handles extreme weather without crash
              - prose: test:Model handles missing data gracefully
              - prose: test:Zero ad spend predicts organic baseline
              - prose: test:pytest tests/model/test_mmm_robustness.py (15/15 pass)
              - prose: critic:modeling_reality_v2
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M-MLR-3
        title: 'Phase 3: Reproducibility & Documentation (1 week)'
        status: pending
        tasks:
          - id: T-MLR-3.1
            title: Create reproducible validation notebook
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/mmm_v2/validation_notebook.ipynb
              - prose: test:Notebook runs end-to-end without errors
              - prose: test:Output matches claimed metrics
              - prose: artifact:experiments/mmm_v2/validation_notebook.html
              - prose: critic:academic_rigor
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-3.2
            title: Write comprehensive ML validation documentation
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:docs/ML_VALIDATION_COMPLETE.md
              - prose: verification:Links to reproducible notebook
              - prose: verification:Includes limitations section
              - prose: verification:Includes baseline comparisons
              - prose: review:External ML practitioner peer review
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
  - id: E-PHASE0
    title: 'Phase 0: Measurement & Confidence'
    status: done
    domain: product
    milestones:
      - id: M0.1
        title: Measurement & Confidence Foundations
        status: done
        tasks:
          - id: T0.1.1
            title: Implement geo holdout plumbing
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:state/analytics/experiments/geo_holdouts/*.json
              - prose: artifact:state/telemetry/experiments/geo_holdout_runs.jsonl
              - prose: critic:data_quality
            domain: product
            description: Wire apps/validation/incrementality.py into ingestion runs with nightly job execution
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T0.1.2
            title: Build lift & confidence UI surfaces
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:apps/api/schemas/plan.py
              - prose: artifact:apps/web/src/pages/plan.tsx
              - prose: critic:tests
              - prose: critic:design_system
            domain: product
            description: Plan API surfaces experiment payloads; Plan UI renders lift/confidence cards with download
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T0.1.3
            title: Generate forecast calibration report
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:docs/modeling/forecast_calibration_report.md
              - prose: artifact:state/telemetry/calibration/*.json
              - prose: critic:forecast_stitch
            domain: product
            description: Quantile calibration metrics with summary published to docs
            complexity_score: 5
            effort_hours: 2
            required_tools: []
  - id: E-PHASE1
    title: 'Phase 1: Experience Delivery'
    status: done
    domain: product
    milestones:
      - id: M1.1
        title: Experience Delivery MVP
        status: done
        tasks:
          - id: T1.1.3
            title: Wire onboarding progress API
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:apps/api/routes/onboarding.py
              - prose: artifact:apps/web/src/hooks/useOnboardingProgress.ts
              - prose: critic:tests
            domain: product
            description: Implement GET/POST /onboarding/progress routes with telemetry instrumentation
            complexity_score: 5
            effort_hours: 2
            required_tools: []
  - id: E-REMEDIATION
    title: '[CRITICAL] Quality Remediation - Audit All Completed Work'
    status: blocked
    domain: product
    milestones:
      - id: M-MLR-2
        title: M-MLR-2
        status: pending
        tasks:
          - id: T-MLR-2.3
            title: Train models on all 20 synthetic tenants with cross-validation
            status: blocked
            dependencies:
              depends_on:
                - T-MLR-2.2
            exit_criteria: []
            domain: product
            description: Train models on all 20 synthetic tenants with cross-validation
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M-MLR-3
        title: M-MLR-3
        status: blocked
        tasks:
          - id: T-MLR-3.3
            title: Package all evidence artifacts for review
            status: blocked
            dependencies:
              depends_on:
                - T-MLR-3.2
            exit_criteria: []
            domain: product
            description: Package all evidence artifacts for review
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M-MLR-4
        title: M-MLR-4
        status: blocked
        tasks:
          - id: T-MLR-4.1
            title: Deploy ModelingReality_v2 critic to production
            status: pending
            dependencies:
              depends_on:
                - T-MLR-0.1
                - T-MLR-3.3
            exit_criteria: []
            domain: product
            description: Deploy ModelingReality_v2 critic to production
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-4.2
            title: Update autopilot policy to require critic approval
            status: done
            dependencies:
              depends_on:
                - T-MLR-4.1
            exit_criteria: []
            domain: product
            description: Update autopilot policy to require critic approval
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-4.3
            title: Create meta-critic to review past completed ML tasks
            status: blocked
            dependencies:
              depends_on:
                - T-MLR-4.2
            exit_criteria: []
            domain: product
            description: Create meta-critic to review past completed ML tasks
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T-MLR-4.4
            title: Document lessons learned and update contributor guide
            status: pending
            dependencies:
              depends_on:
                - T-MLR-4.3
            exit_criteria: []
            domain: product
            description: Document lessons learned and update contributor guide
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M-REM-1
        title: '[CRITICAL] Core Infrastructure Audit'
        status: blocked
        tasks:
          - id: CRIT-PERF-ACADEMICRIGOR-b0301a
            title: '[Critic:academicrigor] Restore performance'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: |-
              Critic academicrigor is underperforming and needs immediate remediation.

              Identity: Academic Rigor (academic_rigor, authority advisory)
              Mission: Safeguard academic_rigor discipline.
              Signature powers: Reports on findings when configuration is missing.

              Critic academicrigor failed 10 of the last 11 runs with 0 consecutive failures.

              Observation window: 11 runs

              Consecutive failures: 0

              Failures: 10 | Successes: 1

              Assigned to: Autopilot

              Expectations:
              - Diagnose root causes for the critic's repeated failures.
              - Patch critic configuration, training data, or underlying automation as needed.
              - Document findings in state/context.md and roadmap notes.
              - Close this task once the critic passes reliably.

              Latest output snippet:
              {
                "epic": "E12",
                "summary": {
                  "done": 15,
                  "in_progress": 0,
                  "pending": 0,
                  "blocked": 0
                },
                "critical_issues": [],
                "high_issues": [],
                "warnings": [],
                "doc_path": "/Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/docs/CRITIQUE_E12_20251022.md"
              }
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-ACADEMICRIGOR-f89932
            title: '[Critic:academicrigor] Restore performance'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: |-
              Critic academicrigor is underperforming and needs immediate remediation.

              Identity: Academic Rigor (academic_rigor, authority advisory)
              Mission: Safeguard academic_rigor discipline.
              Signature powers: Reports on findings when configuration is missing.

              Critic academicrigor failed 8 of the last 10 runs with 0 consecutive failures.

              Observation window: 10 runs

              Consecutive failures: 0

              Failures: 8 | Successes: 2

              Assigned to: Autopilot

              Expectations:
              - Diagnose root causes for the critic's repeated failures.
              - Patch critic configuration, training data, or underlying automation as needed.
              - Document findings in state/context.md and roadmap notes.
              - Close this task once the critic passes reliably.

              Latest output snippet:
              {
                "epic": "E12",
                "summary": {
                  "done": 15,
                  "in_progress": 0,
                  "pending": 0,
                  "blocked": 0
                },
                "critical_issues": [],
                "high_issues": [],
                "warnings": [],
                "doc_path": "/Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/docs/CRITIQUE_E12_20251022.md"
              }
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-ALLOCATOR-bc8604
            title: '[Critic:allocator] Restore performance'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: >-
              Critic allocator is underperforming and needs immediate remediation.


              Identity: Allocator Sentinel (operations, authority advisory)

              Mission: Ensure planner allocation and task routing stay optimal.

              Signature powers: Diagnoses misrouted tasks and capacity imbalances.; Suggests rebalancing across agents
              and squads.

              Autonomy guidance: Auto-adjust planner weights when safe; escalate persistent misallocations to Autopilot.


              Critic allocator failed 8 of the last 10 runs with 0 consecutive failures.


              Observation window: 10 runs


              Consecutive failures: 0


              Failures: 8 | Successes: 2


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              ============================= test session starts ==============================

              platform darwin -- Python 3.10.12, pytest-8.4.2, pluggy-1.6.0

              rootdir: /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane

              configfile: pyproject.toml

              plugins: anyio-3.7.1, asyncio-1.2.0

              asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None,
              asyncio_default_test_loop_scope=function

              collected 5 items


              tests/test_allocator_routes.py ..                                        [ 40%]

              tests/test_creative_route.py .                                           [ 60%]

              tests/apps/model/test_cre...
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-BUILD-958e1f
            title: '[Critic:build] Restore performance'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: >-
              Critic build is underperforming and needs immediate remediation.


              Identity: Build Sentinel (engineering, authority blocking)

              Mission: Guarantee that core build processes remain reproducible and optimized across environments.

              Signature powers: Diagnoses build pipeline regressions and unstable toolchains.; Flags missing build
              artifacts or misconfigured dependencies before release.

              Autonomy guidance: Attempt automated patching of build scripts when safe; escalate infrastructure
              escalations beyond local fixes.


              Critic build failed 5 of the last 6 runs with 0 consecutive failures.


              Observation window: 6 runs


              Consecutive failures: 0


              Failures: 5 | Successes: 1


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              warning: The top-level linter settings are deprecated in favour of their counterparts in the `lint`
              section. Please update the following options in `pyproject.toml`:
                - 'select' -> 'lint.select'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-CAUSAL-070d3d
            title: '[Critic:causal] Restore performance'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: >-
              Critic causal is underperforming and needs immediate remediation.


              Identity: Causal Strategist (ml, authority critical)

              Mission: Guarantee counterfactual validity and causal modeling rigor.

              Signature powers: Checks identifying assumptions, invariances, and instrumentation.; Constructs mitigation
              plans for confounding risks.

              Autonomy guidance: Partner with Research Orchestrator on complex interventions; log learnings for future
              experiments.


              No successful runs recorded in the last 12 observations; 12 consecutive failures detected.


              Observation window: 12 runs


              Consecutive failures: 12


              Failures: 12 | Successes: 0


              Assigned to: Director Dana


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              skipped due to capability profile
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-CAUSAL-e7682e
            title: '[Critic:causal] Restore performance'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: >-
              Critic causal is underperforming and needs immediate remediation.


              Identity: Causal Strategist (ml, authority critical)

              Mission: Guarantee counterfactual validity and causal modeling rigor.

              Signature powers: Checks identifying assumptions, invariances, and instrumentation.; Constructs mitigation
              plans for confounding risks.

              Autonomy guidance: Partner with Research Orchestrator on complex interventions; log learnings for future
              experiments.


              Critic causal failed 8 of the last 10 runs with 0 consecutive failures.


              Observation window: 10 runs


              Consecutive failures: 0


              Failures: 8 | Successes: 2


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              {
                "status": "passed",
                "level": "medium",
                "findings": [
                  {
                    "severity": "INFO",
                    "message": "Weather shock estimator present.",
                    "details": null
                  },
                  {
                    "severity": "INFO",
                    "message": "Weather shock tests passed.",
                    "details": "  /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/shared/libs/causal/weather_shock.py:194: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n  (Deprecated in version 0.20.5)\n    pl.count().alias(\"pre_count\"),\n\ntests/shared/libs/causal/test_weather_shock.py::test_weather_shock...
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-DESIGNSYSTEM-1a886a
            title: '[Critic:designsystem] Restore performance'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: >-
              Critic designsystem is underperforming and needs immediate remediation.


              Identity: Design System (design_system, authority advisory)

              Mission: Safeguard design_system discipline.

              Signature powers: Reports on findings when configuration is missing.


              No successful runs recorded in the last 9 observations; 5 consecutive failures detected.


              Observation window: 9 runs


              Consecutive failures: 5


              Failures: 6 | Successes: 3


              Assigned to: Director Dana


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              ./src/pages/dashboard.tsx

              968:14  Error: Parsing error: ')' expected.


              info  - Need to disable some ESLint rules? Learn more here:
              https://nextjs.org/docs/basic-features/eslint#disabling-rules
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-EXECREVIEW-ef2384
            title: '[Critic:execreview] Restore performance'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: |-
              Critic execreview is underperforming and needs immediate remediation.

              Identity: Exec Review (exec_review, authority advisory)
              Mission: Safeguard exec_review discipline.
              Signature powers: Reports on findings when configuration is missing.

              No successful runs recorded in the last 12 observations; 12 consecutive failures detected.

              Observation window: 12 runs

              Consecutive failures: 12

              Failures: 12 | Successes: 0

              Assigned to: Director Dana

              Expectations:
              - Diagnose root causes for the critic's repeated failures.
              - Patch critic configuration, training data, or underlying automation as needed.
              - Document findings in state/context.md and roadmap notes.
              - Close this task once the critic passes reliably.

              Latest output snippet:
              skipped due to capability profile
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-GLOBAL-9882b7
            title: '[Critics] Systemic performance remediation'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: |-
              Multiple critics are underperforming and require coordinated intervention.

              3 critics require director-level intervention after repeated failures.

              Affected critics: execreview, integrationfury, managerselfcheck

              Critics evaluated in run: 5

              Reports captured: 3

              Assigned to: Director Dana

              Expectations:
              - Review individual remediation tasks and look for systemic issues.
              - Adjust critic configurations, training loops, or staffing mixes.
              - Provide a coordination brief in state/context.md.
              - Close this systemic task once individual critics are back on track.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-HEALTHCHECK-0e6b67
            title: '[Critic:healthcheck] Restore performance'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: >-
              Critic healthcheck is underperforming and needs immediate remediation.


              Identity: Health Check (health_check, authority advisory)

              Mission: Safeguard health_check discipline.

              Signature powers: Reports on findings when configuration is missing.


              Critic healthcheck failed 4 of the last 5 runs with 0 consecutive failures.


              Observation window: 5 runs


              Consecutive failures: 0


              Failures: 4 | Successes: 1


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              warning: The top-level linter settings are deprecated in favour of their counterparts in the `lint`
              section. Please update the following options in `pyproject.toml`:
                - 'select' -> 'lint.select'
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-INTEGRATIONFURY-9401af
            title: '[Critic:integrationfury] Restore performance'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: >-
              Critic integrationfury is underperforming and needs immediate remediation.


              Identity: Integration Fury (integration_fury, authority advisory)

              Mission: Safeguard integration_fury discipline.

              Signature powers: Reports on findings when configuration is missing.


              No successful runs recorded in the last 6 observations; 6 consecutive failures detected.


              Observation window: 6 runs


              Consecutive failures: 6


              Failures: 6 | Successes: 0


              Assigned to: Director Dana


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              ============================= test session starts ==============================

              platform darwin -- Python 3.10.12, pytest-8.4.2, pluggy-1.6.0

              rootdir: /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane

              configfile: pyproject.toml

              plugins: anyio-4.11.0, asyncio-1.2.0

              asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None,
              asyncio_default_test_loop_scope=function

              collected 242 items


              tests/api/onboarding/test_progress.py ....                               [  1%]

              tests/api/test_ad_push_routes.py ....                                    [  3%]

              tests/api/test_dashboa...
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-MANAGERSELFCHECK-61ab48
            title: '[Critic:managerselfcheck] Restore performance'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: >-
              Critic managerselfcheck is underperforming and needs immediate remediation.


              Identity: Manager Self Check (manager_self_check, authority advisory)

              Mission: Safeguard manager_self_check discipline.

              Signature powers: Reports on findings when configuration is missing.


              No successful runs recorded in the last 6 observations; 6 consecutive failures detected.


              Observation window: 6 runs


              Consecutive failures: 6


              Failures: 6 | Successes: 0


              Assigned to: Director Dana


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              Rollback simulation stale (simulated_at=2025-10-15T21:05:00+00:00); rerun executor to refresh promotion
              gate.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-ORGPM-be2140
            title: '[Critic:orgpm] Restore performance'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: |-
              Critic orgpm is underperforming and needs immediate remediation.

              Identity: Org Pm (org_pm, authority advisory)
              Mission: Safeguard org_pm discipline.
              Signature powers: Reports on findings when configuration is missing.

              Critic orgpm failed 5 of the last 6 runs with 0 consecutive failures.

              Observation window: 6 runs

              Consecutive failures: 0

              Failures: 5 | Successes: 1

              Assigned to: Autopilot

              Expectations:
              - Diagnose root causes for the critic's repeated failures.
              - Patch critic configuration, training data, or underlying automation as needed.
              - Document findings in state/context.md and roadmap notes.
              - Close this task once the critic passes reliably.

              Latest output snippet:
              Org PM charter/state checks passed.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-PROMPTBUDGET-2c30f3
            title: '[Critic:promptbudget] Restore performance'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: >-
              Critic promptbudget is underperforming and needs immediate remediation.


              Identity: Prompt Budget (prompt_budget, authority advisory)

              Mission: Safeguard prompt_budget discipline.

              Signature powers: Reports on findings when configuration is missing.


              Critic promptbudget failed 5 of the last 6 runs with 0 consecutive failures.


              Observation window: 6 runs


              Consecutive failures: 0


              Failures: 5 | Successes: 1


              Assigned to: Autopilot


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              {"level":"warning","message":"Code search index rebuild
              failed","timestamp":"2025-10-16T20:39:47.650Z","error":"The database connection is not open"}
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-SECURITY-645edd
            title: '[Critic:security] Restore performance'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: >-
              Critic security is underperforming and needs immediate remediation.


              Identity: Security Sentinel (security, authority critical)

              Mission: Guard secrets, policies, and attack surfaces throughout the stack.

              Signature powers: Identifies credential leaks, insecure defaults, and policy gaps.; Cross-references
              security playbooks to recommend mitigations.

              Autonomy guidance: Demand sign-off for high-risk findings; coordinate with Director Dana and Security
              Stewards.


              No successful runs recorded in the last 9 observations; 6 consecutive failures detected.


              Observation window: 9 runs


              Consecutive failures: 6


              Failures: 6 | Successes: 3


              Assigned to: Director Dana


              Expectations:

              - Diagnose root causes for the critic's repeated failures.

              - Patch critic configuration, training data, or underlying automation as needed.

              - Document findings in state/context.md and roadmap notes.

              - Close this task once the critic passes reliably.


              Latest output snippet:

              skipped due to capability profile
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-TESTS-426598
            title: '[Critic:tests] Restore performance'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: "Critic tests is underperforming and needs immediate remediation.\n\nIdentity: Regression Hunter (quality, authority blocking)\nMission: Keep the test suites healthy and ensure deterministic results across flows.\nSignature powers: Surfaces flaky suites and failing assertions with reproduction notes.; Synthesizes minimal repro commands for Autopilot triage.\nAutonomy guidance: Rerun targeted suites automatically; lean on Autopilot only when new failures persist.\n\nCritic tests failed 5 of the last 6 runs with 0 consecutive failures.\n\nObservation window: 6 runs\n\nConsecutive failures: 0\n\nFailures: 5 | Successes: 1\n\nAssigned to: Autopilot\n\nExpectations:\n- Diagnose root causes for the critic's repeated failures.\n- Patch critic configuration, training data, or underlying automation as needed.\n- Document findings in state/context.md and roadmap notes.\n- Close this task once the critic passes reliably.\n\nLatest output snippet:\n\e[33mThe CJS build of Vite's Node API is deprecated. See https://vite.dev/guide/troubleshooting.html#vite-cjs-node-api-deprecated for more details.\e[39m\n{\"level\":\"info\",\"message\":\"Subscription limit tracker initialized\",\"timestamp\":\"2025-10-16T21:37:34.835Z\",\"providers\":[]}\n{\"level\":\"info\",\"message\":\"Provider registered for usage tracking\",\"timestamp\":\"2025-10-16T21:37:34.837Z\",\"provider\":\"claude\",\"account\":\"test-account\",\"tier\":\"pro\"}\n{\"level\":\"info\",\"message\":\"Subscription limit tracker stopped\",\"timestamp\":\"2025-10-16T21:37:34.840Z\"}\n{\"level\":\"info\",\"message\":\"Subscription limit tracker ..."
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: CRIT-PERF-TESTS-c3fce7
            title: '[Critic:tests] Restore performance'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: "Critic tests is underperforming and needs immediate remediation.\n\nIdentity: Regression Hunter (quality, authority blocking)\nMission: Keep the test suites healthy and ensure deterministic results across flows.\nSignature powers: Surfaces flaky suites and failing assertions with reproduction notes.; Synthesizes minimal repro commands for Autopilot triage.\nAutonomy guidance: Rerun targeted suites automatically; lean on Autopilot only when new failures persist.\n\nCritic tests failed 5 of the last 6 runs with 4 consecutive failures.\n\nObservation window: 6 runs\n\nConsecutive failures: 4\n\nFailures: 5 | Successes: 1\n\nAssigned to: Autopilot\n\nExpectations:\n- Diagnose root causes for the critic's repeated failures.\n- Patch critic configuration, training data, or underlying automation as needed.\n- Document findings in state/context.md and roadmap notes.\n- Close this task once the critic passes reliably.\n\nLatest output snippet:\n\e[33mThe CJS build of Vite's Node API is deprecated. See https://vite.dev/guide/troubleshooting.html#vite-cjs-node-api-deprecated for more details.\e[39m\n⎯⎯⎯⎯⎯⎯⎯ Failed Tests 6 ⎯⎯⎯⎯⎯⎯⎯\n\n FAIL  ../../tests/web/design_system_acceptance.spec.ts > Design system acceptance – Stories page > renders skip navigation, main landmark, and story metadata tokens\nAssertionError: expected \"error\" to not be called at all, but actually been called 5 times\n\nReceived: \n\n  1st error call:\n\n    Array [\n      \"Warning: An update to %s inside a test was not wrapped in act(...).\n    \n    When testing, code that cau..."
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: PHASE-1-HARDENING
            title: 'Phase 1: MCP Hardening'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: >-
              BLOCKING – must complete before other work (disable YAML writes, real usage/cost, correlation IDs,
              coordinator failover)
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: PHASE-2-COMPACT
            title: 'Phase 2: Compact Prompts + Selfchecks'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: BLOCKING – must complete before other work (compact context assembler, snapshot selfcheck)
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: PHASE-3-BATCH
            title: 'Phase 3: Batch Queue & Prompt Headers'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: BLOCKING – must complete before other work (batch queue, stable headers, token heuristics)
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-0.1
            title: '[REM] Verify: Create ModelingReality critic with quantitative thresholds'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T-MLR-0.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Create ModelingReality critic with quantitative thresholds

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T-MLR-0.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T-MLR-0.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T-MLR-0.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T-MLR-0.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T-MLR-0.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T-MLR-0.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-0.2
            title: '[REM] Verify: Update all ML task exit criteria with objective metrics'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T-MLR-0.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Update all ML task exit criteria with objective metrics

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T-MLR-0.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T-MLR-0.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T-MLR-0.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T-MLR-0.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T-MLR-0.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T-MLR-0.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-1.1
            title: '[REM] Verify: Debug and fix weather multiplier logic in data generator'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T-MLR-1.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Debug and fix weather multiplier logic in data generator

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T-MLR-1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T-MLR-1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T-MLR-1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T-MLR-1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T-MLR-1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T-MLR-1.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-1.3
            title: '[REM] Verify: Create validation tests for synthetic data quality'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T-MLR-1.3 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Create validation tests for synthetic data quality

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T-MLR-1.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T-MLR-1.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T-MLR-1.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T-MLR-1.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T-MLR-1.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T-MLR-1.3 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-2.1
            title: '[REM] Verify: Implement proper train/val/test splitting with no leakage'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T-MLR-2.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Implement proper train/val/test splitting with no leakage

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T-MLR-2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T-MLR-2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T-MLR-2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T-MLR-2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T-MLR-2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T-MLR-2.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-2.2
            title: '[REM] Verify: Implement LightweightMMM with weather features'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T-MLR-2.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Implement LightweightMMM with weather features

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T-MLR-2.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T-MLR-2.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T-MLR-2.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T-MLR-2.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T-MLR-2.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T-MLR-2.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-2.5
            title: '[REM] Verify: Compare models to baseline (naive/seasonal/linear)'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T-MLR-2.5 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Compare models to baseline (naive/seasonal/linear)

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T-MLR-2.5
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T-MLR-2.5
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T-MLR-2.5
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T-MLR-2.5
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T-MLR-2.5 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T-MLR-2.5 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T-MLR-3.2
            title: '[REM] Verify: Write comprehensive ML validation documentation'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T-MLR-3.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Write comprehensive ML validation documentation

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T-MLR-3.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T-MLR-3.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T-MLR-3.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T-MLR-3.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T-MLR-3.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T-MLR-3.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T0.1.1
            title: '[REM] Verify: Implement geo holdout plumbing'
            status: blocked
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T0.1.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Implement geo holdout plumbing

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T0.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T0.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T0.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T0.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T0.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T0.1.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T0.1.2
            title: '[REM] Verify: Build lift & confidence UI surfaces'
            status: blocked
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T0.1.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Build lift & confidence UI surfaces

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T0.1.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T0.1.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T0.1.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T0.1.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T0.1.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T0.1.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T0.1.3
            title: '[REM] Verify: Generate forecast calibration report'
            status: blocked
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T0.1.3 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Generate forecast calibration report

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T0.1.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T0.1.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T0.1.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T0.1.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T0.1.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T0.1.3 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T1.1.1
            title: '[REM] Verify: Design Open-Meteo + Shopify connectors and data contracts'
            status: blocked
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T1.1.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Design Open-Meteo + Shopify connectors and data contracts

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T1.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T1.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T1.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T1.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T1.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T1.1.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T1.1.2
            title: '[REM] Verify: Implement ingestion Prefect flow with checkpointing'
            status: blocked
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T1.1.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Implement ingestion Prefect flow with checkpointing

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T1.1.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T1.1.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T1.1.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T1.1.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T1.1.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T1.1.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T1.1.3
            title: '[REM] Verify: Wire onboarding progress API'
            status: needs_review
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T1.1.3 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Wire onboarding progress API

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T1.1.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T1.1.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T1.1.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T1.1.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T1.1.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T1.1.3 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T1.2.1
            title: '[REM] Verify: Blend historical + forecast weather, enforce timezone alignm'
            status: blocked
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T1.2.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Blend historical + forecast weather, enforce timezone alignm

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T1.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T1.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T1.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T1.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T1.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T1.2.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T1.2.2
            title: '[REM] Verify: Add leakage guardrails to feature builder'
            status: blocked
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T1.2.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Add leakage guardrails to feature builder

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T1.2.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T1.2.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T1.2.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T1.2.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T1.2.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T1.2.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T10.1.1
            title: '[REM] Verify: Cost telemetry and budget alerts'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T10.1.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Cost telemetry and budget alerts

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T10.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T10.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T10.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T10.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T10.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T10.1.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T11.1.1
            title: '[REM] Verify: Implement hardware probe & profile persistence'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T11.1.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Implement hardware probe & profile persistence

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T11.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T11.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T11.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T11.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T11.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T11.1.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T11.1.2
            title: '[REM] Verify: Adaptive scheduling for heavy tasks'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T11.1.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Adaptive scheduling for heavy tasks

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T11.1.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T11.1.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T11.1.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T11.1.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T11.1.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T11.1.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T11.2.1
            title: '[REM] Verify: Design system elevation (motion, typography, theming)'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T11.2.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Design system elevation (motion, typography, theming)

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T11.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T11.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T11.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T11.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T11.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T11.2.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T11.2.2
            title: '[REM] Verify: Award-level experience audit & remediation'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T11.2.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Award-level experience audit & remediation

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T11.2.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T11.2.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T11.2.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T11.2.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T11.2.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T11.2.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T11.2.3
            title: '[REM] Verify: Extend calm/aero theme tokens to Automations and Experiments'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T11.2.3 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Extend calm/aero theme tokens to Automations and Experiments

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T11.2.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T11.2.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T11.2.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T11.2.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T11.2.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T11.2.3 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T11.2.4
            title: '[REM] Verify: Refactor landing/marketing gradients into reusable tokens'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T11.2.4 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Refactor landing/marketing gradients into reusable tokens

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T11.2.4
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T11.2.4
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T11.2.4
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T11.2.4
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T11.2.4 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T11.2.4 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T11.2.5
            title: '[REM] Verify: Centralize retry button styles in shared component once App'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T11.2.5 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Centralize retry button styles in shared component once App 

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T11.2.5
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T11.2.5
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T11.2.5
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T11.2.5
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T11.2.5 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T11.2.5 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T11.2.6
            title: '[REM] Verify: Formalize shared panel mixin (border + shadow) to reduce ove'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T11.2.6 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Formalize shared panel mixin (border + shadow) to reduce ove

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T11.2.6
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T11.2.6
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T11.2.6
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T11.2.6
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T11.2.6 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T11.2.6 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T12.0.3
            title: '[REM] Verify: Document synthetic tenant characteristics'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T12.0.3 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Document synthetic tenant characteristics

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T12.0.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T12.0.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T12.0.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T12.0.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T12.0.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T12.0.3 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T12.1.2
            title: '[REM] Verify: Validate feature store joins against historical weather base'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T12.1.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Validate feature store joins against historical weather base

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T12.1.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T12.1.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T12.1.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T12.1.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T12.1.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T12.1.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T12.2.1
            title: '[REM] Verify: Backtest weather-aware model vs control across top tenants'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T12.2.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Backtest weather-aware model vs control across top tenants

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T12.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T12.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T12.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T12.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T12.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T12.2.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T12.PoC.3
            title: '[REM] Verify: Create PoC demo results and proof brief'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T12.PoC.3 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Create PoC demo results and proof brief

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T12.PoC.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T12.PoC.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T12.PoC.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T12.PoC.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T12.PoC.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T12.PoC.3 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T13.1.1
            title: '[REM] Verify: Validate 90-day tenant data coverage across sales, spend, an'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T13.1.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Validate 90-day tenant data coverage across sales, spend, an

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T13.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T13.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T13.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T13.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T13.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T13.1.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T13.1.3
            title: '[REM] Verify: Implement product taxonomy auto-classification with weather'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T13.1.3 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Implement product taxonomy auto-classification with weather 

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T13.1.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T13.1.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T13.1.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T13.1.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T13.1.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T13.1.3 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T13.1.4
            title: '[REM] Verify: Data quality validation framework (verify data fitness for M'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T13.1.4 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Data quality validation framework (verify data fitness for M

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T13.1.4
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T13.1.4
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T13.1.4
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T13.1.4
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T13.1.4 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T13.1.4 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T13.2.1
            title: '[REM] Verify: Replace heuristic MMM with LightweightMMM adstock+saturation'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T13.2.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Replace heuristic MMM with LightweightMMM adstock+saturation

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T13.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T13.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T13.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T13.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T13.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T13.2.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T13.2.3
            title: '[REM] Verify: Replace heuristic allocator with constraint-aware optimizer'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T13.2.3 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Replace heuristic allocator with constraint-aware optimizer

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T13.2.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T13.2.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T13.2.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T13.2.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T13.2.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T13.2.3 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T13.3.2
            title: '[REM] Verify: Implement DMA-first geographic aggregation with hierarchical'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T13.3.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Implement DMA-first geographic aggregation with hierarchical

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T13.3.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T13.3.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T13.3.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T13.3.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T13.3.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T13.3.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T13.4.1
            title: '[REM] Verify: Add modeling reality critic to Autopilot'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T13.4.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Add modeling reality critic to Autopilot

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T13.4.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T13.4.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T13.4.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T13.4.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T13.4.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T13.4.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T13.5.1
            title: '[REM] Verify: Train weather-aware allocation model on top of MMM baseline'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T13.5.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Train weather-aware allocation model on top of MMM baseline

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T13.5.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T13.5.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T13.5.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T13.5.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T13.5.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T13.5.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T2.1.1
            title: '[REM] Verify: Build lag/rolling feature generators with deterministic seed'
            status: blocked
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T2.1.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Build lag/rolling feature generators with deterministic seed

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T2.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T2.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T2.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T2.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T2.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T2.1.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T2.2.1
            title: '[REM] Verify: Train weather-aware GAM baseline and document methodology'
            status: needs_review
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T2.2.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Train weather-aware GAM baseline and document methodology

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T2.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T2.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T2.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T2.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T2.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T2.2.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T3.1.1
            title: '[REM] Verify: Implement budget allocator stress tests and regret bounds'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T3.1.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Implement budget allocator stress tests and regret bounds

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T3.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T3.1.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T3.2.1
            title: '[REM] Verify: Run design system critic and ensure accessibility coverage'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T3.2.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Run design system critic and ensure accessibility coverage

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T3.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T3.2.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T3.2.2
            title: '[REM] Verify: Elevate dashboard storytelling & UX'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T3.2.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Elevate dashboard storytelling & UX

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T3.2.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.2.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.2.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.2.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.2.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T3.2.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T3.3.1
            title: '[REM] Verify: Draft multi-agent charter & delegation mesh (AutoGen/Swarm p'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T3.3.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Draft multi-agent charter & delegation mesh (AutoGen/Swarm p

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T3.3.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.3.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.3.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.3.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.3.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T3.3.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T3.3.2
            title: '[REM] Verify: Implement hierarchical consensus & escalation engine'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T3.3.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Implement hierarchical consensus & escalation engine

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T3.3.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.3.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.3.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.3.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.3.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T3.3.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T3.3.3
            title: '[REM] Verify: Build closed-loop simulation harness for autonomous teams'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T3.3.3 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Build closed-loop simulation harness for autonomous teams

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T3.3.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.3.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.3.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.3.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.3.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T3.3.3 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T3.3.4
            title: '[REM] Verify: Instrument dynamic staffing telemetry & learning pipeline'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T3.3.4 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Instrument dynamic staffing telemetry & learning pipeline

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T3.3.4
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.3.4
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.3.4
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.3.4
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.3.4 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T3.3.4 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T3.4.1
            title: '[REM] Verify: Implement Plan overview page with weather-driven insights'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T3.4.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Implement Plan overview page with weather-driven insights

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T3.4.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.4.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.4.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.4.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.4.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T3.4.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T3.4.2
            title: '[REM] Verify: Build WeatherOps dashboard with allocator + weather KPIs'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T3.4.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Build WeatherOps dashboard with allocator + weather KPIs

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T3.4.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.4.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.4.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.4.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.4.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T3.4.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T3.4.3
            title: '[REM] Verify: Ship Experiments hub UI for uplift & incrementality reviews'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T3.4.3 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Ship Experiments hub UI for uplift & incrementality reviews

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T3.4.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.4.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.4.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.4.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.4.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T3.4.3 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T3.4.4
            title: '[REM] Verify: Deliver storytelling Reports view with weather + spend narra'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T3.4.4 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Deliver storytelling Reports view with weather + spend narra

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T3.4.4
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.4.4
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.4.4
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.4.4
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.4.4 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T3.4.4 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T3.4.5
            title: '[REM] Verify: Conduct design_system + UX acceptance review across implemen'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T3.4.5 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Conduct design_system + UX acceptance review across implemen

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T3.4.5
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.4.5
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.4.5
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.4.5
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.4.5 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T3.4.5 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T3.4.6
            title: '[REM] Verify: Rewrite WeatherOps dashboard around plain-language decisions'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T3.4.6 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Rewrite WeatherOps dashboard around plain-language decisions

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T3.4.6
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.4.6
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.4.6
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.4.6
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.4.6 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T3.4.6 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T3.4.7
            title: '[REM] Verify: Reimagine Automations change log as a trust-first narrative'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T3.4.7 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Reimagine Automations change log as a trust-first narrative

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T3.4.7
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T3.4.7
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T3.4.7
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T3.4.7
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T3.4.7 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T3.4.7 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T4.1.10
            title: '[REM] Verify: Cross-market saturation optimization (fairness-aware)'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T4.1.10 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Cross-market saturation optimization (fairness-aware)

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T4.1.10
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T4.1.10
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T4.1.10
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T4.1.10
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T4.1.10 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T4.1.10 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T4.1.3
            title: '[REM] Verify: Causal uplift modeling & incremental lift validation'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T4.1.3 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Causal uplift modeling & incremental lift validation

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T4.1.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T4.1.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T4.1.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T4.1.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T4.1.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T4.1.3 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T4.1.4
            title: '[REM] Verify: Multi-horizon ensemble forecasting'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T4.1.4 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Multi-horizon ensemble forecasting

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T4.1.4
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T4.1.4
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T4.1.4
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T4.1.4
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T4.1.4 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T4.1.4 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T4.1.5
            title: '[REM] Verify: Non-linear allocation optimizer with constraints (ROAS, spen'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T4.1.5 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Non-linear allocation optimizer with constraints (ROAS, spen

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T4.1.5
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T4.1.5
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T4.1.5
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T4.1.5
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T4.1.5 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T4.1.5 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T4.1.6
            title: '[REM] Verify: High-frequency spend response modeling (intraday adjustments'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T4.1.6 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: High-frequency spend response modeling (intraday adjustments

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T4.1.6
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T4.1.6
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T4.1.6
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T4.1.6
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T4.1.6 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T4.1.6 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T4.1.7
            title: '[REM] Verify: Marketing mix budget solver (multi-channel, weather-aware)'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T4.1.7 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Marketing mix budget solver (multi-channel, weather-aware)

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T4.1.7
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T4.1.7
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T4.1.7
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T4.1.7
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T4.1.7 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T4.1.7 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T4.1.8
            title: '[REM] Verify: Reinforcement-learning shadow mode (safe exploration)'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T4.1.8 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Reinforcement-learning shadow mode (safe exploration)

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T4.1.8
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T4.1.8
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T4.1.8
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T4.1.8
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T4.1.8 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T4.1.8 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T4.1.9
            title: '[REM] Verify: Creative-level response modeling with brand safety guardrail'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T4.1.9 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Creative-level response modeling with brand safety guardrail

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T4.1.9
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T4.1.9
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T4.1.9
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T4.1.9
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T4.1.9 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T4.1.9 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T5.1.1
            title: '[REM] Verify: Implement Meta Marketing API client (creative + campaign man'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T5.1.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Implement Meta Marketing API client (creative + campaign man

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T5.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T5.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T5.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T5.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T5.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T5.1.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T5.1.2
            title: '[REM] Verify: Meta sandbox and dry-run executor with credential vaulting'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T5.1.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Meta sandbox and dry-run executor with credential vaulting

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T5.1.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T5.1.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T5.1.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T5.1.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T5.1.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T5.1.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T5.2.1
            title: '[REM] Verify: Google Ads API integration (campaign create/update, shared b'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T5.2.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Google Ads API integration (campaign create/update, shared b

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T5.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T5.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T5.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T5.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T5.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T5.2.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T5.2.2
            title: '[REM] Verify: Budget reconciliation & spend guardrails across platforms'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T5.2.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Budget reconciliation & spend guardrails across platforms

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T5.2.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T5.2.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T5.2.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T5.2.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T5.2.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T5.2.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T5.3.1
            title: '[REM] Verify: Dry-run & diff visualizer for ad pushes (pre-flight checks)'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T5.3.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Dry-run & diff visualizer for ad pushes (pre-flight checks)

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T5.3.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T5.3.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T5.3.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T5.3.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T5.3.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T5.3.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T5.3.2
            title: '[REM] Verify: Automated rollback + alerting when performance/regression de'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T5.3.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Automated rollback + alerting when performance/regression de

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T5.3.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T5.3.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T5.3.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T5.3.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T5.3.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T5.3.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T6.1.1
            title: '[REM] Verify: MCP server integration tests (all 25 tools across both provi'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T6.1.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: MCP server integration tests (all 25 tools across both provi

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T6.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T6.1.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T6.1.2
            title: '[REM] Verify: Provider failover testing (token limit simulation & automati'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T6.1.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Provider failover testing (token limit simulation & automati

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T6.1.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.1.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.1.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.1.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.1.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T6.1.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T6.1.3
            title: '[REM] Verify: State persistence testing (checkpoint recovery across sessio'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T6.1.3 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: State persistence testing (checkpoint recovery across sessio

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T6.1.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.1.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.1.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.1.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.1.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T6.1.3 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T6.1.4
            title: '[REM] Verify: Quality framework validation (10 dimensions operational)'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T6.1.4 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Quality framework validation (10 dimensions operational)

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T6.1.4
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.1.4
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.1.4
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.1.4
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.1.4 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T6.1.4 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T6.2.1
            title: '[REM] Verify: Credentials security audit (auth.json, API keys, token rotat'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T6.2.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Credentials security audit (auth.json, API keys, token rotat

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T6.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T6.2.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T6.2.2
            title: '[REM] Verify: Error recovery testing (graceful degradation, retry logic)'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T6.2.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Error recovery testing (graceful degradation, retry logic)

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T6.2.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.2.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.2.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.2.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.2.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T6.2.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T6.2.3
            title: '[REM] Verify: Schema validation enforcement (all data contracts validated)'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T6.2.3 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Schema validation enforcement (all data contracts validated)

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T6.2.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.2.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.2.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.2.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.2.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T6.2.3 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T6.2.4
            title: '[REM] Verify: API rate limiting & exponential backoff (Open-Meteo, Shopify'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T6.2.4 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: API rate limiting & exponential backoff (Open-Meteo, Shopify

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T6.2.4
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.2.4
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.2.4
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.2.4
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.2.4 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T6.2.4 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T6.3.1
            title: '[REM] Verify: Performance benchmarking (MCP overhead, checkpoint size, tok'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T6.3.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Performance benchmarking (MCP overhead, checkpoint size, tok

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T6.3.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.3.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.3.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.3.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.3.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T6.3.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T6.3.2
            title: '[REM] Verify: Enhanced observability export (structured logs, metrics dash'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T6.3.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Enhanced observability export (structured logs, metrics dash

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T6.3.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.3.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.3.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.3.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.3.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T6.3.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T6.3.3
            title: '[REM] Verify: Autopilot loop end-to-end testing (full autonomous cycle val'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T6.3.3 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Autopilot loop end-to-end testing (full autonomous cycle val

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T6.3.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.3.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.3.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.3.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.3.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T6.3.3 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T6.4.0
            title: '[REM] Verify: Upgrade invariants & preflight guardrails'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T6.4.0 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Upgrade invariants & preflight guardrails

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T6.4.0
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.4.0
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.4.0
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.4.0
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.4.0 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T6.4.0 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T6.4.1
            title: '[REM] Verify: Live feature flag store with kill switch'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T6.4.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Live feature flag store with kill switch

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T6.4.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.4.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.4.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.4.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.4.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T6.4.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T6.4.2
            title: '[REM] Verify: Blue/green worker manager & front-end proxy'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T6.4.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Blue/green worker manager & front-end proxy

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T6.4.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.4.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.4.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.4.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.4.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T6.4.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T6.4.3
            title: '[REM] Verify: Worker entrypoint with DRY_RUN safeguards'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T6.4.3 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Worker entrypoint with DRY_RUN safeguards

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T6.4.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.4.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.4.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.4.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.4.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T6.4.3 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T6.4.4
            title: '[REM] Verify: Canary upgrade harness & shadow validation'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T6.4.4 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Canary upgrade harness & shadow validation

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T6.4.4
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.4.4
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.4.4
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.4.4
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.4.4 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T6.4.4 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T6.4.7
            title: '[REM] Verify: Automatic rollback monitors & kill-switch reset'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T6.4.7 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Automatic rollback monitors & kill-switch reset

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T6.4.7
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.4.7
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.4.7
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.4.7
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.4.7 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T6.4.7 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T6.4.8
            title: '[REM] Verify: Observability & resource budgets during upgrade'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T6.4.8 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Observability & resource budgets during upgrade

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T6.4.8
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T6.4.8
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T6.4.8
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T6.4.8
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T6.4.8 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T6.4.8 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T7.1.1
            title: '[REM] Verify: Complete geocoding integration (city->lat/lon, cache strateg'
            status: needs_review
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T7.1.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Complete geocoding integration (city->lat/lon, cache strateg

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T7.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T7.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T7.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T7.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T7.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T7.1.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T7.1.2
            title: '[REM] Verify: Weather feature join to model matrix (prevent future leakage'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T7.1.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Weather feature join to model matrix (prevent future leakage

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T7.1.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T7.1.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T7.1.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T7.1.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T7.1.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T7.1.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T7.1.3
            title: '[REM] Verify: Data contract schema validation (Shopify, weather, ads)'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T7.1.3 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Data contract schema validation (Shopify, weather, ads)

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T7.1.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T7.1.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T7.1.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T7.1.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T7.1.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T7.1.3 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T7.2.1
            title: '[REM] Verify: Incremental ingestion with deduplication & checkpointing'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T7.2.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Incremental ingestion with deduplication & checkpointing

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T7.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T7.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T7.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T7.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T7.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T7.2.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T7.2.2
            title: '[REM] Verify: Data quality monitoring & alerting (anomaly detection)'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T7.2.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Data quality monitoring & alerting (anomaly detection)

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T7.2.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T7.2.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T7.2.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T7.2.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T7.2.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T7.2.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T8.1.1
            title: '[REM] Verify: Lock MCP schemas to Zod shapes (SAFE: guardrail)'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T8.1.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Lock MCP schemas to Zod shapes (SAFE: guardrail)

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T8.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T8.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T8.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T8.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T8.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T8.1.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T8.1.2
            title: '[REM] Verify: Implement command allow-list in guardrails (SAFE: additive s'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T8.1.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Implement command allow-list in guardrails (SAFE: additive s

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T8.1.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T8.1.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T8.1.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T8.1.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T8.1.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T8.1.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T8.1.3
            title: '[REM] Verify: Thread correlation IDs through state transitions (SAFE: obse'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T8.1.3 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Thread correlation IDs through state transitions (SAFE: obse

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T8.1.3
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T8.1.3
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T8.1.3
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T8.1.3
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T8.1.3 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T8.1.3 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T8.2.1
            title: '[REM] Verify: Implement compact evidence-pack prompt mode (SAFE: new funct'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T8.2.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Implement compact evidence-pack prompt mode (SAFE: new funct

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T8.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T8.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T8.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T8.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T8.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T8.2.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T8.2.2
            title: '[REM] Verify: Finalize Claude↔Codex coordinator failover (SAFE: expose exi'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T8.2.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Finalize Claude↔Codex coordinator failover (SAFE: expose exi

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T8.2.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T8.2.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T8.2.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T8.2.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T8.2.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T8.2.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T9.1.1
            title: '[REM] Verify: Stable prompt headers with provider caching (SAFE: additive'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T9.1.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Stable prompt headers with provider caching (SAFE: additive 

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T9.1.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T9.1.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T9.1.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T9.1.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T9.1.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T9.1.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T9.1.2
            title: '[REM] Verify: Batch queue for non-urgent prompts (SAFE: new queueing syste'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T9.1.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Batch queue for non-urgent prompts (SAFE: new queueing syste

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T9.1.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T9.1.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T9.1.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T9.1.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T9.1.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T9.1.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T9.2.1
            title: '[REM] Verify: Strict output DSL validation (SAFE: validation layer only)'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T9.2.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Strict output DSL validation (SAFE: validation layer only)

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T9.2.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T9.2.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T9.2.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T9.2.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T9.2.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T9.2.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T9.2.2
            title: '[REM] Verify: Idempotency keys for mutating tools (SAFE: caching layer)'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T9.2.2 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: Idempotency keys for mutating tools (SAFE: caching layer)

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T9.2.2
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T9.2.2
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T9.2.2
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T9.2.2
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T9.2.2 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T9.2.2 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T9.3.1
            title: '[REM] Verify: OpenTelemetry spans for all operations (SAFE: tracing wrappe'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T9.3.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: OpenTelemetry spans for all operations (SAFE: tracing wrappe

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T9.3.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T9.3.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T9.3.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T9.3.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T9.3.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T9.3.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REM-T9.4.1
            title: '[REM] Verify: SQLite FTS5 index for code search (SAFE: new index)'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Implementation verified
              - prose: Tests verified
              - prose: Quality gate APPROVED
              - prose: Runtime verification PASSED
              - prose: Critical issues fixed
            domain: product
            description: |
              VERIFY task T9.4.1 was completed correctly with quality.

              **ORIGINAL TASK**: [REM] Verify: SQLite FTS5 index for code search (SAFE: new index)

              **VERIFICATION CHECKLIST**:

              1. **Code Exists**:
                 - Locate all files modified/created for task T9.4.1
                 - Verify code is not empty, stub, or placeholder
                 - Check for TODO/FIXME comments indicating incomplete work

              2. **Tests Exist & Pass**:
                 - Find test files for task T9.4.1
                 - Run tests: npm test or pytest (must ALL pass)
                 - Verify tests are meaningful (check behavior, not just run code)
                 - Check test coverage for new code (target: 80%+)

              3. **Build Passes**:
                 - Run build: npm run build or python build.py (0 errors)
                 - Fix any TypeScript errors, import errors, or syntax issues

              4. **Documentation Matches Code**:
                 - Read documentation for task T9.4.1
                 - Verify every claimed feature has actual implementation
                 - Check for lies: Features documented but not implemented
                 - Update docs if implementation differs from claims

              5. **Runtime Works**:
                 - Actually RUN the feature end-to-end with real data
                 - Provide runtime evidence: screenshot/logs showing it working
                 - Test error cases: Does it fail gracefully?
                 - Check resource usage: Memory leaks? CPU spikes?

              6. **Adversarial Checks**:
                 - Run adversarial_bullshit_detector on task T9.4.1
                 - Check for superficial completion (empty metrics, unused infrastructure)
                 - Verify no test manipulation (tests changed to pass without fixing bugs)
                 - Check for documentation-code mismatches

              7. **Integration**:
                 - Is task T9.4.1 actually integrated into the system?
                 - Can you demonstrate it working in context?
                 - Are there any dead code paths that never execute?

              **WHAT TO FIX IF ISSUES FOUND**:
              - Missing code → Implement it
              - Failing tests → Fix the bugs OR fix the tests (if tests are wrong)
              - Build errors → Fix compilation issues
              - Doc mismatches → Update docs to match code OR implement missing features
              - No runtime evidence → Actually run it and capture evidence
              - Superficial completion → Do the real work

              **EXIT CRITERIA**:
              - ✅ Code exists and is complete
              - ✅ All tests pass (100%)
              - ✅ Build passes (0 errors)
              - ✅ Documentation matches implementation
              - ✅ Runtime evidence provided (screenshot/logs)
              - ✅ Adversarial detector: APPROVED
              - ✅ Integration verified (feature works in context)
              - ✅ No critical issues found

              **SEVERITY**: Task T9.4.1 was marked "done" but needs verification
              **PRIORITY**: Must verify before claiming remediation complete

              **MANDATORY EVIDENCE COLLECTION** (for quality gates):

              1. **BUILD Evidence**:
                 ```bash
                 cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp
                 npm run build 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 errors"
                 - Provide output in verification report

              2. **TEST Evidence**:
                 ```bash
                 npm test 2>&1
                 ```
                 - Capture FULL output
                 - Must show "X/X passing" (all tests pass)
                 - Provide output in verification report

              3. **AUDIT Evidence**:
                 ```bash
                 npm audit 2>&1
                 ```
                 - Capture FULL output
                 - Must show "0 vulnerabilities"
                 - Provide output in verification report

              4. **RUNTIME Evidence** (at least ONE of):
                 - Screenshot of feature running in browser/CLI
                 - Log file from feature execution
                 - Artifact created by feature (JSON file, report, etc.)
                 - Demonstration video/recording

              5. **DOCUMENTATION Evidence**:
                 - List files modified/created
                 - Quote relevant documentation sections
                 - Verify docs match implementation

              **Quality Gate Checklist**:
              - [ ] Build output collected (0 errors required)
              - [ ] Test output collected (all passing required)
              - [ ] Audit output collected (0 vulnerabilities required)
              - [ ] Runtime evidence provided (artifacts/logs/screenshots)
              - [ ] Documentation verified (no mismatches)

              **NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.
              Do NOT skip evidence collection. Do NOT assume quality gates will pass without proof.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REMEDIATION-ALL-MCP-SERVER
            title: '[CRITICAL] Audit ALL MCP server code for quality issues'
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Build passes with 0 errors
              - prose: ALL tests pass (currently 865 tests)
              - prose: npm audit shows 0 vulnerabilities
              - prose: Quality gate adversarial detector APPROVED
              - prose: Runtime evidence provided for each major system
              - prose: No superficial completion detected
              - prose: No documentation-code mismatches
              - prose: Decision log shows APPROVED status
            domain: product
            description: |-
              AUDIT all MCP server implementation for quality issues.

              **SCOPE**: tools/wvo_mcp/src/ (all TypeScript code)

              **WHAT TO AUDIT**:
              1. Orchestrator: unified_orchestrator.ts, quality_gate_orchestrator.ts
              2. State Management: state_machine.ts, roadmap_tracker.ts
              3. Model Routing: model_router.ts, capacity tracking
              4. Telemetry: logging, metrics, analytics
              5. Critics: All critic implementations
              6. Resource Management: agent_pool.ts, resource_lifecycle_manager.ts

              **VERIFICATION STEPS**:
              1. Build: cd tools/wvo_mcp && npm run build (0 errors required)
              2. Tests: npm test (ALL must pass, currently 985+)
              3. Audit: npm audit (0 vulnerabilities required)
              4. Coverage: npm run test:coverage (80%+ on orchestrator code)
              5. Adversarial detector: Run on all modules
              6. Runtime: Start orchestrator, run 1 task end-to-end, collect logs
              7. Decision log: Verify state/analytics/quality_gate_decisions.jsonl has real decisions

              **EXIT CRITERIA**:
              - ✅ Build: 0 errors
              - ✅ Tests: 985/985+ passing
              - ✅ Audit: 0 vulnerabilities
              - ✅ Coverage: 80%+ on critical code
              - ✅ Runtime evidence: Screenshots/logs of orchestrator running
              - ✅ Decision log: Real decisions from autopilot (not demos)
              - ✅ No superficial completion detected
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REMEDIATION-ALL-QUALITY-GATES-DOGFOOD
            title: '[CRITICAL] Integrate multi-domain genius-level reviews (GATE 5)'
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: unknown
              - prose: Decision log shows decisions from REAL tasks (not demos)
              - prose: Post-task verification confirmed in autopilot logs
              - prose: unknown
              - prose: unknown
            domain: product
            description: |-
              INTEGRATE multi-domain genius-level reviews as GATE 5.

              Transform quality gates from checkbox thinking to expert-level domain analysis.

              **WHAT TO BUILD**:
              1. Import DomainExpertReviewer into quality_gate_orchestrator.ts
              2. Add GATE 5: Multi-domain expert review (after GATE 4)
              3. Update QualityGateDecision to include domainExpert results
              4. Extend makeConsensusDecision() to check domain expert approval
              5. Update TaskEvidence to include title + description

              **IMPLEMENTATION**:
              - quality_gate_orchestrator.ts:20-25: Add imports
              - quality_gate_orchestrator.ts:71: Update interface
              - quality_gate_orchestrator.ts:105: Instantiate reviewer
              - quality_gate_orchestrator.ts:255-258: Execute GATE 5
              - quality_gate_orchestrator.ts:397-421: Update consensus
              - adversarial_bullshit_detector.ts:26-27: Add title/description to TaskEvidence

              **VERIFICATION**:
              1. Build: cd tools/wvo_mcp && npm run build (0 errors)
              2. Tests: npm test (985+ passing)
              3. Audit: npm audit (0 vulnerabilities)
              4. Run orchestrator: See GATE 5 execute with 3+ domain experts
              5. Check logs: Domain expert reviews appear in quality_gate_decisions.jsonl
              6. Test rejection: Verify tasks rejected when experts find issues

              **EXIT CRITERIA**:
              - ✅ GATE 5 integrated and executing
              - ✅ Build: 0 errors
              - ✅ Tests: 985/985+ passing
              - ✅ Audit: 0 vulnerabilities
              - ✅ Logs show domain expert reviews (3+ experts per task)
              - ✅ Evidence document created (docs/DOMAIN_EXPERT_INTEGRATION_EVIDENCE.md)
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REMEDIATION-ALL-TESTING-INFRASTRUCTURE
            title: '[CRITICAL] Verify testing infrastructure quality'
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: npm test shows 865/865 passing (100%)
              - prose: Test quality validation passes on ALL test files
              - prose: No unconditional success mocks detected
              - prose: Integration tests exist and pass
              - prose: Edge case coverage verified
            domain: product
            description: |-
              VERIFY testing infrastructure is high-quality and tests are meaningful.

              **WHAT TO VERIFY**:
              - Test QUALITY: Do tests verify behavior or just run code?
              - Test COVERAGE: Is critical code actually tested?
              - Test ASSERTIONS: Are assertions checking real conditions?
              - Integration tests: Do they test actual integration or just mocks?
              - Runtime tests: Do critical systems have end-to-end tests?

              **SPECIFIC FILES TO AUDIT**:
              1. adversarial_bullshit_detector.test.ts: 15+ tests, all 6 detection categories
              2. quality_gate_orchestrator.test.ts: All 5 gates + consensus
              3. unified_orchestrator.test.ts: End-to-end task execution
              4. domain_expert_reviewer.test.ts: Multi-domain reviews
              5. state_machine.test.ts: State transitions, concurrent access
              6. model_router.test.ts: Model selection, capacity tracking

              **VERIFICATION STEPS**:
              1. Run tests: npm test (must ALL pass)
              2. Coverage: npm run test:coverage (generate report)
              3. Break test: Intentionally break critical code, verify tests FAIL
              4. Fix test: Fix the break, verify tests PASS
              5. Review assertions: Check every test's assertions
              6. Add missing tests: For untested critical code

              **QUALITY CHECKS**:
              - Are tests checking behavior (GOOD) or just running code (BAD)?
              - Do tests use meaningful assertions (GOOD) or just expect(x).toBeDefined() (BAD)?
              - Do integration tests actually integrate (GOOD) or mock everything (BAD)?
              - Do tests fail when code breaks (GOOD) or always pass (BAD)?

              **EXIT CRITERIA**:
              - ✅ All tests passing (985+)
              - ✅ Coverage: 80%+ on critical code
              - ✅ Tests demonstrate they catch bugs (tested by breaking code)
              - ✅ No superficial tests (all verify behavior)
              - ✅ Integration tests run orchestrator end-to-end
              - ✅ Evidence: Coverage report + test quality analysis
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REMEDIATION-T1.1.2-PREFECT-FLOW
            title: '[URGENT] Convert ingestion to actual Prefect flow'
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Code uses @flow and @task decorators from Prefect
              - prose: Flow can be registered with Prefect server
              - prose: Flow execution produces Prefect UI artifacts
              - prose: Checkpointing uses Prefect state management
              - prose: 'Runtime evidence: Prefect UI screenshot showing flow run'
              - prose: Tests validate Prefect integration
            domain: product
            description: |-
              REMEDIATION: Task T1.1.2 was marked "done" but WRONG framework used.

              **Audit Finding**: Code exists but doesn't use Prefect.
              - Found: shared/libs/ingestion/ contains code
              - Problem: No @flow or @task decorators found
              - Problem: Not using Prefect framework despite task requirement

              **Required Work**:
              1. Convert ingestion pipeline to use Prefect decorators
              2. Define flow with @flow decorator
              3. Define tasks with @task decorator
              4. Integrate with Prefect state/checkpoint system
              5. Test flow registration and execution
              6. Document Prefect-specific features used

              **Verification Requirements**:
              - grep -r "@flow|@task" shared/libs/ingestion/ returns matches
              - Can run: prefect deployment build (or equivalent)
              - Flow appears in Prefect UI (screenshot required)
              - Tests cover Prefect integration

              **Severity**: HIGH - Wrong technology implementation
              **Priority**: HIGH - Must use correct framework
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REMEDIATION-T2.2.1-GAM-BASELINE
            title: '[URGENT] Implement missing Weather-aware GAM baseline'
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Training script apps/modeling/train_weather_gam.py exists and runs
              - prose: Script produces model artifacts in expected location
              - prose: Documentation in WEATHER_PROOF_OF_CONCEPT.md references actual implementation
              - prose: 'Runtime evidence: screenshot of training run with metrics'
              - prose: Tests exist and pass for GAM training pipeline
            domain: product
            description: |-
              REMEDIATION: Task T2.2.1 was marked "done" but implementation is missing.

              **Audit Finding**: Documentation exists but NO code found.
              - Expected: apps/modeling/train_weather_gam.py or apps/modeling/weather_gam.py
              - Found: Nothing
              - Documentation references features that don't exist

              **Required Work**:
              1. Implement weather-aware GAM baseline training script
              2. Integrate with existing feature pipeline
              3. Produce model artifacts matching documentation claims
              4. Add tests covering training, prediction, and validation
              5. Run end-to-end and provide runtime evidence

              **Verification Requirements**:
              - Build passes
              - Tests pass
              - Script actually runs: python apps/modeling/train_weather_gam.py --dry-run
              - Produces expected outputs
              - Documentation matches implementation

              **Severity**: HIGH - Claimed feature completely missing
              **Priority**: CRITICAL - Must fix before claiming task complete
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: REMEDIATION-T6.3.1-PERF-BENCHMARKING
            title: '[URGENT] Fix empty performance benchmarking system'
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: state/analytics/orchestration_metrics.json contains >0 decision entries
              - prose: Performance benchmarks exist in docs with REAL data
              - prose: MCP overhead measured and documented
              - prose: Checkpoint size limits validated
              - prose: Token usage tracked over time
              - prose: 'Runtime evidence: metrics collection in action'
            domain: product
            description: |-
              REMEDIATION: Task T6.3.1 was marked "done" but system is EMPTY.

              **Audit Finding**: Infrastructure exists but unused (0 metrics recorded).
              - Found: state/analytics/orchestration_metrics.json (empty: 0 decisions)
              - Found: docs/MODEL_PERFORMANCE_THRESHOLDS.md (generic thresholds, no real data)
              - Problem: System built but never actually used

              **Required Work**:
              1. Actually USE the performance monitoring system
              2. Collect real performance data from autopilot runs
              3. Measure MCP overhead vs direct calls
              4. Measure checkpoint sizes over time
              5. Track token efficiency metrics
              6. Update docs with ACTUAL measured data

              **Verification Requirements**:
              - Run autopilot for at least 10 iterations
              - Verify metrics file grows (check file size before/after)
              - Extract sample metrics: jq '.decisions | length' state/analytics/orchestration_metrics.json
              - Document shows real numbers from real runs

              **Severity**: MEDIUM - Feature exists but superficially completed
              **Priority**: HIGH - Must demonstrate system actually works
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: TASK-RESEARCH-AD-AUTOMATION
            title: Research Meta/Google ads automation constraints
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: doc:docs/api/ads_capability_matrix.md
              - prose: doc:docs/security/ads_automation_sop.md
              - prose: artifact:state/artifacts/research/ads_api_compliance.json
            domain: product
            description: >-
              Consolidate API capabilities, credential flows, and compliance requirements so E5 automation tasks can
              launch with allocator and security critics satisfied.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: TASK-RESEARCH-CONSENSUS-BENCHMARKS
            title: Research staffing heuristics for consensus engine rollout
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:state/analytics/consensus_workload.json
              - prose: doc:docs/research/consensus_staffing_playbook.md
              - prose: artifact:state/analytics/orchestration_metrics.json
            domain: product
            description: >-
              Collect decision workload traces, benchmark quorum cost, and codify staffing heuristics so T3.3.x tasks
              can wire consensus + telemetry with real evidence.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: TASK-RESEARCH-DATA-GUARDRAILS
            title: Research ingestion data-quality guardrails
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: doc:docs/research/data_quality_guardrails.md
              - prose: artifact:state/analytics/data_quality_baselines.json
              - prose: artifact:state/artifacts/research/geocoding_coverage_report.json
            domain: product
            description: >-
              Define geocoding coverage thresholds, schema validation rules, and incremental dedupe checks so E7 and
              E12/E13 work inherit trusted data.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: TASK-RESEARCH-DEMO
            title: 'Research: evaluate cache warming pattern'
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: Investigate academic cache warming strategies and summarize findings for orchestration upgrades.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: TASK-RESEARCH-EXPERIENCE-VALIDATION
            title: Research Experiments/Reports validation with core personas
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: doc:docs/research/experiments_reports_validation.md
              - prose: artifact:state/artifacts/research/experiments_sessions
              - prose: doc:docs/UX_CRITIQUE.md
            domain: product
            description: >-
              Run moderated sessions across Sarah, Leo, and Priya personas, produce evidence-backed acceptance metrics,
              and update UX briefs so T3.4.x implementation unblocks without rework.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: TASK-RESEARCH-SWEEP
            title: Research backlog sweep
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: Identify high-impact roadmap areas lacking research coverage and propose follow-up research tasks.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: TEST-1
            title: Run build and verify no errors
            status: blocked
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: Execute npm run build in tools/wvo_mcp and verify 0 TypeScript errors
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: TEST-2
            title: Run npm audit and verify no vulnerabilities
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: Execute npm audit and verify 0 vulnerabilities found
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: TEST-3
            title: Review CLAUDE.md for completeness
            status: done
            dependencies:
              depends_on: []
            exit_criteria: []
            domain: product
            description: Read CLAUDE.md and verify all verification loop documentation is present
            complexity_score: 5
            effort_hours: 2
            required_tools: []
    description: |
      CRITICAL PRIORITY: Comprehensive quality audit of ALL work completed before quality gates.

      Assumption: ALL "done" tasks have quality issues until proven otherwise.

      This epic contains remediation tasks for every major system.
  - id: E1
    title: Epic 1 — Ingest & Weather Foundations
    status: done
    domain: product
    milestones:
      - id: M1.1
        title: Connector scaffolding
        status: done
        tasks:
          - id: T1.1.1
            title: Design Open-Meteo + Shopify connectors and data contracts
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:build
              - prose: critic:tests
              - prose: doc:docs/INGESTION.md
            domain: product
            description: Interactive scenario flows with API endpoints for scenario snapshots and storybook coverage
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T1.1.2
            title: Implement ingestion Prefect flow with checkpointing
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:data_quality
              - prose: critic:org_pm
              - prose: artifact:experiments/ingest/dq_report.json
            domain: product
            description: Map + chart overlays with export service (PPT/CSV)
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M1.2
        title: Weather harmonisation
        status: done
        tasks:
          - id: T1.2.1
            title: Blend historical + forecast weather, enforce timezone alignment
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:forecast_stitch
              - prose: doc:docs/weather/blending.md
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T1.2.2
            title: Add leakage guardrails to feature builder
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:leakage
              - prose: critic:tests
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
    description: Stand up weather + marketing ingestion, harmonise geo/time, and validate data quality.
  - id: E10
    title: PHASE-6-COST — Usage-Based Optimisations
    status: done
    domain: mcp
    milestones:
      - id: M10.1
        title: Usage telemetry & guardrails
        status: done
        tasks:
          - id: T10.1.1
            title: Cost telemetry and budget alerts
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Provider cost telemetry recorded in state/telemetry/operations.jsonl
              - prose: Budget thresholds configurable per environment
              - prose: Alert surfaced via state/context.md and orchestration logs
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
  - id: E11
    title: Resource-Aware Intelligence & Personalisation
    status: done
    domain: product
    milestones:
      - id: M11.1
        title: Capability Detection
        status: done
        tasks:
          - id: T11.1.1
            title: Implement hardware probe & profile persistence
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:build
              - prose: doc:docs/ROADMAP.md
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T11.1.2
            title: Adaptive scheduling for heavy tasks
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:tests
              - prose: artifact:state/device_profiles.json
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M11.2
        title: Falcon Design System & Award-ready UX
        status: done
        tasks:
          - id: T11.2.1
            title: Design system elevation (motion, typography, theming)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:design_system
              - prose: doc:docs/WEB_DESIGN_SYSTEM.md
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T11.2.2
            title: Award-level experience audit & remediation
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:exec_review
              - prose: artifact:docs/UX_CRITIQUE.md
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T11.2.3
            title: Extend calm/aero theme tokens to Automations and Experiments surfaces
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:apps/web/styles/themes/calm.ts
              - prose: artifact:apps/web/styles/themes/aero.ts
              - prose: critic:design_system
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T11.2.4
            title: Refactor landing/marketing gradients into reusable tokens
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:apps/web/styles/tokens/gradients.md
              - prose: critic:design_system
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T11.2.5
            title: Centralize retry button styles in shared component once App Router lands
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:apps/web/components/buttons/RetryButton.tsx
              - prose: unknown
              - prose: critic:design_system
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T11.2.6
            title: Formalize shared panel mixin (border + shadow) to reduce overrides
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:apps/web/styles/mixins/panel.css
              - prose: critic:design_system
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
    description: Auto-detect hardware, adapt workloads, and guarantee great performance on constrained machines.
  - id: E12
    title: Epic 12 — Weather Model Production Validation
    status: blocked
    domain: product
    milestones:
      - id: M12.0
        title: Synthetic Multi-Tenant Dataset Generation
        status: blocked
        tasks:
          - id: T12.0.1
            title: Generate synthetic multi-tenant dataset with weather-sensitive products
            status: blocked
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:storage/seeds/synthetic/*.parquet
              - prose: artifact:state/analytics/synthetic_tenant_profiles.json
              - prose: metric:extreme_tenant_correlation >= 0.80
              - prose: metric:high_tenant_correlation >= 0.65
              - prose: metric:medium_tenant_correlation >= 0.35
              - prose: metric:none_tenant_correlation < 0.15
              - prose: metric:data_completeness = 1.0
              - prose: metric:no_missing_dates = true
              - prose: critic:data_quality
              - prose: critic:modeling_reality_v2
            domain: product
            description: >-
              Create 4 simulated tenants with Shopify products, Meta/Google ads spend, Klaviyo data, and weather-driven
              demand patterns
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T12.0.2
            title: Validate synthetic data quality and weather correlation
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:state/analytics/synthetic_data_validation.json
              - prose: artifact:docs/DATA_GENERATION.md
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:test_mape < 0.20
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:causal
              - prose: critic:data_quality
            domain: product
            description: Confirm data volume, coverage, completeness; measure weather elasticity for each tenant
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T12.0.3
            title: Document synthetic tenant characteristics
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:docs/SYNTHETIC_TENANTS.md
              - prose: artifact:state/analytics/tenant_weather_profiles.json
              - prose: metric:extreme_tenant_correlation >= 0.80
              - prose: metric:high_tenant_correlation >= 0.65
              - prose: metric:medium_tenant_correlation >= 0.35
              - prose: metric:none_tenant_correlation < 0.15
              - prose: metric:data_completeness = 1.0
              - prose: metric:no_missing_dates = true
              - prose: critic:data_quality
              - prose: critic:modeling_reality_v2
            domain: product
            description: Create data dictionary with tenant profiles, weather sensitivity, expected model behaviors
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M12.1
        title: Weather ingestion + feature QA
        status: pending
        tasks:
          - id: T12.1.1
            title: Run smoke-context and weather ingestion regression suite nightly
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:state/telemetry/weather_ingestion.json
              - prose: artifact:experiments/t12/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:data_quality
              - prose: critic:causal
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T12.1.2
            title: Validate feature store joins against historical weather baselines
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/weather/feature_backfill_report.md
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:test_mape < 0.20
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:causal
              - prose: critic:data_quality
              - prose: critic:forecast_stitch
              - prose: critic:tests
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M12.2
        title: Weather model capability sign-off
        status: pending
        tasks:
          - id: T12.2.1
            title: Backtest weather-aware model vs control across top tenants
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/weather/model_backtest_summary.md
              - prose: artifact:experiments/t12/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:causal
              - prose: critic:allocator
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T12.2.2
            title: Publish weather capability runbook and monitoring dashboards
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:apps/web/pages/ops/weather-capabilities.tsx
              - prose: artifact:experiments/t12/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:org_pm
              - prose: critic:causal
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M12.3
        title: Weather-Aware MMM Model Training
        status: pending
        tasks:
          - id: T12.3.1
            title: Train weather-aware MMM on validated 90-day tenant data
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/mcp/mmm_weather_model.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:test_mape < 0.20
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:causal
              - prose: critic:data_quality
            domain: product
            description: Train multi-channel MMM using validated 90-day data with weather features integrated
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T12.3.2
            title: Implement weather sensitivity elasticity estimation
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:docs/models/weather_elasticity_analysis.md
              - prose: artifact:experiments/t12/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:causal
            domain: product
            description: Quantify how demand elasticity varies by weather (temperature sensitivity, rain impact, seasonal patterns)
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T12.3.3
            title: Ship production MMM inference service with real-time weather scoring
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:apps/api/routes/models/weather_mixin.py
              - prose: artifact:apps/worker/models/mmm_weather_inference.py
              - prose: artifact:experiments/t12/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:tests
              - prose: critic:causal
            domain: product
            description: Deploy MMM with weather features as production inference service
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M12.Demo
        title: Executive Demo & Stakeholder Sign-Off
        status: pending
        tasks:
          - id: T12.Demo.1
            title: Build interactive demo UI showing weather impact on ROAS
            status: pending
            dependencies:
              depends_on:
                - T12.UX.1
                - T12.UX.2
                - T12.UX.3
                - T12.UX.4
            exit_criteria:
              - prose: artifact:apps/web/src/pages/demo-weather-analysis.tsx
              - prose: artifact:state/artifacts/screenshots/demo_ui_iteration/
              - prose: artifact:experiments/t12/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:design_system
            domain: product
            description: >-
              Create web UI that lets stakeholders toggle weather on/off and see impact on predicted revenue and ROAS
              for each synthetic tenant. Interactive proof that weather matters for demand forecasting. DESIGN
              EXCELLENCE REQUIRED - Follow M12.UXExcellence standards (Playwright iteration, world-class inspiration,
              surprise & delight, zero AI aesthetic). Use screenshot_session for visual QA. Validate with design_system
              critic.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T12.Demo.2
            title: Record demo video and create stakeholder brief
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:docs/WEATHER_DEMO_BRIEF.md
              - prose: artifact:state/artifacts/stakeholder/weather_demo_script.md
              - prose: artifact:experiments/t12/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
            domain: product
            description: >-
              Record 5-min demo video showing weather-aware model in action. Create 1-page brief for executives
              explaining business impact (revenue upside, forecast accuracy improvement, ROAS optimization potential).
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M12.PoC
        title: Proof of Concept & Model Testing
        status: pending
        tasks:
          - id: T12.PoC.1
            title: Train weather-aware model on synthetic tenant data
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/mcp/weather_poc_model.pkl
              - prose: artifact:experiments/mcp/weather_poc_metrics.json
              - prose: artifact:experiments/t12/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:data_quality
            domain: product
            description: >-
              Train a baseline weather-aware regression model on each synthetic tenant (high/extreme/medium/none
              sensitivity) to validate the weather correlation detection works across different product types and
              sensitivity profiles.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T12.PoC.2
            title: Validate PoC model predictions on hold-out data
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/mcp/weather_poc_validation.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:test_mape < 0.20
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:causal
              - prose: critic:data_quality
            domain: product
            description: >-
              Test PoC model on final 30 days of each synthetic tenant. Verify that: (1) High/Extreme tenants show
              strong weather effects, (2) None tenant shows no weather effect, (3) Model R² > 0.6 on validation set
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T12.PoC.3
            title: Create PoC demo results and proof brief
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:docs/WEATHER_PROOF_OF_CONCEPT.md
              - prose: artifact:experiments/mcp/poc_demo_charts.json
              - prose: artifact:experiments/t12/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
            domain: product
            description: >-
              Create executive brief demonstrating that weather-aware modeling works: show before/after model
              performance, weather elasticity coefficients, and prediction examples. Use this to get stakeholder buy-in
              before full MMM training.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M12.UXExcellence
        title: Design Excellence Infrastructure (Lower Priority - After Data Gen)
        status: pending
        tasks:
          - id: T12.UX.1
            title: Setup Playwright screenshot workflow for design iteration
            status: pending
            dependencies:
              depends_on:
                - T12.0.1
            exit_criteria:
              - prose: artifact:state/screenshot_config.yaml
              - prose: artifact:docs/DESIGN_WORKFLOW.md
              - prose: artifact:experiments/t12/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:tests
            domain: product
            description: >-
              Configure screenshot_session, screenshot_capture_multiple tools for visual design iteration. Document
              workflow for Build→Screenshot→Critique→Refine loop. PRIORITY NOTE - Execute only after T12.0.1 (data
              generation) completes.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T12.UX.2
            title: Create design research process and inspiration library
            status: pending
            dependencies:
              depends_on:
                - T12.0.1
            exit_criteria:
              - prose: artifact:docs/DESIGN_INSPIRATION.md
              - prose: artifact:state/design_references.json
              - prose: artifact:experiments/t12/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
            domain: product
            description: >-
              Document process for researching award-winning UIs (Awwwards, FWA, Stripe, Linear, Observable). Create
              reference library of world-class design patterns to inform WeatherVane UI decisions. Extract principles
              from Dieter Rams, Müller-Brockmann, Vignelli, Paul Rand.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T12.UX.3
            title: Establish surprise & delight checklist and validation criteria
            status: pending
            dependencies:
              depends_on:
                - T12.0.1
            exit_criteria:
              - prose: artifact:docs/SURPRISE_DELIGHT_CHECKLIST.md
              - prose: artifact:tools/wvo_mcp/src/critics/ux_delight_scorer.ts
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:test_mape < 0.20
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:causal
              - prose: critic:data_quality
            domain: product
            description: >-
              Create checklist for trivial delights (micro-interactions, loading states, empty states, witty copy) and
              non-trivial delights (anticipatory UX, intelligent recovery, time-saving magic). Validation - Would user
              screenshot and share? Would Don Norman/Kathy Sierra/Julie Zhuo approve?
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T12.UX.4
            title: Configure design_system critic for zero-AI-aesthetic enforcement
            status: pending
            dependencies:
              depends_on:
                - T12.UX.1
                - T12.UX.2
                - T12.UX.3
            exit_criteria:
              - prose: artifact:tools/wvo_mcp/src/critics/design_system_enhanced.ts
              - prose: artifact:experiments/t12/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:design_system
            domain: product
            description: >-
              Enhance design_system critic to detect generic gradients, stock layouts, template components. Enforce
              heritage design principles. Integrate with screenshot workflow to compare against world-class references.
              Block merge if AI aesthetic detected.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
    description: >-
      Prioritise end-to-end weather ingestion QA, model backtests, and operational readiness so WeatherVane shiproom can
      demo weather insights with confidence.
  - id: E13
    title: Epic 13 — Weather-Aware Modeling Reality
    status: pending
    domain: product
    milestones:
      - id: M13.1
        title: Data Backbone Verified
        status: pending
        tasks:
          - id: T13.1.1
            title: Validate 90-day tenant data coverage across sales, spend, and weather
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/features/weather_join_validation.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:test_mape < 0.20
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:causal
              - prose: critic:data_quality
              - prose: critic:data_quality passes with weather join metrics captured
            domain: product
            description: >-
              Ensure the Shopify/ads/weather ingestion flows actually populate product_daily with geocoded spend and 90+
              days of history so MMM inputs are real, not theoretical.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T13.1.2
            title: Autopilot guardrail for ingestion + weather drift
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/t13/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:weather_coverage autop-run nightly with failure escalation to Atlas
              - prose: critic:data_quality
              - prose: critic:causal
            domain: product
            description: >-
              Bake data completeness checks into Autopilot so future weather-awareness regressions trigger automated
              investigations.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T13.1.3
            title: Implement product taxonomy auto-classification with weather affinity
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/t13/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:data_quality
              - prose: critic:causal
            domain: product
            description: >
              Auto-classify products from Shopify/Meta/Google using LLM (Claude/GPT-4) to tag products with weather
              affinity and category hierarchy. This enables product-level modeling (not just brand-level) and cold-start
              for new brands.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T13.1.4
            title: Data quality validation framework (verify data fitness for ML)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:test_mape < 0.20
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:causal
              - prose: critic:data_quality
            domain: product
            description: >
              Implement data quality checks to verify data is ready for ML training. Prevents training models on
              insufficient/corrupted data.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M13.2
        title: MMM Upgrade & Backtests
        status: pending
        tasks:
          - id: T13.2.1
            title: Replace heuristic MMM with LightweightMMM adstock+saturation fit
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/t13/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:model_fit passes with synthetic recovery tests
              - prose: critic:causal
            domain: product
            description: >-
              Integrate the existing LightweightMMM wrapper so allocations use Bayesian adstock/saturation estimates
              instead of covariance heuristics.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T13.2.2
            title: Build MMM backtesting + regression suite
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/t13/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:causal
            domain: product
            description: Establish out-of-sample evaluation so MMM recommendations are validated continuously.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T13.2.3
            title: Replace heuristic allocator with constraint-aware optimizer
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/t13/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:causal
            domain: product
            description: >
              Replace heuristic allocation rules (±10% budget adjustments) with proper constrained optimization. Use
              cvxpy or OR-Tools to maximize ROAS subject to budget, inventory, and platform constraints.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M13.3
        title: Causal & Geography Alignment
        status: pending
        tasks:
          - id: T13.3.1
            title: Swap uplift propensity scoring with DID/synthetic control for weather shocks
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: metric:extreme_tenant_correlation >= 0.80
              - prose: metric:high_tenant_correlation >= 0.65
              - prose: metric:medium_tenant_correlation >= 0.35
              - prose: metric:none_tenant_correlation < 0.15
              - prose: metric:data_completeness = 1.0
              - prose: metric:no_missing_dates = true
              - prose: critic:data_quality
              - prose: critic:modeling_reality_v2
            domain: product
            description: >-
              Adopt causal estimators appropriate for non-manipulable treatments so weather impact claims are
              statistically defensible.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T13.3.2
            title: Implement DMA-first geographic aggregation with hierarchical fallback
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/t13/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:causal
            domain: product
            description: >-
              Resolve the open question on geographic granularity by codifying DMA-first modeling with automatic
              fallback.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M13.4
        title: Autopilot Meta-Critique Loop
        status: pending
        tasks:
          - id: T13.4.1
            title: Add modeling reality critic to Autopilot
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/t13/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
            domain: product
            description: >-
              Teach Autopilot to generate the sort of gap analysis we just performed so future discrepancies surface
              automatically.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T13.4.2
            title: Meta-evaluation playbook for modeling roadmap
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/t13/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
            domain: product
            description: >-
              Provide a repeatable process—documentation, scheduling, and telemetry—for leadership to review modeling
              execution against strategy.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M13.5
        title: Weather-Aware Allocation Model Deployment
        status: pending
        tasks:
          - id: T13.5.1
            title: Train weather-aware allocation model on top of MMM baseline
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/allocation/weather_aware_model.json
              - prose: artifact:experiments/t13/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:allocator
              - prose: critic:causal
            domain: product
            description: Build allocation optimization model that incorporates weather-driven demand elasticity from MMM training
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T13.5.2
            title: Implement weather-responsive budget allocation constraints
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/allocation/constraint_validation.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:tests
              - prose: critic:causal
            domain: product
            description: >-
              Add constraints that adjust budget allocation based on weather forecasts (e.g., reduce spend on low-demand
              weather days)
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T13.5.3
            title: Deploy weather-aware allocator to production
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:apps/api/routes/allocate.py updated with weather model
              - prose: artifact:apps/worker/allocation_service.py deployed
              - prose: artifact:experiments/t13/validation_report.json
              - prose: metric:out_of_sample_r2 > 0.50
              - prose: metric:weather_elasticity_sign_correct = true
              - prose: metric:beats_naive_baseline_mape > 1.10
              - prose: metric:beats_seasonal_baseline_mape > 1.10
              - prose: metric:no_overfitting_detected = true
              - prose: critic:modeling_reality_v2
              - prose: critic:academic_rigor
              - prose: critic:tests
              - prose: critic:allocator
            domain: product
            description: Ship weather-aware allocation as the primary recommendation engine
            complexity_score: 5
            effort_hours: 2
            required_tools: []
    description: >-
      Close the execution gap between sophisticated modeling plans and the current codebase, while embedding Autopilot
      self-critique so these regressions cannot hide in the future.
  - id: E13.1
    title: Research and design for Epic 13 — Weather-Aware Modeling Reality
    status: pending
    domain: product
    milestones: []
    description: 'Research phase: Understand requirements and design approach for Epic 13 — Weather-Aware Modeling Reality'
  - id: E13.2
    title: Implement Epic 13 — Weather-Aware Modeling Reality
    status: pending
    domain: product
    milestones: []
    description: 'Implementation phase: Execute the plan for Epic 13 — Weather-Aware Modeling Reality'
  - id: E13.3
    title: Validate and test Epic 13 — Weather-Aware Modeling Reality
    status: pending
    domain: product
    milestones: []
    description: 'Validation phase: Test and verify Epic 13 — Weather-Aware Modeling Reality'
  - id: E2
    title: Epic 2 — Features & Modeling Baseline
    status: done
    domain: product
    milestones:
      - id: M2.1
        title: Feature pipeline
        status: done
        tasks:
          - id: T2.1.1
            title: Build lag/rolling feature generators with deterministic seeds
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:build
              - prose: critic:tests
              - prose: critic:data_quality
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M2.2
        title: Baseline modeling
        status: done
        tasks:
          - id: T2.2.1
            title: Train weather-aware GAM baseline and document methodology
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:causal
              - prose: critic:academic_rigor
              - prose: doc:docs/models/baseline.md
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
    description: Ship lagged features, baseline models, and evaluation harness.
  - id: E3
    title: Epic 3 — Allocation & UX
    status: done
    domain: product
    milestones:
      - id: M3.1
        title: Allocator guardrails
        status: done
        tasks:
          - id: T3.1.1
            title: Implement budget allocator stress tests and regret bounds
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:allocator
              - prose: critic:cost_perf
              - prose: artifact:experiments/policy/regret.json
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M3.2
        title: Dashboard + UX review
        status: done
        tasks:
          - id: T3.2.1
            title: Run design system critic and ensure accessibility coverage
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:design_system
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T3.2.2
            title: Elevate dashboard storytelling & UX
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:design_system
              - prose: critic:exec_review
              - prose: doc:docs/UX_CRITIQUE.md
              - prose: artifact:docs/product/UX_CRITIQUE.md
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M3.3
        title: Autonomous Orchestration Blueprints
        status: done
        tasks:
          - id: T3.3.1
            title: Draft multi-agent charter & delegation mesh (AutoGen/Swarm patterns)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:docs/orchestration/multi_agent_charter.md
              - prose: critic:manager_self_check
              - prose: critic:org_pm
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T3.3.2
            title: Implement hierarchical consensus & escalation engine
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:integration_fury
              - prose: critic:manager_self_check
              - prose: doc:docs/orchestration/consensus_engine.md
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T3.3.3
            title: Build closed-loop simulation harness for autonomous teams
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:experiments/orchestration/simulation_report.md
              - prose: critic:tests
              - prose: critic:health_check
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T3.3.4
            title: Instrument dynamic staffing telemetry & learning pipeline
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:prompt_budget
              - prose: critic:exec_review
              - prose: artifact:state/analytics/orchestration_metrics.json
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M3.4
        title: Experience Implementation
        status: done
        tasks:
          - id: T3.4.1
            title: Implement Plan overview page with weather-driven insights
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:apps/web/pages/plan.tsx
              - prose: unknown
              - prose: critic:design_system
              - prose: unknown
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T3.4.2
            title: Build WeatherOps dashboard with allocator + weather KPIs
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:apps/web/pages/dashboard.tsx
              - prose: unknown
              - prose: critic:design_system
              - prose: unknown
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T3.4.3
            title: Ship Experiments hub UI for uplift & incrementality reviews
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:apps/web/pages/experiments.tsx
              - prose: unknown
              - prose: critic:design_system
              - prose: unknown
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T3.4.4
            title: Deliver storytelling Reports view with weather + spend narratives
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: artifact:apps/web/pages/reports.tsx
              - prose: unknown
              - prose: critic:design_system
              - prose: unknown
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T3.4.5
            title: Conduct design_system + UX acceptance review across implemented pages
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:design_system
              - prose: doc:docs/product/acceptance_report.md
              - prose: artifact:state/critics/designsystem.json
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T3.4.6
            title: Rewrite WeatherOps dashboard around plain-language decisions
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: unknown
              - prose: unknown
              - prose: critic:design_system
              - prose: critic:exec_review
              - prose: unknown
            domain: product
            description: |-
              Requirements:
                - Show operators exactly what WeatherVane changed or recommends changing, with clear “what/why/next” messaging.
                - Remove jargon (“guardrail”, “triage”) in favour of user-facing language (e.g., “Overspend alert”, “Weather action”).
                - Keep first view scannable: one hero recommendation, secondary cards, optional detail drill-down.
              Standards:
                - Copy: conversational, action-oriented, no internal terminology.
                - UX: minimalist layout, responsive to desktop/tablet, accessible (WCAG AA).
                - Engineering: Playwright smoke must pass; analytics instrumentation preserved; Vitest coverage for helpers.
              Implementation Plan:
                - Draft/record design brief in docs/UX_CRITIQUE.md.
                - Refactor hero + summary components around new copy and layout.
                - Update analytics helpers/tests, run Vitest + Playwright.
                - Capture iteration in state/context.md with screenshots and critic notes.
              Deliverables:
                - Updated React/CSS modules under apps/web/src/pages/dashboard.tsx and styles.
                - Revised helper libraries/tests (apps/web/src/lib/**, tests/web/**).
                - Playwright report + screenshots stored under state/artifacts/ui/weatherops.
              Integration Points:
                - API suggestion telemetry (shared/services/dashboard_analytics_ingestion.py) ensuring copy aligns with payload fields.
                - Analytics events (`trackDashboardEvent`) and downstream dashboards; coordinate with data/ML owners if field names change.
                - Worker-generated suggestion summaries (apps/worker/flows/poc_pipeline.py) to maintain consistency across channels.
              Evidence:
                - Playwright run ID + html report.
                - design_system + exec_review critic outputs (once available).
                - Context entry summarising decisions, open questions, next iteration.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T3.4.7
            title: Reimagine Automations change log as a trust-first narrative
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: unknown
              - prose: unknown
              - prose: critic:design_system
              - prose: critic:exec_review
              - prose: unknown
            domain: product
            description: |-
              Requirements:
                - Explain every autonomous change in plain language (what changed, when, why, impact).
                - Provide explicit approval/rollback affordances for humans and highlight pending reviews.
                - Surface evidence (metrics, weather context, spend forecasts) inline or a click away.
              Standards:
                - Copy: transparent, confidence-building, avoids “audit/guardrail” jargon.
                - UX: timeline or table must prioritise newest changes, support filtering; accessible controls for approvals.
                - Engineering: tests updated (Vitest, Playwright), telemetry preserved, change log data schema documented.
                - ML context (if applicable): explain model confidence/reason codes clearly.
              Implementation Plan:
                - Extend docs/UX_CRITIQUE.md with Automations brief, list user questions + acceptance metrics.
                - Redesign components/layout in apps/web/src/pages/automations.tsx; integrate evidence panels.
                - Update helpers/tests, run Vitest + Playwright, capture critics.
                - Log iterations in state/context.md with before/after screenshots and open questions.
              Deliverables:
                - Updated Automations page/components/styles.
                - Supporting helper modules/tests (automationInsights, validation, etc.).
                - Evidence artifacts (Playwright report, screenshots, context notes).
              Integration Points:
                - Automation audit APIs/events (apps/api/services/dashboard_service.py, shared schemas) so reason codes remain synchronized.
                - Worker automation execution logs (`apps/worker/flows/**`) and telemetry exports consumed by directors/Dana.
                - Notification channels or forthcoming approval workflows (e.g., Slack/email) to ensure new statuses map correctly.
              Evidence:
                - Playwright run + report stored under state/artifacts/ui/automations.
                - design_system + exec_review critic confirmation.
                - Context log summarising decisions, trade-offs, next steps.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
    description: Allocator robustness checks, dashboards, and UI polish.
  - id: E4
    title: Epic 4 — Operational Excellence
    status: done
    domain: product
    milestones:
      - id: M4.1
        title: Optimization sprint
        status: done
        tasks:
          - id: T4.1.10
            title: Cross-market saturation optimization (fairness-aware)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:allocator
              - prose: artifact:experiments/allocator/saturation_report.json
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T4.1.3
            title: Causal uplift modeling & incremental lift validation
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:causal
              - prose: artifact:experiments/causal/uplift_report.json
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T4.1.4
            title: Multi-horizon ensemble forecasting
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:forecast_stitch
              - prose: artifact:experiments/forecast/ensemble_metrics.json
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T4.1.5
            title: Non-linear allocation optimizer with constraints (ROAS, spend caps)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:allocator
              - prose: unknown
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T4.1.6
            title: High-frequency spend response modeling (intraday adjustments)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:allocator
              - prose: artifact:experiments/allocator/hf_response.json
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T4.1.7
            title: Marketing mix budget solver (multi-channel, weather-aware)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:allocator
              - prose: unknown
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T4.1.8
            title: Reinforcement-learning shadow mode (safe exploration)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:allocator
              - prose: artifact:experiments/rl/shadow_mode.json
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T4.1.9
            title: Creative-level response modeling with brand safety guardrails
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:design_system
              - prose: artifact:experiments/creative/response_scores.json
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
    description: Maintain velocity while hardening performance and delivery processes.
  - id: E5
    title: Ad Platform Execution & Automation
    status: done
    domain: product
    milestones:
      - id: M5.1
        title: Meta Ads Command Pipeline
        status: done
        tasks:
          - id: T5.1.1
            title: Implement Meta Marketing API client (creative + campaign management)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:allocator
              - prose: unknown
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T5.1.2
            title: Meta sandbox and dry-run executor with credential vaulting
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:security
              - prose: artifact:experiments/meta/sandbox_run.json
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M5.2
        title: Google Ads Execution & Budget Sync
        status: done
        tasks:
          - id: T5.2.1
            title: Google Ads API integration (campaign create/update, shared budgets)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:allocator
              - prose: unknown
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T5.2.2
            title: Budget reconciliation & spend guardrails across platforms
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:allocator
              - prose: artifact:experiments/allocator/spend_guardrails.json
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M5.3
        title: QA, Rollback & Safety Harness
        status: done
        tasks:
          - id: T5.3.1
            title: Dry-run & diff visualizer for ad pushes (pre-flight checks)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:tests
              - prose: artifact:state/ad_push_diffs.json
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T5.3.2
            title: Automated rollback + alerting when performance/regression detected
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:manager_self_check
              - prose: artifact:experiments/allocator/rollback_sim.json
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
    description: Enable WeatherVane to programmatically create, update, monitor, and rollback ads across major platforms.
  - id: E6
    title: MCP Orchestrator Production Readiness
    status: done
    domain: mcp
    milestones:
      - id: M6.1
        title: Core Infrastructure Validation
        status: done
        tasks:
          - id: T6.1.1
            title: MCP server integration tests (all 25 tools across both providers)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:tests
              - prose: artifact:tests/test_mcp_tools.py
              - prose: >-
                  Guardrail: integration suite enforces blue/green safety invariants (no unhandled throws, DRY_RUN
                  parity)
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T6.1.2
            title: Provider failover testing (token limit simulation & automatic switching)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:manager_self_check
              - prose: artifact:experiments/mcp/failover_test.json
              - prose: 'Guardrail: circuit-breaker rollback and DISABLE_NEW kill switch verified under simulated failures'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T6.1.3
            title: State persistence testing (checkpoint recovery across sessions)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:tests
              - prose: artifact:tests/test_state_persistence.py
              - prose: 'Guardrail: recovery flow preserves upgrade locks and safety state without manual intervention'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T6.1.4
            title: Quality framework validation (10 dimensions operational)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:manager_self_check
              - prose: artifact:state/quality/assessment_log.json
              - prose: 'Guardrail: quality checks confirm run-safety metrics from blue/green playbook remain green'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M6.2
        title: Security & Reliability Hardening
        status: done
        tasks:
          - id: T6.2.1
            title: Credentials security audit (auth.json, API keys, token rotation)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:security
              - prose: doc:docs/SECURITY_AUDIT.md
              - prose: 'Guardrail: audit verifies secrets handling inside blue/green upgrade flow and DRY_RUN constraints'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T6.2.2
            title: Error recovery testing (graceful degradation, retry logic)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:tests
              - prose: artifact:experiments/mcp/error_recovery.json
              - prose: 'Guardrail: automated rollback path exercised with observation window + DISABLE_NEW reset'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T6.2.3
            title: Schema validation enforcement (all data contracts validated)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:data_quality
              - prose: artifact:shared/contracts/*.schema.json
              - prose: 'Guardrail: dual-write / expand-cutover-contract workflow logged for 100% safe migrations'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T6.2.4
            title: API rate limiting & exponential backoff (Open-Meteo, Shopify, Ads APIs)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:allocator
              - prose: unknown
              - prose: >-
                  Guardrail: rate-limit handling respects worker timeouts and prevents cascading failures during
                  upgrades
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M6.3
        title: Observability & Performance
        status: done
        tasks:
          - id: T6.3.1
            title: Performance benchmarking (MCP overhead, checkpoint size, token usage)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:cost_perf
              - prose: artifact:experiments/mcp/performance_benchmarks.json
              - prose: 'Guardrail: benchmarks include worker swap scenarios and confirm resource limits (timeouts, RSS) hold'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T6.3.2
            title: Enhanced observability export (structured logs, metrics dashboards)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:manager_self_check
              - prose: artifact:state/telemetry/metrics_summary.json
              - prose: 'Guardrail: telemetry captures Step 0–15 safety signals with alerting on breaches'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T6.3.3
            title: Autopilot loop end-to-end testing (full autonomous cycle validation)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:manager_self_check
              - prose: artifact:experiments/mcp/autopilot_e2e.json
              - prose: 'Guardrail: autonomous loop validates automatic promotion + rollback without manual resets'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M6.4
        title: Zero-downtime self-upgrade
        status: pending
        tasks:
          - id: T6.4.0
            title: Upgrade invariants & preflight guardrails
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: state/upgrade.lock created before work and removed on exit
              - prose: Preflight script validates git status, Node/npm versions, disk ≥500MB, sandbox availability
              - prose: Four-step gate recorded in logs; any failure returns {error:"upgrade_aborted"}
            domain: mcp
            description: >-
              Define upgrade preflight: clean git, version sanity, ≥500MB disk, SQLite lock probe, and single-flight
              upgrade.lock. Gate promotion through build → unit → selfchecks → canary, aborting with
              {error:"upgrade_aborted"} on any failure.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T6.4.1
            title: Live feature flag store with kill switch
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: settings table created with defaults + DISABLE_NEW
              - prose: LiveFlags poller refreshes in-memory cache during runtime
              - prose: Integration test flips PROMPT_MODE without restart
            domain: mcp
            description: |
              Replace environment toggles with a SQLite-backed `settings` table, seed
              defaults, and hot-refresh cached flags (≤500 ms poll). Include a
              `DISABLE_NEW` global kill switch that forces legacy behaviour instantly.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T6.4.2
            title: Blue/green worker manager & front-end proxy
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: WorkerManager exposes startActive/startCanary/switchToCanary
              - prose: Front-end tool handlers call workers.getActive().call(...)
              - prose: RPC protocol enforces ready handshake, 30s timeouts, and structured {ok,error} results
              - prose: Test demonstrates zero-downtime swap between worker binaries
            domain: mcp
            description: |
              Keep the MCP front-end process stable while managing active and canary
              worker children over IPC. Ensure requests route through a proxy that can
              atomically switch to the validated canary without disconnecting clients.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T6.4.3
            title: Worker entrypoint with DRY_RUN safeguards
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Route function covers health/plan/dispatch/runTool/verify/report.mo
              - prose: SQLite opened via file:state/state.db?mode=ro when DRY_RUN=1
              - prose: applyPatch/mutate operations rejected while DRY_RUN=1
              - prose: tests/test_worker_dry_run.py captures read-only guarantees
            domain: mcp
            description: |
              Implement a dedicated worker entry that routes RPCs, enforces DRY_RUN=1
              by opening the state DB read-only, refuses mutating calls, and confirms
              legacy behaviour when DRY_RUN=0.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T6.4.4
            title: Canary upgrade harness & shadow validation
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: scripts/mcp_safe_upgrade.sh orchestrates worktree build + tests
              - prose: Shadow checks compare active vs canary outputs in logs
              - prose: Promotion flow documents gate order and staged routing (DRY → live) with metrics snapshots
              - prose: experiments/mcp/upgrade/<ts>/report.json recorded for each run
            domain: mcp
            description: |
              Automate the upgrade flow: create a separate git worktree, build/test new
              code, spawn a DRY_RUN canary, run shadow health/plan/report checks, then
              promote only if outputs match expectations.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T6.4.5
            title: Feature flag gating for compact prompts & sandbox pool
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: >-
                  PROMPT_MODE, SANDBOX_MODE, SCHEDULER_MODE, SELECTIVE_TESTS, DANGER_GATES, MO_ENGINE read from
                  LiveFlags
              - prose: Regression fixtures cover legacy vs new mode per feature
              - prose: docs/MCP_ORCHESTRATOR.md updated with flag toggle order
            domain: mcp
            description: |
              Gate compact prompt headers, sandbox pooling, scheduler WSJF mode,
              selective tests, danger gates, and MO engine behind live flags so they
              only activate after successful canary validation.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T6.4.6
            title: Runtime tool registration & admin flag controls
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Tool handlers return 'disabled' until corresponding flag enabled
              - prose: settings.update, upgrade.applyPatch, route.switch commands exposed with structured errors
              - prose: Operator guide added under docs/MCP_AUTOMATION.md#live-flags
            domain: mcp
            description: |
              Ensure tool surfaces remain stable while routing to v1/v2 handlers based
              on flags. Provide an MCP admin tool or CLI to update settings atomically
              without restarts.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T6.4.7
            title: Automatic rollback monitors & kill-switch reset
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Heartbeat every 2s with 3-strike circuit breaker routes back to standby
              - prose: Error budget (5%/2min) and SLO monitors trigger automatic rollback
              - prose: DISABLE_NEW flag automatically flipped during rollback
              - prose: docs/MCP_ORCHESTRATOR.md includes rollback playbook
            domain: mcp
            description: |
              Add health monitoring that reverts to the previous worker and resets
              flags when error rates spike post-promotion. Document on-call rollback
              steps and ensure DISABLE_NEW restores legacy behaviour.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T6.4.8
            title: Observability & resource budgets during upgrade
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Span/log attributes include method, lane, ok/error, duration, task.id
              - prose: runTool/plan timeouts (30s/120s) & lane concurrency limits enforced
              - prose: RSS watchdog throttles batch lane when >1.5x baseline
            domain: mcp
            description: |
              Emit OTel spans (or structured JSON logs) for every worker call with
              timing, lane, task, and outcome metadata. Enforce concurrency, timeout,
              and RSS guards to prevent runaway resource usage.
            complexity_score: 5
            effort_hours: 2
            required_tools: []
    description: >-
      Validate and harden the dual-provider MCP orchestrator for autonomous operation while preserving 100% run safety.
      All milestones under this epic must enforce the blue/green upgrade guardrails defined in
      docs/MCP_ORCHESTRATOR.md#1113-tight-integration-playbook-steps-0-15.
  - id: E7
    title: Data Pipeline Hardening
    status: done
    domain: product
    milestones:
      - id: M7.1
        title: Geocoding & Weather Integration
        status: done
        tasks:
          - id: T7.1.1
            title: Complete geocoding integration (city->lat/lon, cache strategy)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:data_quality
              - prose: unknown
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T7.1.2
            title: Weather feature join to model matrix (prevent future leakage)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:leakage
              - prose: artifact:experiments/features/weather_join_validation.json
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T7.1.3
            title: Data contract schema validation (Shopify, weather, ads)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:data_quality
              - prose: unknown
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M7.2
        title: Pipeline Robustness
        status: done
        tasks:
          - id: T7.2.1
            title: Incremental ingestion with deduplication & checkpointing
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:data_quality
              - prose: unknown
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T7.2.2
            title: Data quality monitoring & alerting (anomaly detection)
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: critic:data_quality
              - prose: artifact:state/dq_monitoring.json
            domain: product
            complexity_score: 5
            effort_hours: 2
            required_tools: []
    description: Complete geocoding integration, weather feature joins, and data quality validation.
  - id: E8
    title: PHASE-4-POLISH — MCP Production Hardening
    status: done
    domain: mcp
    milestones:
      - id: M8.1
        title: MCP Compliance & Security
        status: done
        tasks:
          - id: T8.1.1
            title: 'Lock MCP schemas to Zod shapes (SAFE: guardrail)'
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: utils/schema.ts returns schema.shape with guardrail comment
              - prose: MCP entrypoints register raw shapes only
              - prose: Autopilot documentation updated to reflect guardrail
              - prose: critic:build passes
              - prose: 'Guardrail: validation confirms schema handling does not weaken blue/green safety gates'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T8.1.2
            title: 'Implement command allow-list in guardrails (SAFE: additive security)'
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: ALLOWED_COMMANDS constant defined
              - prose: isCommandAllowed() enforced before execution
              - prose: Deny-list kept as secondary check
              - prose: critic:tests passes with new test_command_allowlist.py
              - prose: critic:manager_self_check passes
              - prose: 'Guardrail: allow-list integration verified against blue/green upgrade scenarios'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T8.1.3
            title: 'Thread correlation IDs through state transitions (SAFE: observability only)'
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: All tool handlers generate correlationId
              - prose: All state transitions include correlationId
              - prose: Events in SQLite include correlation_id column populated
              - prose: critic:manager_self_check passes
              - prose: End-to-end trace visible in state/orchestrator.db
              - prose: 'Guardrail: correlation IDs trace compliance with Step 0–15 safety checks'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M8.2
        title: Context & Performance Optimization
        status: done
        tasks:
          - id: T8.2.1
            title: 'Implement compact evidence-pack prompt mode (SAFE: new function, backward compatible)'
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: formatForPromptCompact() returns JSON evidence pack
              - prose: unknown
              - prose: All coordinator calls use compact mode
              - prose: critic:build passes
              - prose: critic:manager_self_check passes
              - prose: unknown
              - prose: 'Guardrail: compact mode flip integrated with Step 15 staged flag process'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T8.2.2
            title: 'Finalize Claude↔Codex coordinator failover (SAFE: expose existing functionality)'
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: orchestrator_status tool shows coordinator type and availability
              - prose: Telemetry includes coordinator field in execution logs
              - prose: Documentation updated in IMPLEMENTATION_STATUS.md
              - prose: critic:manager_self_check passes
              - prose: Failover behavior visible and logged
              - prose: 'Guardrail: failover reporting feeds SLO/error budget monitors for auto rollback'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
    description: >-
      Critical production readiness tasks for MCP orchestrator. Complete before WeatherVane v1 launch while maintaining
      the Step 0–15 run-safety guardrails (docs/MCP_ORCHESTRATOR.md#1113-tight-integration-playbook-steps-0-15).
  - id: E9
    title: PHASE-5-OPTIMIZATION — Performance & Observability
    status: pending
    domain: mcp
    milestones:
      - id: M9.1
        title: Cost Optimization & Caching
        status: done
        tasks:
          - id: T9.1.1
            title: 'Stable prompt headers with provider caching (SAFE: additive optimization)'
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: standardPromptHeader() returns deterministic header
              - prose: All prompts include standard header
              - prose: Header enables provider caching (verified with API logs)
              - prose: critic:cost_perf shows token cache hit rate
              - prose: critic:manager_self_check passes
              - prose: 'Guardrail: caching rollout assessed via Step 0–15 safety checks before staying live'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T9.1.2
            title: 'Batch queue for non-urgent prompts (SAFE: new queueing system)'
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Priority queue with 3 lanes operational
              - prose: Semaphore limits enforced per lane
              - prose: Interactive tasks always get priority
              - prose: critic:tests passes
              - prose: critic:manager_self_check passes
              - prose: 'Guardrail: queue respects worker concurrency caps from blue/green playbook'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M9.2
        title: Reliability & Quality Improvements
        status: done
        tasks:
          - id: T9.2.1
            title: 'Strict output DSL validation (SAFE: validation layer only)'
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: validateDiff() rejects non-diff outputs
              - prose: validateJSON() rejects invalid JSON
              - prose: Retry rate reduction measured
              - prose: critic:tests passes
              - prose: 'Guardrail: validation enforced in canary shadow runs before live promotion'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T9.2.2
            title: 'Idempotency keys for mutating tools (SAFE: caching layer)'
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Idempotency cache operational
              - prose: Duplicate operations return cached results
              - prose: 1-hour TTL enforced
              - prose: critic:tests passes
              - prose: 'Guardrail: cache respects DRY_RUN mode and avoids side effects during canary runs'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M9.3
        title: Production Observability
        status: pending
        tasks:
          - id: T9.3.1
            title: 'OpenTelemetry spans for all operations (SAFE: tracing wrapper)'
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: All tool handlers instrumented
              - prose: Spans exported to tracing backend
              - prose: End-to-end traces visible
              - prose: Performance insights available
              - prose: critic:manager_self_check passes
              - prose: 'Guardrail: telemetry alerts on Step 0–15 safety breaches'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T9.3.2
            title: 'Sandbox pooling for test execution (SAFE: new executor)'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: Sandbox pool with 3 pre-warmed containers
              - prose: Test execution uses pooled sandboxes
              - prose: 10x speedup measured
              - prose: Fallback to non-pooled works
              - prose: critic:tests passes
              - prose: 'Guardrail: pool enforces DRY_RUN read-only mode during canary validation'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
      - id: M9.4
        title: Advanced Context & Search
        status: pending
        tasks:
          - id: T9.4.1
            title: 'SQLite FTS5 index for code search (SAFE: new index)'
            status: done
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: code_fts virtual table created
              - prose: Index populated on repo sync
              - prose: Search performance <50ms
              - prose: critic:tests passes
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
          - id: T9.4.2
            title: 'LSP proxy tools for symbol-aware context (SAFE: new tools)'
            status: pending
            dependencies:
              depends_on: []
            exit_criteria:
              - prose: tsserver and pyright proxies running
              - prose: lsp.definition and lsp.references tools work
              - prose: Context assembler uses LSP for code slices
              - prose: Context relevance measured and improved
              - prose: critic:tests passes
              - prose: 'Guardrail: LSP tools routed through worker proxy with Step 0–15 safety enforcement'
            domain: mcp
            complexity_score: 5
            effort_hours: 2
            required_tools: []
    description: >-
      Post-v1 performance improvements and production observability. High ROI optimizations that still honour the
      blue/green guardrail contract.
  - id: E9.1
    title: Research and design for PHASE-5-OPTIMIZATION — Performance & Observability
    status: pending
    domain: product
    milestones: []
    description: 'Research phase: Understand requirements and design approach for PHASE-5-OPTIMIZATION — Performance & Observability'
  - id: E9.2
    title: Implement PHASE-5-OPTIMIZATION — Performance & Observability
    status: pending
    domain: product
    milestones: []
    description: 'Implementation phase: Execute the plan for PHASE-5-OPTIMIZATION — Performance & Observability'
  - id: E9.3
    title: Validate and test PHASE-5-OPTIMIZATION — Performance & Observability
    status: pending
    domain: product
    milestones: []
    description: 'Validation phase: Test and verify PHASE-5-OPTIMIZATION — Performance & Observability'
last_updated: '2025-10-29T23:28:10.440Z'
updated_by: migrate_roadmap.ts
