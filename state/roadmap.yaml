epics:
- description: 'CRITICAL PRIORITY: Comprehensive quality audit of ALL work completed before quality gates.


    Assumption: ALL "done" tasks have quality issues until proven otherwise.


    This epic contains remediation tasks for every major system.

    '
  domain: product
  id: E-REMEDIATION
  milestones:
  - id: M-REM-1
    priority: critical
    status: in_progress
    tasks:
    - dependencies: []
      description: "AUDIT all MCP server implementation for quality issues.\n\n**SCOPE**: tools/wvo_mcp/src/ (all TypeScript\
        \ code)\n\n**WHAT TO AUDIT**:\n1. Orchestrator: unified_orchestrator.ts, quality_gate_orchestrator.ts\n2. State Management:\
        \ state_machine.ts, roadmap_tracker.ts\n3. Model Routing: model_router.ts, capacity tracking\n4. Telemetry: logging,\
        \ metrics, analytics\n5. Critics: All critic implementations\n6. Resource Management: agent_pool.ts, resource_lifecycle_manager.ts\n\
        \n**VERIFICATION STEPS**:\n1. Build: cd tools/wvo_mcp && npm run build (0 errors required)\n2. Tests: npm test (ALL\
        \ must pass, currently 985+)\n3. Audit: npm audit (0 vulnerabilities required)\n4. Coverage: npm run test:coverage\
        \ (80%+ on orchestrator code)\n5. Adversarial detector: Run on all modules\n6. Runtime: Start orchestrator, run 1\
        \ task end-to-end, collect logs\n7. Decision log: Verify state/analytics/quality_gate_decisions.jsonl has real decisions\n\
        \n**EXIT CRITERIA**:\n- \u2705 Build: 0 errors\n- \u2705 Tests: 985/985+ passing\n- \u2705 Audit: 0 vulnerabilities\n\
        - \u2705 Coverage: 80%+ on critical code\n- \u2705 Runtime evidence: Screenshots/logs of orchestrator running\n- \u2705\
        \ Decision log: Real decisions from autopilot (not demos)\n- \u2705 No superficial completion detected"
      domain: product
      exit_criteria:
      - Build passes with 0 errors
      - ALL tests pass (currently 865 tests)
      - npm audit shows 0 vulnerabilities
      - Quality gate adversarial detector APPROVED
      - Runtime evidence provided for each major system
      - No superficial completion detected
      - No documentation-code mismatches
      - Decision log shows APPROVED status
      id: REMEDIATION-ALL-MCP-SERVER
      priority: critical
      status: blocked
      title: '[CRITICAL] Audit ALL MCP server code for quality issues'
    - dependencies: []
      description: "VERIFY testing infrastructure is high-quality and tests are meaningful.\n\n**WHAT TO VERIFY**:\n- Test\
        \ QUALITY: Do tests verify behavior or just run code?\n- Test COVERAGE: Is critical code actually tested?\n- Test\
        \ ASSERTIONS: Are assertions checking real conditions?\n- Integration tests: Do they test actual integration or just\
        \ mocks?\n- Runtime tests: Do critical systems have end-to-end tests?\n\n**SPECIFIC FILES TO AUDIT**:\n1. adversarial_bullshit_detector.test.ts:\
        \ 15+ tests, all 6 detection categories\n2. quality_gate_orchestrator.test.ts: All 5 gates + consensus\n3. unified_orchestrator.test.ts:\
        \ End-to-end task execution\n4. domain_expert_reviewer.test.ts: Multi-domain reviews\n5. state_machine.test.ts: State\
        \ transitions, concurrent access\n6. model_router.test.ts: Model selection, capacity tracking\n\n**VERIFICATION STEPS**:\n\
        1. Run tests: npm test (must ALL pass)\n2. Coverage: npm run test:coverage (generate report)\n3. Break test: Intentionally\
        \ break critical code, verify tests FAIL\n4. Fix test: Fix the break, verify tests PASS\n5. Review assertions: Check\
        \ every test's assertions\n6. Add missing tests: For untested critical code\n\n**QUALITY CHECKS**:\n- Are tests checking\
        \ behavior (GOOD) or just running code (BAD)?\n- Do tests use meaningful assertions (GOOD) or just expect(x).toBeDefined()\
        \ (BAD)?\n- Do integration tests actually integrate (GOOD) or mock everything (BAD)?\n- Do tests fail when code breaks\
        \ (GOOD) or always pass (BAD)?\n\n**EXIT CRITERIA**:\n- \u2705 All tests passing (985+)\n- \u2705 Coverage: 80%+ on\
        \ critical code\n- \u2705 Tests demonstrate they catch bugs (tested by breaking code)\n- \u2705 No superficial tests\
        \ (all verify behavior)\n- \u2705 Integration tests run orchestrator end-to-end\n- \u2705 Evidence: Coverage report\
        \ + test quality analysis"
      domain: product
      exit_criteria:
      - npm test shows 865/865 passing (100%)
      - Test quality validation passes on ALL test files
      - No unconditional success mocks detected
      - Integration tests exist and pass
      - Edge case coverage verified
      id: REMEDIATION-ALL-TESTING-INFRASTRUCTURE
      priority: critical
      status: blocked
      title: '[CRITICAL] Verify testing infrastructure quality'
    - dependencies: []
      description: "INTEGRATE multi-domain genius-level reviews as GATE 5.\n\nTransform quality gates from checkbox thinking\
        \ to expert-level domain analysis.\n\n**WHAT TO BUILD**:\n1. Import DomainExpertReviewer into quality_gate_orchestrator.ts\n\
        2. Add GATE 5: Multi-domain expert review (after GATE 4)\n3. Update QualityGateDecision to include domainExpert results\n\
        4. Extend makeConsensusDecision() to check domain expert approval\n5. Update TaskEvidence to include title + description\n\
        \n**IMPLEMENTATION**:\n- quality_gate_orchestrator.ts:20-25: Add imports\n- quality_gate_orchestrator.ts:71: Update\
        \ interface\n- quality_gate_orchestrator.ts:105: Instantiate reviewer\n- quality_gate_orchestrator.ts:255-258: Execute\
        \ GATE 5\n- quality_gate_orchestrator.ts:397-421: Update consensus\n- adversarial_bullshit_detector.ts:26-27: Add\
        \ title/description to TaskEvidence\n\n**VERIFICATION**:\n1. Build: cd tools/wvo_mcp && npm run build (0 errors)\n\
        2. Tests: npm test (985+ passing)\n3. Audit: npm audit (0 vulnerabilities)\n4. Run orchestrator: See GATE 5 execute\
        \ with 3+ domain experts\n5. Check logs: Domain expert reviews appear in quality_gate_decisions.jsonl\n6. Test rejection:\
        \ Verify tasks rejected when experts find issues\n\n**EXIT CRITERIA**:\n- \u2705 GATE 5 integrated and executing\n\
        - \u2705 Build: 0 errors\n- \u2705 Tests: 985/985+ passing\n- \u2705 Audit: 0 vulnerabilities\n- \u2705 Logs show\
        \ domain expert reviews (3+ experts per task)\n- \u2705 Evidence document created (docs/DOMAIN_EXPERT_INTEGRATION_EVIDENCE.md)"
      domain: product
      exit_criteria:
      - Tests: 36/36 passing (unit + integration)
      - Decision log shows decisions from REAL tasks (not demos)
      - Post-task verification confirmed in autopilot logs
      - Test with bad code: gates correctly REJECT
      - Test with good code: gates correctly APPROVE
      id: REMEDIATION-ALL-QUALITY-GATES-DOGFOOD
      priority: critical
      status: blocked
      title: '[CRITICAL] Integrate multi-domain genius-level reviews (GATE 5)'
    - dependencies: []
      description: 'REMEDIATION: Task T2.2.1 was marked "done" but implementation is missing.


        **Audit Finding**: Documentation exists but NO code found.

        - Expected: apps/modeling/train_weather_gam.py or apps/modeling/weather_gam.py

        - Found: Nothing

        - Documentation references features that don''t exist


        **Required Work**:

        1. Implement weather-aware GAM baseline training script

        2. Integrate with existing feature pipeline

        3. Produce model artifacts matching documentation claims

        4. Add tests covering training, prediction, and validation

        5. Run end-to-end and provide runtime evidence


        **Verification Requirements**:

        - Build passes

        - Tests pass

        - Script actually runs: python apps/modeling/train_weather_gam.py --dry-run

        - Produces expected outputs

        - Documentation matches implementation


        **Severity**: HIGH - Claimed feature completely missing

        **Priority**: CRITICAL - Must fix before claiming task complete'
      domain: product
      exit_criteria:
      - Training script apps/modeling/train_weather_gam.py exists and runs
      - Script produces model artifacts in expected location
      - Documentation in WEATHER_PROOF_OF_CONCEPT.md references actual implementation
      - 'Runtime evidence: screenshot of training run with metrics'
      - Tests exist and pass for GAM training pipeline
      id: REMEDIATION-T2.2.1-GAM-BASELINE
      priority: high
      status: blocked
      title: '[URGENT] Implement missing Weather-aware GAM baseline'
    - dependencies: []
      description: 'REMEDIATION: Task T6.3.1 was marked "done" but system is EMPTY.


        **Audit Finding**: Infrastructure exists but unused (0 metrics recorded).

        - Found: state/analytics/orchestration_metrics.json (empty: 0 decisions)

        - Found: docs/MODEL_PERFORMANCE_THRESHOLDS.md (generic thresholds, no real data)

        - Problem: System built but never actually used


        **Required Work**:

        1. Actually USE the performance monitoring system

        2. Collect real performance data from autopilot runs

        3. Measure MCP overhead vs direct calls

        4. Measure checkpoint sizes over time

        5. Track token efficiency metrics

        6. Update docs with ACTUAL measured data


        **Verification Requirements**:

        - Run autopilot for at least 10 iterations

        - Verify metrics file grows (check file size before/after)

        - Extract sample metrics: jq ''.decisions | length'' state/analytics/orchestration_metrics.json

        - Document shows real numbers from real runs


        **Severity**: MEDIUM - Feature exists but superficially completed

        **Priority**: HIGH - Must demonstrate system actually works'
      domain: product
      exit_criteria:
      - state/analytics/orchestration_metrics.json contains >0 decision entries
      - Performance benchmarks exist in docs with REAL data
      - MCP overhead measured and documented
      - Checkpoint size limits validated
      - Token usage tracked over time
      - 'Runtime evidence: metrics collection in action'
      id: REMEDIATION-T6.3.1-PERF-BENCHMARKING
      priority: high
      status: blocked
      title: '[URGENT] Fix empty performance benchmarking system'
    - dependencies: []
      description: 'REMEDIATION: Task T1.1.2 was marked "done" but WRONG framework used.


        **Audit Finding**: Code exists but doesn''t use Prefect.

        - Found: shared/libs/ingestion/ contains code

        - Problem: No @flow or @task decorators found

        - Problem: Not using Prefect framework despite task requirement


        **Required Work**:

        1. Convert ingestion pipeline to use Prefect decorators

        2. Define flow with @flow decorator

        3. Define tasks with @task decorator

        4. Integrate with Prefect state/checkpoint system

        5. Test flow registration and execution

        6. Document Prefect-specific features used


        **Verification Requirements**:

        - grep -r "@flow|@task" shared/libs/ingestion/ returns matches

        - Can run: prefect deployment build (or equivalent)

        - Flow appears in Prefect UI (screenshot required)

        - Tests cover Prefect integration


        **Severity**: HIGH - Wrong technology implementation

        **Priority**: HIGH - Must use correct framework'
      domain: product
      exit_criteria:
      - Code uses @flow and @task decorators from Prefect
      - Flow can be registered with Prefect server
      - Flow execution produces Prefect UI artifacts
      - Checkpointing uses Prefect state management
      - 'Runtime evidence: Prefect UI screenshot showing flow run'
      - Tests validate Prefect integration
      id: REMEDIATION-T1.1.2-PREFECT-FLOW
      priority: high
      status: blocked
      title: '[URGENT] Convert ingestion to actual Prefect flow'
    - dependencies: []
      description: "Critic academicrigor is underperforming and needs immediate remediation.\n\nIdentity: Academic Rigor (academic_rigor,\
        \ authority advisory)\nMission: Safeguard academic_rigor discipline.\nSignature powers: Reports on findings when configuration\
        \ is missing.\n\nCritic academicrigor failed 10 of the last 11 runs with 0 consecutive failures.\n\nObservation window:\
        \ 11 runs\n\nConsecutive failures: 0\n\nFailures: 10 | Successes: 1\n\nAssigned to: Autopilot\n\nExpectations:\n-\
        \ Diagnose root causes for the critic's repeated failures.\n- Patch critic configuration, training data, or underlying\
        \ automation as needed.\n- Document findings in state/context.md and roadmap notes.\n- Close this task once the critic\
        \ passes reliably.\n\nLatest output snippet:\n{\n  \"epic\": \"E12\",\n  \"summary\": {\n    \"done\": 15,\n    \"\
        in_progress\": 0,\n    \"pending\": 0,\n    \"blocked\": 0\n  },\n  \"critical_issues\": [],\n  \"high_issues\": [],\n\
        \  \"warnings\": [],\n  \"doc_path\": \"/Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/docs/CRITIQUE_E12_20251022.md\"\
        \n}"
      domain: product
      exit_criteria: []
      id: CRIT-PERF-ACADEMICRIGOR-b0301a
      status: done
      title: '[Critic:academicrigor] Restore performance'
    - dependencies: []
      description: "Critic academicrigor is underperforming and needs immediate remediation.\n\nIdentity: Academic Rigor (academic_rigor,\
        \ authority advisory)\nMission: Safeguard academic_rigor discipline.\nSignature powers: Reports on findings when configuration\
        \ is missing.\n\nCritic academicrigor failed 8 of the last 10 runs with 0 consecutive failures.\n\nObservation window:\
        \ 10 runs\n\nConsecutive failures: 0\n\nFailures: 8 | Successes: 2\n\nAssigned to: Autopilot\n\nExpectations:\n- Diagnose\
        \ root causes for the critic's repeated failures.\n- Patch critic configuration, training data, or underlying automation\
        \ as needed.\n- Document findings in state/context.md and roadmap notes.\n- Close this task once the critic passes\
        \ reliably.\n\nLatest output snippet:\n{\n  \"epic\": \"E12\",\n  \"summary\": {\n    \"done\": 15,\n    \"in_progress\"\
        : 0,\n    \"pending\": 0,\n    \"blocked\": 0\n  },\n  \"critical_issues\": [],\n  \"high_issues\": [],\n  \"warnings\"\
        : [],\n  \"doc_path\": \"/Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/docs/CRITIQUE_E12_20251022.md\"\
        \n}"
      domain: product
      exit_criteria: []
      id: CRIT-PERF-ACADEMICRIGOR-f89932
      status: done
      title: '[Critic:academicrigor] Restore performance'
    - dependencies: []
      description: 'Critic allocator is underperforming and needs immediate remediation.


        Identity: Allocator Sentinel (operations, authority advisory)

        Mission: Ensure planner allocation and task routing stay optimal.

        Signature powers: Diagnoses misrouted tasks and capacity imbalances.; Suggests rebalancing across agents and squads.

        Autonomy guidance: Auto-adjust planner weights when safe; escalate persistent misallocations to Autopilot.


        Critic allocator failed 8 of the last 10 runs with 0 consecutive failures.


        Observation window: 10 runs


        Consecutive failures: 0


        Failures: 8 | Successes: 2


        Assigned to: Autopilot


        Expectations:

        - Diagnose root causes for the critic''s repeated failures.

        - Patch critic configuration, training data, or underlying automation as needed.

        - Document findings in state/context.md and roadmap notes.

        - Close this task once the critic passes reliably.


        Latest output snippet:

        ============================= test session starts ==============================

        platform darwin -- Python 3.10.12, pytest-8.4.2, pluggy-1.6.0

        rootdir: /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane

        configfile: pyproject.toml

        plugins: anyio-3.7.1, asyncio-1.2.0

        asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function

        collected 5 items


        tests/test_allocator_routes.py ..                                        [ 40%]

        tests/test_creative_route.py .                                           [ 60%]

        tests/apps/model/test_cre...'
      domain: product
      exit_criteria: []
      id: CRIT-PERF-ALLOCATOR-bc8604
      status: done
      title: '[Critic:allocator] Restore performance'
    - dependencies: []
      description: "Critic build is underperforming and needs immediate remediation.\n\nIdentity: Build Sentinel (engineering,\
        \ authority blocking)\nMission: Guarantee that core build processes remain reproducible and optimized across environments.\n\
        Signature powers: Diagnoses build pipeline regressions and unstable toolchains.; Flags missing build artifacts or\
        \ misconfigured dependencies before release.\nAutonomy guidance: Attempt automated patching of build scripts when\
        \ safe; escalate infrastructure escalations beyond local fixes.\n\nCritic build failed 5 of the last 6 runs with 0\
        \ consecutive failures.\n\nObservation window: 6 runs\n\nConsecutive failures: 0\n\nFailures: 5 | Successes: 1\n\n\
        Assigned to: Autopilot\n\nExpectations:\n- Diagnose root causes for the critic's repeated failures.\n- Patch critic\
        \ configuration, training data, or underlying automation as needed.\n- Document findings in state/context.md and roadmap\
        \ notes.\n- Close this task once the critic passes reliably.\n\nLatest output snippet:\nwarning: The top-level linter\
        \ settings are deprecated in favour of their counterparts in the `lint` section. Please update the following options\
        \ in `pyproject.toml`:\n  - 'select' -> 'lint.select'"
      domain: product
      exit_criteria: []
      id: CRIT-PERF-BUILD-958e1f
      status: done
      title: '[Critic:build] Restore performance'
    - dependencies: []
      description: 'Critic causal is underperforming and needs immediate remediation.


        Identity: Causal Strategist (ml, authority critical)

        Mission: Guarantee counterfactual validity and causal modeling rigor.

        Signature powers: Checks identifying assumptions, invariances, and instrumentation.; Constructs mitigation plans for
        confounding risks.

        Autonomy guidance: Partner with Research Orchestrator on complex interventions; log learnings for future experiments.


        No successful runs recorded in the last 12 observations; 12 consecutive failures detected.


        Observation window: 12 runs


        Consecutive failures: 12


        Failures: 12 | Successes: 0


        Assigned to: Director Dana


        Expectations:

        - Diagnose root causes for the critic''s repeated failures.

        - Patch critic configuration, training data, or underlying automation as needed.

        - Document findings in state/context.md and roadmap notes.

        - Close this task once the critic passes reliably.


        Latest output snippet:

        skipped due to capability profile'
      domain: product
      exit_criteria: []
      id: CRIT-PERF-CAUSAL-070d3d
      status: done
      title: '[Critic:causal] Restore performance'
    - dependencies: []
      description: "Critic causal is underperforming and needs immediate remediation.\n\nIdentity: Causal Strategist (ml,\
        \ authority critical)\nMission: Guarantee counterfactual validity and causal modeling rigor.\nSignature powers: Checks\
        \ identifying assumptions, invariances, and instrumentation.; Constructs mitigation plans for confounding risks.\n\
        Autonomy guidance: Partner with Research Orchestrator on complex interventions; log learnings for future experiments.\n\
        \nCritic causal failed 8 of the last 10 runs with 0 consecutive failures.\n\nObservation window: 10 runs\n\nConsecutive\
        \ failures: 0\n\nFailures: 8 | Successes: 2\n\nAssigned to: Autopilot\n\nExpectations:\n- Diagnose root causes for\
        \ the critic's repeated failures.\n- Patch critic configuration, training data, or underlying automation as needed.\n\
        - Document findings in state/context.md and roadmap notes.\n- Close this task once the critic passes reliably.\n\n\
        Latest output snippet:\n{\n  \"status\": \"passed\",\n  \"level\": \"medium\",\n  \"findings\": [\n    {\n      \"\
        severity\": \"INFO\",\n      \"message\": \"Weather shock estimator present.\",\n      \"details\": null\n    },\n\
        \    {\n      \"severity\": \"INFO\",\n      \"message\": \"Weather shock tests passed.\",\n      \"details\": \"\
        \  /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/shared/libs/causal/weather_shock.py:194: DeprecationWarning:\
        \ `pl.count()` is deprecated. Please use `pl.len()` instead.\\n  (Deprecated in version 0.20.5)\\n    pl.count().alias(\\\
        \"pre_count\\\"),\\n\\ntests/shared/libs/causal/test_weather_shock.py::test_weather_shock..."
      domain: product
      exit_criteria: []
      id: CRIT-PERF-CAUSAL-e7682e
      status: done
      title: '[Critic:causal] Restore performance'
    - dependencies: []
      description: 'Critic designsystem is underperforming and needs immediate remediation.


        Identity: Design System (design_system, authority advisory)

        Mission: Safeguard design_system discipline.

        Signature powers: Reports on findings when configuration is missing.


        No successful runs recorded in the last 9 observations; 5 consecutive failures detected.


        Observation window: 9 runs


        Consecutive failures: 5


        Failures: 6 | Successes: 3


        Assigned to: Director Dana


        Expectations:

        - Diagnose root causes for the critic''s repeated failures.

        - Patch critic configuration, training data, or underlying automation as needed.

        - Document findings in state/context.md and roadmap notes.

        - Close this task once the critic passes reliably.


        Latest output snippet:

        ./src/pages/dashboard.tsx

        968:14  Error: Parsing error: '')'' expected.


        info  - Need to disable some ESLint rules? Learn more here: https://nextjs.org/docs/basic-features/eslint#disabling-rules'
      domain: product
      exit_criteria: []
      id: CRIT-PERF-DESIGNSYSTEM-1a886a
      status: done
      title: '[Critic:designsystem] Restore performance'
    - dependencies: []
      description: 'Critic execreview is underperforming and needs immediate remediation.


        Identity: Exec Review (exec_review, authority advisory)

        Mission: Safeguard exec_review discipline.

        Signature powers: Reports on findings when configuration is missing.


        No successful runs recorded in the last 12 observations; 12 consecutive failures detected.


        Observation window: 12 runs


        Consecutive failures: 12


        Failures: 12 | Successes: 0


        Assigned to: Director Dana


        Expectations:

        - Diagnose root causes for the critic''s repeated failures.

        - Patch critic configuration, training data, or underlying automation as needed.

        - Document findings in state/context.md and roadmap notes.

        - Close this task once the critic passes reliably.


        Latest output snippet:

        skipped due to capability profile'
      domain: product
      exit_criteria: []
      id: CRIT-PERF-EXECREVIEW-ef2384
      status: done
      title: '[Critic:execreview] Restore performance'
    - dependencies: []
      description: 'Multiple critics are underperforming and require coordinated intervention.


        3 critics require director-level intervention after repeated failures.


        Affected critics: execreview, integrationfury, managerselfcheck


        Critics evaluated in run: 5


        Reports captured: 3


        Assigned to: Director Dana


        Expectations:

        - Review individual remediation tasks and look for systemic issues.

        - Adjust critic configurations, training loops, or staffing mixes.

        - Provide a coordination brief in state/context.md.

        - Close this systemic task once individual critics are back on track.'
      domain: product
      exit_criteria: []
      id: CRIT-PERF-GLOBAL-9882b7
      status: done
      title: '[Critics] Systemic performance remediation'
    - dependencies: []
      description: "Critic healthcheck is underperforming and needs immediate remediation.\n\nIdentity: Health Check (health_check,\
        \ authority advisory)\nMission: Safeguard health_check discipline.\nSignature powers: Reports on findings when configuration\
        \ is missing.\n\nCritic healthcheck failed 4 of the last 5 runs with 0 consecutive failures.\n\nObservation window:\
        \ 5 runs\n\nConsecutive failures: 0\n\nFailures: 4 | Successes: 1\n\nAssigned to: Autopilot\n\nExpectations:\n- Diagnose\
        \ root causes for the critic's repeated failures.\n- Patch critic configuration, training data, or underlying automation\
        \ as needed.\n- Document findings in state/context.md and roadmap notes.\n- Close this task once the critic passes\
        \ reliably.\n\nLatest output snippet:\nwarning: The top-level linter settings are deprecated in favour of their counterparts\
        \ in the `lint` section. Please update the following options in `pyproject.toml`:\n  - 'select' -> 'lint.select'"
      domain: product
      exit_criteria: []
      id: CRIT-PERF-HEALTHCHECK-0e6b67
      status: done
      title: '[Critic:healthcheck] Restore performance'
    - dependencies: []
      description: 'Critic integrationfury is underperforming and needs immediate remediation.


        Identity: Integration Fury (integration_fury, authority advisory)

        Mission: Safeguard integration_fury discipline.

        Signature powers: Reports on findings when configuration is missing.


        No successful runs recorded in the last 6 observations; 6 consecutive failures detected.


        Observation window: 6 runs


        Consecutive failures: 6


        Failures: 6 | Successes: 0


        Assigned to: Director Dana


        Expectations:

        - Diagnose root causes for the critic''s repeated failures.

        - Patch critic configuration, training data, or underlying automation as needed.

        - Document findings in state/context.md and roadmap notes.

        - Close this task once the critic passes reliably.


        Latest output snippet:

        ============================= test session starts ==============================

        platform darwin -- Python 3.10.12, pytest-8.4.2, pluggy-1.6.0

        rootdir: /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane

        configfile: pyproject.toml

        plugins: anyio-4.11.0, asyncio-1.2.0

        asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function

        collected 242 items


        tests/api/onboarding/test_progress.py ....                               [  1%]

        tests/api/test_ad_push_routes.py ....                                    [  3%]

        tests/api/test_dashboa...'
      domain: product
      exit_criteria: []
      id: CRIT-PERF-INTEGRATIONFURY-9401af
      status: done
      title: '[Critic:integrationfury] Restore performance'
    - dependencies: []
      description: 'Critic managerselfcheck is underperforming and needs immediate remediation.


        Identity: Manager Self Check (manager_self_check, authority advisory)

        Mission: Safeguard manager_self_check discipline.

        Signature powers: Reports on findings when configuration is missing.


        No successful runs recorded in the last 6 observations; 6 consecutive failures detected.


        Observation window: 6 runs


        Consecutive failures: 6


        Failures: 6 | Successes: 0


        Assigned to: Director Dana


        Expectations:

        - Diagnose root causes for the critic''s repeated failures.

        - Patch critic configuration, training data, or underlying automation as needed.

        - Document findings in state/context.md and roadmap notes.

        - Close this task once the critic passes reliably.


        Latest output snippet:

        Rollback simulation stale (simulated_at=2025-10-15T21:05:00+00:00); rerun executor to refresh promotion gate.'
      domain: product
      exit_criteria: []
      id: CRIT-PERF-MANAGERSELFCHECK-61ab48
      status: done
      title: '[Critic:managerselfcheck] Restore performance'
    - dependencies: []
      description: 'Critic orgpm is underperforming and needs immediate remediation.


        Identity: Org Pm (org_pm, authority advisory)

        Mission: Safeguard org_pm discipline.

        Signature powers: Reports on findings when configuration is missing.


        Critic orgpm failed 5 of the last 6 runs with 0 consecutive failures.


        Observation window: 6 runs


        Consecutive failures: 0


        Failures: 5 | Successes: 1


        Assigned to: Autopilot


        Expectations:

        - Diagnose root causes for the critic''s repeated failures.

        - Patch critic configuration, training data, or underlying automation as needed.

        - Document findings in state/context.md and roadmap notes.

        - Close this task once the critic passes reliably.


        Latest output snippet:

        Org PM charter/state checks passed.'
      domain: product
      exit_criteria: []
      id: CRIT-PERF-ORGPM-be2140
      status: done
      title: '[Critic:orgpm] Restore performance'
    - dependencies: []
      description: 'Critic promptbudget is underperforming and needs immediate remediation.


        Identity: Prompt Budget (prompt_budget, authority advisory)

        Mission: Safeguard prompt_budget discipline.

        Signature powers: Reports on findings when configuration is missing.


        Critic promptbudget failed 5 of the last 6 runs with 0 consecutive failures.


        Observation window: 6 runs


        Consecutive failures: 0


        Failures: 5 | Successes: 1


        Assigned to: Autopilot


        Expectations:

        - Diagnose root causes for the critic''s repeated failures.

        - Patch critic configuration, training data, or underlying automation as needed.

        - Document findings in state/context.md and roadmap notes.

        - Close this task once the critic passes reliably.


        Latest output snippet:

        {"level":"warning","message":"Code search index rebuild failed","timestamp":"2025-10-16T20:39:47.650Z","error":"The
        database connection is not open"}'
      domain: product
      exit_criteria: []
      id: CRIT-PERF-PROMPTBUDGET-2c30f3
      status: done
      title: '[Critic:promptbudget] Restore performance'
    - dependencies: []
      description: 'Critic security is underperforming and needs immediate remediation.


        Identity: Security Sentinel (security, authority critical)

        Mission: Guard secrets, policies, and attack surfaces throughout the stack.

        Signature powers: Identifies credential leaks, insecure defaults, and policy gaps.; Cross-references security playbooks
        to recommend mitigations.

        Autonomy guidance: Demand sign-off for high-risk findings; coordinate with Director Dana and Security Stewards.


        No successful runs recorded in the last 9 observations; 6 consecutive failures detected.


        Observation window: 9 runs


        Consecutive failures: 6


        Failures: 6 | Successes: 3


        Assigned to: Director Dana


        Expectations:

        - Diagnose root causes for the critic''s repeated failures.

        - Patch critic configuration, training data, or underlying automation as needed.

        - Document findings in state/context.md and roadmap notes.

        - Close this task once the critic passes reliably.


        Latest output snippet:

        skipped due to capability profile'
      domain: product
      exit_criteria: []
      id: CRIT-PERF-SECURITY-645edd
      status: done
      title: '[Critic:security] Restore performance'
    - dependencies: []
      description: "Critic tests is underperforming and needs immediate remediation.\n\nIdentity: Regression Hunter (quality,\
        \ authority blocking)\nMission: Keep the test suites healthy and ensure deterministic results across flows.\nSignature\
        \ powers: Surfaces flaky suites and failing assertions with reproduction notes.; Synthesizes minimal repro commands\
        \ for Autopilot triage.\nAutonomy guidance: Rerun targeted suites automatically; lean on Autopilot only when new failures\
        \ persist.\n\nCritic tests failed 5 of the last 6 runs with 0 consecutive failures.\n\nObservation window: 6 runs\n\
        \nConsecutive failures: 0\n\nFailures: 5 | Successes: 1\n\nAssigned to: Autopilot\n\nExpectations:\n- Diagnose root\
        \ causes for the critic's repeated failures.\n- Patch critic configuration, training data, or underlying automation\
        \ as needed.\n- Document findings in state/context.md and roadmap notes.\n- Close this task once the critic passes\
        \ reliably.\n\nLatest output snippet:\n\e[33mThe CJS build of Vite's Node API is deprecated. See https://vite.dev/guide/troubleshooting.html#vite-cjs-node-api-deprecated\
        \ for more details.\e[39m\n{\"level\":\"info\",\"message\":\"Subscription limit tracker initialized\",\"timestamp\"\
        :\"2025-10-16T21:37:34.835Z\",\"providers\":[]}\n{\"level\":\"info\",\"message\":\"Provider registered for usage tracking\"\
        ,\"timestamp\":\"2025-10-16T21:37:34.837Z\",\"provider\":\"claude\",\"account\":\"test-account\",\"tier\":\"pro\"\
        }\n{\"level\":\"info\",\"message\":\"Subscription limit tracker stopped\",\"timestamp\":\"2025-10-16T21:37:34.840Z\"\
        }\n{\"level\":\"info\",\"message\":\"Subscription limit tracker ..."
      domain: product
      exit_criteria: []
      id: CRIT-PERF-TESTS-426598
      status: done
      title: '[Critic:tests] Restore performance'
    - dependencies: []
      description: "Critic tests is underperforming and needs immediate remediation.\n\nIdentity: Regression Hunter (quality,\
        \ authority blocking)\nMission: Keep the test suites healthy and ensure deterministic results across flows.\nSignature\
        \ powers: Surfaces flaky suites and failing assertions with reproduction notes.; Synthesizes minimal repro commands\
        \ for Autopilot triage.\nAutonomy guidance: Rerun targeted suites automatically; lean on Autopilot only when new failures\
        \ persist.\n\nCritic tests failed 5 of the last 6 runs with 4 consecutive failures.\n\nObservation window: 6 runs\n\
        \nConsecutive failures: 4\n\nFailures: 5 | Successes: 1\n\nAssigned to: Autopilot\n\nExpectations:\n- Diagnose root\
        \ causes for the critic's repeated failures.\n- Patch critic configuration, training data, or underlying automation\
        \ as needed.\n- Document findings in state/context.md and roadmap notes.\n- Close this task once the critic passes\
        \ reliably.\n\nLatest output snippet:\n\e[33mThe CJS build of Vite's Node API is deprecated. See https://vite.dev/guide/troubleshooting.html#vite-cjs-node-api-deprecated\
        \ for more details.\e[39m\n\u23AF\u23AF\u23AF\u23AF\u23AF\u23AF\u23AF Failed Tests 6 \u23AF\u23AF\u23AF\u23AF\u23AF\
        \u23AF\u23AF\n\n FAIL  ../../tests/web/design_system_acceptance.spec.ts > Design system acceptance \u2013 Stories\
        \ page > renders skip navigation, main landmark, and story metadata tokens\nAssertionError: expected \"error\" to\
        \ not be called at all, but actually been called 5 times\n\nReceived: \n\n  1st error call:\n\n    Array [\n     \
        \ \"Warning: An update to %s inside a test was not wrapped in act(...).\n    \n    When testing, code that cau..."
      domain: product
      exit_criteria: []
      id: CRIT-PERF-TESTS-c3fce7
      status: done
      title: '[Critic:tests] Restore performance'
    - dependencies: []
      description: "BLOCKING \u2013 must complete before other work (disable YAML writes, real usage/cost, correlation IDs,\
        \ coordinator failover)"
      domain: product
      exit_criteria: []
      id: PHASE-1-HARDENING
      status: done
      title: 'Phase 1: MCP Hardening'
    - dependencies: []
      description: "BLOCKING \u2013 must complete before other work (compact context assembler, snapshot selfcheck)"
      domain: product
      exit_criteria: []
      id: PHASE-2-COMPACT
      status: done
      title: 'Phase 2: Compact Prompts + Selfchecks'
    - dependencies: []
      description: "BLOCKING \u2013 must complete before other work (batch queue, stable headers, token heuristics)"
      domain: product
      exit_criteria: []
      id: PHASE-3-BATCH
      status: done
      title: 'Phase 3: Batch Queue & Prompt Headers'
    - dependencies: []
      description: Consolidate API capabilities, credential flows, and compliance requirements so E5 automation tasks can
        launch with allocator and security critics satisfied.
      domain: product
      exit_criteria:
      - doc:docs/api/ads_capability_matrix.md
      - doc:docs/security/ads_automation_sop.md
      - artifact:state/artifacts/research/ads_api_compliance.json
      id: TASK-RESEARCH-AD-AUTOMATION
      status: done
      title: Research Meta/Google ads automation constraints
    - dependencies: []
      description: Collect decision workload traces, benchmark quorum cost, and codify staffing heuristics so T3.3.x tasks
        can wire consensus + telemetry with real evidence.
      domain: product
      exit_criteria:
      - artifact:state/analytics/consensus_workload.json
      - doc:docs/research/consensus_staffing_playbook.md
      - artifact:state/analytics/orchestration_metrics.json
      id: TASK-RESEARCH-CONSENSUS-BENCHMARKS
      status: done
      title: Research staffing heuristics for consensus engine rollout
    - dependencies: []
      description: Define geocoding coverage thresholds, schema validation rules, and incremental dedupe checks so E7 and
        E12/E13 work inherit trusted data.
      domain: product
      exit_criteria:
      - doc:docs/research/data_quality_guardrails.md
      - artifact:state/analytics/data_quality_baselines.json
      - artifact:state/artifacts/research/geocoding_coverage_report.json
      id: TASK-RESEARCH-DATA-GUARDRAILS
      status: done
      title: Research ingestion data-quality guardrails
    - dependencies: []
      description: Investigate academic cache warming strategies and summarize findings for orchestration upgrades.
      domain: product
      exit_criteria: []
      id: TASK-RESEARCH-DEMO
      status: done
      title: 'Research: evaluate cache warming pattern'
    - dependencies: []
      description: Run moderated sessions across Sarah, Leo, and Priya personas, produce evidence-backed acceptance metrics,
        and update UX briefs so T3.4.x implementation unblocks without rework.
      domain: product
      exit_criteria:
      - doc:docs/research/experiments_reports_validation.md
      - artifact:state/artifacts/research/experiments_sessions
      - doc:docs/UX_CRITIQUE.md
      id: TASK-RESEARCH-EXPERIENCE-VALIDATION
      status: done
      title: Research Experiments/Reports validation with core personas
    - dependencies: []
      description: Identify high-impact roadmap areas lacking research coverage and propose follow-up research tasks.
      domain: product
      exit_criteria: []
      id: TASK-RESEARCH-SWEEP
      status: done
      title: Research backlog sweep
    - dependencies: []
      description: Execute npm run build in tools/wvo_mcp and verify 0 TypeScript errors
      domain: product
      exit_criteria: []
      id: TEST-1
      status: blocked
      title: Run build and verify no errors
    - dependencies: []
      description: Execute npm audit and verify 0 vulnerabilities found
      domain: product
      exit_criteria: []
      id: TEST-2
      status: done
      title: Run npm audit and verify no vulnerabilities
    - dependencies: []
      description: Read CLAUDE.md and verify all verification loop documentation is present
      domain: product
      exit_criteria: []
      id: TEST-3
      status: done
      title: Review CLAUDE.md for completeness
    - dependencies: []
      description: "VERIFY task T0.1.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Implement\
        \ geo holdout plumbing\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T0.1.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T0.1.1\n   - Run tests: npm test or\
        \ pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n\
        \   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read\
        \ documentation for task T0.1.1\n   - Verify every claimed feature has actual implementation\n   - Check for lies:\
        \ Features documented but not implemented\n   - Update docs if implementation differs from claims\n\n5. **Runtime\
        \ Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T0.1.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T0.1.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T0.1.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T0.1.1
      milestone_id: M-REM-1
      original_epic: E-PHASE0
      original_task_id: T0.1.1
      priority: high
      status: blocked
      title: '[REM] Verify: Implement geo holdout plumbing'
    - dependencies: []
      description: "VERIFY task T0.1.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Build lift\
        \ & confidence UI surfaces\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T0.1.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T0.1.2\n   - Run tests: npm test or\
        \ pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n\
        \   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read\
        \ documentation for task T0.1.2\n   - Verify every claimed feature has actual implementation\n   - Check for lies:\
        \ Features documented but not implemented\n   - Update docs if implementation differs from claims\n\n5. **Runtime\
        \ Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T0.1.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T0.1.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T0.1.2 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T0.1.2
      milestone_id: M-REM-1
      original_epic: E-PHASE0
      original_task_id: T0.1.2
      priority: high
      status: needs_improvement
      title: '[REM] Verify: Build lift & confidence UI surfaces'
    - dependencies: []
      description: "VERIFY task T0.1.3 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Generate\
        \ forecast calibration report\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T0.1.3\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T0.1.3\n   - Run tests: npm test or\
        \ pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n\
        \   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read\
        \ documentation for task T0.1.3\n   - Verify every claimed feature has actual implementation\n   - Check for lies:\
        \ Features documented but not implemented\n   - Update docs if implementation differs from claims\n\n5. **Runtime\
        \ Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T0.1.3\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T0.1.3 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T0.1.3 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T0.1.3
      milestone_id: M-REM-1
      original_epic: E-PHASE0
      original_task_id: T0.1.3
      priority: high
      status: blocked
      title: '[REM] Verify: Generate forecast calibration report'
    - dependencies: []
      description: "VERIFY task T1.1.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Design Open-Meteo\
        \ + Shopify connectors and data contracts\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T1.1.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T1.1.1\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T1.1.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T1.1.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T1.1.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T1.1.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T1.1.1
      milestone_id: M-REM-1
      original_epic: E1
      original_task_id: T1.1.1
      priority: high
      status: needs_improvement
      title: '[REM] Verify: Design Open-Meteo + Shopify connectors and data contracts'
    - dependencies: []
      description: "VERIFY task T1.1.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Implement\
        \ ingestion Prefect flow with checkpointing\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T1.1.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T1.1.2\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T1.1.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T1.1.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T1.1.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T1.1.2 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T1.1.2
      milestone_id: M-REM-1
      original_epic: E1
      original_task_id: T1.1.2
      priority: high
      status: blocked
      title: '[REM] Verify: Implement ingestion Prefect flow with checkpointing'
    - dependencies: []
      description: "VERIFY task T1.2.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Blend historical\
        \ + forecast weather, enforce timezone alignm\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T1.2.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T1.2.1\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T1.2.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T1.2.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T1.2.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T1.2.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T1.2.1
      milestone_id: M-REM-1
      original_epic: E1
      original_task_id: T1.2.1
      priority: high
      status: needs_improvement
      title: '[REM] Verify: Blend historical + forecast weather, enforce timezone alignm'
    - dependencies: []
      description: "VERIFY task T1.2.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Add leakage\
        \ guardrails to feature builder\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T1.2.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T1.2.2\n   - Run tests: npm test or\
        \ pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n\
        \   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read\
        \ documentation for task T1.2.2\n   - Verify every claimed feature has actual implementation\n   - Check for lies:\
        \ Features documented but not implemented\n   - Update docs if implementation differs from claims\n\n5. **Runtime\
        \ Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T1.2.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T1.2.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T1.2.2 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T1.2.2
      milestone_id: M-REM-1
      original_epic: E1
      original_task_id: T1.2.2
      priority: high
      status: blocked
      title: '[REM] Verify: Add leakage guardrails to feature builder'
    - dependencies: []
      description: "VERIFY task T1.1.3 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Wire onboarding\
        \ progress API\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created for task\
        \ T1.1.3\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T1.1.3\n   - Run tests: npm test or pytest (must\
        \ ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n   - Check test coverage for new\
        \ code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n   - Fix\
        \ any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read documentation\
        \ for task T1.1.3\n   - Verify every claimed feature has actual implementation\n   - Check for lies: Features documented\
        \ but not implemented\n   - Update docs if implementation differs from claims\n\n5. **Runtime Works**:\n   - Actually\
        \ RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs showing it working\n \
        \  - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU spikes?\n\n6. **Adversarial\
        \ Checks**:\n   - Run adversarial_bullshit_detector on task T1.1.3\n   - Check for superficial completion (empty metrics,\
        \ unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without fixing bugs)\n   - Check\
        \ for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T1.1.3 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead code paths that never execute?\n\n**WHAT\
        \ TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the tests\
        \ (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches \u2192 Update docs to match\
        \ code OR implement missing features\n- No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n- \u2705 All tests\
        \ pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime evidence\
        \ provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration verified (feature works\
        \ in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T1.1.3 was marked \"done\" but needs verification\n\
        **PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\
        \n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T1.1.3
      milestone_id: M-REM-1
      original_epic: E-PHASE1
      original_task_id: T1.1.3
      priority: high
      status: needs_improvement
      title: '[REM] Verify: Wire onboarding progress API'
    - dependencies: []
      description: "VERIFY task T2.1.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Build lag/rolling\
        \ feature generators with deterministic seed\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T2.1.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T2.1.1\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T2.1.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T2.1.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T2.1.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T2.1.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T2.1.1
      milestone_id: M-REM-1
      original_epic: E2
      original_task_id: T2.1.1
      priority: high
      status: pending
      title: '[REM] Verify: Build lag/rolling feature generators with deterministic seed'
    - dependencies: []
      description: "VERIFY task T2.2.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Train weather-aware\
        \ GAM baseline and document methodology\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T2.2.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T2.2.1\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T2.2.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T2.2.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T2.2.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T2.2.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T2.2.1
      milestone_id: M-REM-1
      original_epic: E2
      original_task_id: T2.2.1
      priority: high
      status: pending
      title: '[REM] Verify: Train weather-aware GAM baseline and document methodology'
    - dependencies: []
      description: "VERIFY task T7.1.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Complete\
        \ geocoding integration (city->lat/lon, cache strateg\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   -\
        \ Locate all files modified/created for task T7.1.1\n   - Verify code is not empty, stub, or placeholder\n   - Check\
        \ for TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task\
        \ T7.1.1\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not\
        \ just run code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T7.1.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T7.1.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T7.1.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T7.1.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T7.1.1
      milestone_id: M-REM-1
      original_epic: E7
      original_task_id: T7.1.1
      priority: high
      status: pending
      title: '[REM] Verify: Complete geocoding integration (city->lat/lon, cache strateg'
    - dependencies: []
      description: "VERIFY task T7.1.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Weather feature\
        \ join to model matrix (prevent future leakage\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T7.1.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T7.1.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T7.1.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T7.1.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T7.1.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T7.1.2 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T7.1.2
      milestone_id: M-REM-1
      original_epic: E7
      original_task_id: T7.1.2
      priority: high
      status: pending
      title: '[REM] Verify: Weather feature join to model matrix (prevent future leakage'
    - dependencies: []
      description: "VERIFY task T7.1.3 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Data contract\
        \ schema validation (Shopify, weather, ads)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T7.1.3\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T7.1.3\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T7.1.3\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T7.1.3\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T7.1.3 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T7.1.3 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T7.1.3
      milestone_id: M-REM-1
      original_epic: E7
      original_task_id: T7.1.3
      priority: high
      status: pending
      title: '[REM] Verify: Data contract schema validation (Shopify, weather, ads)'
    - dependencies: []
      description: "VERIFY task T7.2.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Incremental\
        \ ingestion with deduplication & checkpointing\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T7.2.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T7.2.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T7.2.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T7.2.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T7.2.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T7.2.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T7.2.1
      milestone_id: M-REM-1
      original_epic: E7
      original_task_id: T7.2.1
      priority: high
      status: pending
      title: '[REM] Verify: Incremental ingestion with deduplication & checkpointing'
    - dependencies: []
      description: "VERIFY task T7.2.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Data quality\
        \ monitoring & alerting (anomaly detection)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T7.2.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T7.2.2\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T7.2.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T7.2.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T7.2.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T7.2.2 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T7.2.2
      milestone_id: M-REM-1
      original_epic: E7
      original_task_id: T7.2.2
      priority: high
      status: pending
      title: '[REM] Verify: Data quality monitoring & alerting (anomaly detection)'
    - dependencies: []
      description: "VERIFY task T3.1.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Implement\
        \ budget allocator stress tests and regret bounds\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T3.1.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.1.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T3.1.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T3.1.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T3.1.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T3.1.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T3.1.1
      milestone_id: M-REM-1
      original_epic: E3
      original_task_id: T3.1.1
      priority: high
      status: pending
      title: '[REM] Verify: Implement budget allocator stress tests and regret bounds'
    - dependencies: []
      description: "VERIFY task T3.2.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Run design\
        \ system critic and ensure accessibility coverage\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T3.2.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.2.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T3.2.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T3.2.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T3.2.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T3.2.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T3.2.1
      milestone_id: M-REM-1
      original_epic: E3
      original_task_id: T3.2.1
      priority: high
      status: pending
      title: '[REM] Verify: Run design system critic and ensure accessibility coverage'
    - dependencies: []
      description: "VERIFY task T3.2.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Elevate dashboard\
        \ storytelling & UX\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T3.2.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.2.2\n   - Run tests: npm test or\
        \ pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n\
        \   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read\
        \ documentation for task T3.2.2\n   - Verify every claimed feature has actual implementation\n   - Check for lies:\
        \ Features documented but not implemented\n   - Update docs if implementation differs from claims\n\n5. **Runtime\
        \ Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T3.2.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T3.2.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T3.2.2 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T3.2.2
      milestone_id: M-REM-1
      original_epic: E3
      original_task_id: T3.2.2
      priority: high
      status: pending
      title: '[REM] Verify: Elevate dashboard storytelling & UX'
    - dependencies: []
      description: "VERIFY task T3.3.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Draft multi-agent\
        \ charter & delegation mesh (AutoGen/Swarm p\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T3.3.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.3.1\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T3.3.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T3.3.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T3.3.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T3.3.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T3.3.1
      milestone_id: M-REM-1
      original_epic: E3
      original_task_id: T3.3.1
      priority: high
      status: pending
      title: '[REM] Verify: Draft multi-agent charter & delegation mesh (AutoGen/Swarm p'
    - dependencies: []
      description: "VERIFY task T3.3.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Implement\
        \ hierarchical consensus & escalation engine\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T3.3.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.3.2\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T3.3.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T3.3.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T3.3.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T3.3.2 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T3.3.2
      milestone_id: M-REM-1
      original_epic: E3
      original_task_id: T3.3.2
      priority: high
      status: pending
      title: '[REM] Verify: Implement hierarchical consensus & escalation engine'
    - dependencies: []
      description: "VERIFY task T3.3.3 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Build closed-loop\
        \ simulation harness for autonomous teams\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T3.3.3\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.3.3\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T3.3.3\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T3.3.3\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T3.3.3 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T3.3.3 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T3.3.3
      milestone_id: M-REM-1
      original_epic: E3
      original_task_id: T3.3.3
      priority: high
      status: pending
      title: '[REM] Verify: Build closed-loop simulation harness for autonomous teams'
    - dependencies: []
      description: "VERIFY task T3.3.4 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Instrument\
        \ dynamic staffing telemetry & learning pipeline\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T3.3.4\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.3.4\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T3.3.4\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T3.3.4\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T3.3.4 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T3.3.4 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T3.3.4
      milestone_id: M-REM-1
      original_epic: E3
      original_task_id: T3.3.4
      priority: high
      status: pending
      title: '[REM] Verify: Instrument dynamic staffing telemetry & learning pipeline'
    - dependencies: []
      description: "VERIFY task T3.4.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Implement\
        \ Plan overview page with weather-driven insights\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T3.4.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.4.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T3.4.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T3.4.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T3.4.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T3.4.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T3.4.1
      milestone_id: M-REM-1
      original_epic: E3
      original_task_id: T3.4.1
      priority: high
      status: pending
      title: '[REM] Verify: Implement Plan overview page with weather-driven insights'
    - dependencies: []
      description: "VERIFY task T3.4.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Build WeatherOps\
        \ dashboard with allocator + weather KPIs\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T3.4.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.4.2\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T3.4.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T3.4.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T3.4.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T3.4.2 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T3.4.2
      milestone_id: M-REM-1
      original_epic: E3
      original_task_id: T3.4.2
      priority: high
      status: pending
      title: '[REM] Verify: Build WeatherOps dashboard with allocator + weather KPIs'
    - dependencies: []
      description: "VERIFY task T3.4.3 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Ship Experiments\
        \ hub UI for uplift & incrementality reviews\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T3.4.3\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.4.3\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T3.4.3\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T3.4.3\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T3.4.3 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T3.4.3 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T3.4.3
      milestone_id: M-REM-1
      original_epic: E3
      original_task_id: T3.4.3
      priority: high
      status: pending
      title: '[REM] Verify: Ship Experiments hub UI for uplift & incrementality reviews'
    - dependencies: []
      description: "VERIFY task T3.4.4 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Deliver storytelling\
        \ Reports view with weather + spend narra\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T3.4.4\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.4.4\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T3.4.4\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T3.4.4\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T3.4.4 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T3.4.4 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T3.4.4
      milestone_id: M-REM-1
      original_epic: E3
      original_task_id: T3.4.4
      priority: high
      status: pending
      title: '[REM] Verify: Deliver storytelling Reports view with weather + spend narra'
    - dependencies: []
      description: "VERIFY task T3.4.5 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Conduct design_system\
        \ + UX acceptance review across implemen\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T3.4.5\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.4.5\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T3.4.5\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T3.4.5\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T3.4.5 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T3.4.5 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T3.4.5
      milestone_id: M-REM-1
      original_epic: E3
      original_task_id: T3.4.5
      priority: high
      status: pending
      title: '[REM] Verify: Conduct design_system + UX acceptance review across implemen'
    - dependencies: []
      description: "VERIFY task T3.4.6 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Rewrite WeatherOps\
        \ dashboard around plain-language decisions\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T3.4.6\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.4.6\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T3.4.6\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T3.4.6\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T3.4.6 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T3.4.6 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T3.4.6
      milestone_id: M-REM-1
      original_epic: E3
      original_task_id: T3.4.6
      priority: high
      status: pending
      title: '[REM] Verify: Rewrite WeatherOps dashboard around plain-language decisions'
    - dependencies: []
      description: "VERIFY task T3.4.7 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Reimagine\
        \ Automations change log as a trust-first narrative\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T3.4.7\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.4.7\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T3.4.7\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T3.4.7\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T3.4.7 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T3.4.7 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T3.4.7
      milestone_id: M-REM-1
      original_epic: E3
      original_task_id: T3.4.7
      priority: high
      status: pending
      title: '[REM] Verify: Reimagine Automations change log as a trust-first narrative'
    - dependencies: []
      description: "VERIFY task T5.1.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Implement\
        \ Meta Marketing API client (creative + campaign man\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T5.1.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T5.1.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T5.1.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T5.1.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T5.1.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T5.1.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T5.1.1
      milestone_id: M-REM-1
      original_epic: E5
      original_task_id: T5.1.1
      priority: high
      status: pending
      title: '[REM] Verify: Implement Meta Marketing API client (creative + campaign man'
    - dependencies: []
      description: "VERIFY task T5.1.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Meta sandbox\
        \ and dry-run executor with credential vaulting\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T5.1.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T5.1.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T5.1.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T5.1.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T5.1.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T5.1.2 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T5.1.2
      milestone_id: M-REM-1
      original_epic: E5
      original_task_id: T5.1.2
      priority: high
      status: pending
      title: '[REM] Verify: Meta sandbox and dry-run executor with credential vaulting'
    - dependencies: []
      description: "VERIFY task T5.2.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Google Ads\
        \ API integration (campaign create/update, shared b\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T5.2.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T5.2.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T5.2.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T5.2.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T5.2.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T5.2.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T5.2.1
      milestone_id: M-REM-1
      original_epic: E5
      original_task_id: T5.2.1
      priority: high
      status: pending
      title: '[REM] Verify: Google Ads API integration (campaign create/update, shared b'
    - dependencies: []
      description: "VERIFY task T5.2.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Budget reconciliation\
        \ & spend guardrails across platforms\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T5.2.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T5.2.2\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T5.2.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T5.2.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T5.2.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T5.2.2 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T5.2.2
      milestone_id: M-REM-1
      original_epic: E5
      original_task_id: T5.2.2
      priority: high
      status: pending
      title: '[REM] Verify: Budget reconciliation & spend guardrails across platforms'
    - dependencies: []
      description: "VERIFY task T5.3.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Dry-run &\
        \ diff visualizer for ad pushes (pre-flight checks)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T5.3.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T5.3.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T5.3.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T5.3.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T5.3.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T5.3.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T5.3.1
      milestone_id: M-REM-1
      original_epic: E5
      original_task_id: T5.3.1
      priority: high
      status: pending
      title: '[REM] Verify: Dry-run & diff visualizer for ad pushes (pre-flight checks)'
    - dependencies: []
      description: "VERIFY task T5.3.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Automated\
        \ rollback + alerting when performance/regression de\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T5.3.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T5.3.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T5.3.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T5.3.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T5.3.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T5.3.2 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T5.3.2
      milestone_id: M-REM-1
      original_epic: E5
      original_task_id: T5.3.2
      priority: high
      status: pending
      title: '[REM] Verify: Automated rollback + alerting when performance/regression de'
    - dependencies: []
      description: "VERIFY task T11.1.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Implement\
        \ hardware probe & profile persistence\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T11.1.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T11.1.1\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T11.1.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T11.1.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T11.1.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T11.1.1 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T11.1.1
      milestone_id: M-REM-1
      original_epic: E11
      original_task_id: T11.1.1
      priority: high
      status: pending
      title: '[REM] Verify: Implement hardware probe & profile persistence'
    - dependencies: []
      description: "VERIFY task T11.1.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Adaptive\
        \ scheduling for heavy tasks\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T11.1.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T11.1.2\n   - Run tests: npm test or\
        \ pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n\
        \   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read\
        \ documentation for task T11.1.2\n   - Verify every claimed feature has actual implementation\n   - Check for lies:\
        \ Features documented but not implemented\n   - Update docs if implementation differs from claims\n\n5. **Runtime\
        \ Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T11.1.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T11.1.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T11.1.2 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T11.1.2
      milestone_id: M-REM-1
      original_epic: E11
      original_task_id: T11.1.2
      priority: high
      status: pending
      title: '[REM] Verify: Adaptive scheduling for heavy tasks'
    - dependencies: []
      description: "VERIFY task T11.2.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Design system\
        \ elevation (motion, typography, theming)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T11.2.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T11.2.1\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T11.2.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T11.2.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T11.2.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T11.2.1 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T11.2.1
      milestone_id: M-REM-1
      original_epic: E11
      original_task_id: T11.2.1
      priority: high
      status: pending
      title: '[REM] Verify: Design system elevation (motion, typography, theming)'
    - dependencies: []
      description: "VERIFY task T11.2.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Award-level\
        \ experience audit & remediation\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T11.2.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T11.2.2\n   - Run tests: npm test or\
        \ pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n\
        \   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read\
        \ documentation for task T11.2.2\n   - Verify every claimed feature has actual implementation\n   - Check for lies:\
        \ Features documented but not implemented\n   - Update docs if implementation differs from claims\n\n5. **Runtime\
        \ Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T11.2.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T11.2.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T11.2.2 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T11.2.2
      milestone_id: M-REM-1
      original_epic: E11
      original_task_id: T11.2.2
      priority: high
      status: pending
      title: '[REM] Verify: Award-level experience audit & remediation'
    - dependencies: []
      description: "VERIFY task T11.2.3 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Extend calm/aero\
        \ theme tokens to Automations and Experiments\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T11.2.3\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T11.2.3\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T11.2.3\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T11.2.3\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T11.2.3 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T11.2.3 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T11.2.3
      milestone_id: M-REM-1
      original_epic: E11
      original_task_id: T11.2.3
      priority: high
      status: pending
      title: '[REM] Verify: Extend calm/aero theme tokens to Automations and Experiments'
    - dependencies: []
      description: "VERIFY task T11.2.4 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Refactor\
        \ landing/marketing gradients into reusable tokens\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T11.2.4\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T11.2.4\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T11.2.4\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T11.2.4\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T11.2.4 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T11.2.4 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T11.2.4
      milestone_id: M-REM-1
      original_epic: E11
      original_task_id: T11.2.4
      priority: high
      status: pending
      title: '[REM] Verify: Refactor landing/marketing gradients into reusable tokens'
    - dependencies: []
      description: "VERIFY task T11.2.5 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Centralize\
        \ retry button styles in shared component once App \n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T11.2.5\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T11.2.5\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T11.2.5\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T11.2.5\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T11.2.5 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T11.2.5 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T11.2.5
      milestone_id: M-REM-1
      original_epic: E11
      original_task_id: T11.2.5
      priority: high
      status: pending
      title: '[REM] Verify: Centralize retry button styles in shared component once App '
    - dependencies: []
      description: "VERIFY task T11.2.6 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Formalize\
        \ shared panel mixin (border + shadow) to reduce ove\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T11.2.6\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T11.2.6\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T11.2.6\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T11.2.6\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T11.2.6 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T11.2.6 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T11.2.6
      milestone_id: M-REM-1
      original_epic: E11
      original_task_id: T11.2.6
      priority: high
      status: pending
      title: '[REM] Verify: Formalize shared panel mixin (border + shadow) to reduce ove'
    - dependencies: []
      description: "VERIFY task T4.1.10 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Cross-market\
        \ saturation optimization (fairness-aware)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T4.1.10\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T4.1.10\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T4.1.10\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T4.1.10\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T4.1.10 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T4.1.10 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T4.1.10
      milestone_id: M-REM-1
      original_epic: E4
      original_task_id: T4.1.10
      priority: high
      status: pending
      title: '[REM] Verify: Cross-market saturation optimization (fairness-aware)'
    - dependencies: []
      description: "VERIFY task T4.1.3 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Causal uplift\
        \ modeling & incremental lift validation\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T4.1.3\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T4.1.3\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T4.1.3\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T4.1.3\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T4.1.3 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T4.1.3 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T4.1.3
      milestone_id: M-REM-1
      original_epic: E4
      original_task_id: T4.1.3
      priority: high
      status: pending
      title: '[REM] Verify: Causal uplift modeling & incremental lift validation'
    - dependencies: []
      description: "VERIFY task T4.1.4 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Multi-horizon\
        \ ensemble forecasting\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T4.1.4\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T4.1.4\n   - Run tests: npm test or\
        \ pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n\
        \   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read\
        \ documentation for task T4.1.4\n   - Verify every claimed feature has actual implementation\n   - Check for lies:\
        \ Features documented but not implemented\n   - Update docs if implementation differs from claims\n\n5. **Runtime\
        \ Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T4.1.4\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T4.1.4 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T4.1.4 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T4.1.4
      milestone_id: M-REM-1
      original_epic: E4
      original_task_id: T4.1.4
      priority: high
      status: pending
      title: '[REM] Verify: Multi-horizon ensemble forecasting'
    - dependencies: []
      description: "VERIFY task T4.1.5 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Non-linear\
        \ allocation optimizer with constraints (ROAS, spen\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T4.1.5\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T4.1.5\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T4.1.5\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T4.1.5\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T4.1.5 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T4.1.5 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T4.1.5
      milestone_id: M-REM-1
      original_epic: E4
      original_task_id: T4.1.5
      priority: high
      status: pending
      title: '[REM] Verify: Non-linear allocation optimizer with constraints (ROAS, spen'
    - dependencies: []
      description: "VERIFY task T4.1.6 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: High-frequency\
        \ spend response modeling (intraday adjustments\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T4.1.6\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T4.1.6\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T4.1.6\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T4.1.6\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T4.1.6 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T4.1.6 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T4.1.6
      milestone_id: M-REM-1
      original_epic: E4
      original_task_id: T4.1.6
      priority: high
      status: pending
      title: '[REM] Verify: High-frequency spend response modeling (intraday adjustments'
    - dependencies: []
      description: "VERIFY task T4.1.7 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Marketing\
        \ mix budget solver (multi-channel, weather-aware)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T4.1.7\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T4.1.7\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T4.1.7\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T4.1.7\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T4.1.7 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T4.1.7 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T4.1.7
      milestone_id: M-REM-1
      original_epic: E4
      original_task_id: T4.1.7
      priority: high
      status: pending
      title: '[REM] Verify: Marketing mix budget solver (multi-channel, weather-aware)'
    - dependencies: []
      description: "VERIFY task T4.1.8 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Reinforcement-learning\
        \ shadow mode (safe exploration)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T4.1.8\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T4.1.8\n   - Run tests: npm test or\
        \ pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n\
        \   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read\
        \ documentation for task T4.1.8\n   - Verify every claimed feature has actual implementation\n   - Check for lies:\
        \ Features documented but not implemented\n   - Update docs if implementation differs from claims\n\n5. **Runtime\
        \ Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T4.1.8\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T4.1.8 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T4.1.8 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T4.1.8
      milestone_id: M-REM-1
      original_epic: E4
      original_task_id: T4.1.8
      priority: high
      status: pending
      title: '[REM] Verify: Reinforcement-learning shadow mode (safe exploration)'
    - dependencies: []
      description: "VERIFY task T4.1.9 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Creative-level\
        \ response modeling with brand safety guardrail\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T4.1.9\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T4.1.9\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T4.1.9\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T4.1.9\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T4.1.9 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T4.1.9 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T4.1.9
      milestone_id: M-REM-1
      original_epic: E4
      original_task_id: T4.1.9
      priority: high
      status: pending
      title: '[REM] Verify: Creative-level response modeling with brand safety guardrail'
    - dependencies: []
      description: "VERIFY task T6.1.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: MCP server\
        \ integration tests (all 25 tools across both provi\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T6.1.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.1.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T6.1.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T6.1.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T6.1.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T6.1.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T6.1.1
      milestone_id: M-REM-1
      original_epic: E6
      original_task_id: T6.1.1
      priority: high
      status: pending
      title: '[REM] Verify: MCP server integration tests (all 25 tools across both provi'
    - dependencies: []
      description: "VERIFY task T6.1.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Provider\
        \ failover testing (token limit simulation & automati\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   -\
        \ Locate all files modified/created for task T6.1.2\n   - Verify code is not empty, stub, or placeholder\n   - Check\
        \ for TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task\
        \ T6.1.2\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not\
        \ just run code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T6.1.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T6.1.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T6.1.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T6.1.2 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T6.1.2
      milestone_id: M-REM-1
      original_epic: E6
      original_task_id: T6.1.2
      priority: high
      status: pending
      title: '[REM] Verify: Provider failover testing (token limit simulation & automati'
    - dependencies: []
      description: "VERIFY task T6.1.3 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: State persistence\
        \ testing (checkpoint recovery across sessio\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T6.1.3\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.1.3\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T6.1.3\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T6.1.3\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T6.1.3 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T6.1.3 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T6.1.3
      milestone_id: M-REM-1
      original_epic: E6
      original_task_id: T6.1.3
      priority: high
      status: pending
      title: '[REM] Verify: State persistence testing (checkpoint recovery across sessio'
    - dependencies: []
      description: "VERIFY task T6.1.4 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Quality framework\
        \ validation (10 dimensions operational)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T6.1.4\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.1.4\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T6.1.4\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T6.1.4\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T6.1.4 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T6.1.4 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T6.1.4
      milestone_id: M-REM-1
      original_epic: E6
      original_task_id: T6.1.4
      priority: high
      status: pending
      title: '[REM] Verify: Quality framework validation (10 dimensions operational)'
    - dependencies: []
      description: "VERIFY task T6.2.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Credentials\
        \ security audit (auth.json, API keys, token rotat\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T6.2.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.2.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T6.2.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T6.2.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T6.2.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T6.2.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T6.2.1
      milestone_id: M-REM-1
      original_epic: E6
      original_task_id: T6.2.1
      priority: high
      status: pending
      title: '[REM] Verify: Credentials security audit (auth.json, API keys, token rotat'
    - dependencies: []
      description: "VERIFY task T6.2.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Error recovery\
        \ testing (graceful degradation, retry logic)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T6.2.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.2.2\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T6.2.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T6.2.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T6.2.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T6.2.2 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T6.2.2
      milestone_id: M-REM-1
      original_epic: E6
      original_task_id: T6.2.2
      priority: high
      status: pending
      title: '[REM] Verify: Error recovery testing (graceful degradation, retry logic)'
    - dependencies: []
      description: "VERIFY task T6.2.3 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Schema validation\
        \ enforcement (all data contracts validated)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T6.2.3\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.2.3\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T6.2.3\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T6.2.3\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T6.2.3 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T6.2.3 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T6.2.3
      milestone_id: M-REM-1
      original_epic: E6
      original_task_id: T6.2.3
      priority: high
      status: pending
      title: '[REM] Verify: Schema validation enforcement (all data contracts validated)'
    - dependencies: []
      description: "VERIFY task T6.2.4 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: API rate\
        \ limiting & exponential backoff (Open-Meteo, Shopify\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   -\
        \ Locate all files modified/created for task T6.2.4\n   - Verify code is not empty, stub, or placeholder\n   - Check\
        \ for TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task\
        \ T6.2.4\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not\
        \ just run code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T6.2.4\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T6.2.4\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T6.2.4 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T6.2.4 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T6.2.4
      milestone_id: M-REM-1
      original_epic: E6
      original_task_id: T6.2.4
      priority: high
      status: pending
      title: '[REM] Verify: API rate limiting & exponential backoff (Open-Meteo, Shopify'
    - dependencies: []
      description: "VERIFY task T6.3.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Performance\
        \ benchmarking (MCP overhead, checkpoint size, tok\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T6.3.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.3.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T6.3.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T6.3.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T6.3.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T6.3.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T6.3.1
      milestone_id: M-REM-1
      original_epic: E6
      original_task_id: T6.3.1
      priority: high
      status: pending
      title: '[REM] Verify: Performance benchmarking (MCP overhead, checkpoint size, tok'
    - dependencies: []
      description: "VERIFY task T6.3.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Enhanced\
        \ observability export (structured logs, metrics dash\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   -\
        \ Locate all files modified/created for task T6.3.2\n   - Verify code is not empty, stub, or placeholder\n   - Check\
        \ for TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task\
        \ T6.3.2\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not\
        \ just run code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T6.3.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T6.3.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T6.3.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T6.3.2 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T6.3.2
      milestone_id: M-REM-1
      original_epic: E6
      original_task_id: T6.3.2
      priority: high
      status: pending
      title: '[REM] Verify: Enhanced observability export (structured logs, metrics dash'
    - dependencies: []
      description: "VERIFY task T6.3.3 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Autopilot\
        \ loop end-to-end testing (full autonomous cycle val\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T6.3.3\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.3.3\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T6.3.3\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T6.3.3\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T6.3.3 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T6.3.3 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T6.3.3
      milestone_id: M-REM-1
      original_epic: E6
      original_task_id: T6.3.3
      priority: high
      status: pending
      title: '[REM] Verify: Autopilot loop end-to-end testing (full autonomous cycle val'
    - dependencies: []
      description: "VERIFY task T6.4.0 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Upgrade invariants\
        \ & preflight guardrails\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T6.4.0\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.4.0\n   - Run tests: npm test or\
        \ pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n\
        \   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read\
        \ documentation for task T6.4.0\n   - Verify every claimed feature has actual implementation\n   - Check for lies:\
        \ Features documented but not implemented\n   - Update docs if implementation differs from claims\n\n5. **Runtime\
        \ Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T6.4.0\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T6.4.0 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T6.4.0 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T6.4.0
      milestone_id: M-REM-1
      original_epic: E6
      original_task_id: T6.4.0
      priority: high
      status: pending
      title: '[REM] Verify: Upgrade invariants & preflight guardrails'
    - dependencies: []
      description: "VERIFY task T6.4.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Live feature\
        \ flag store with kill switch\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T6.4.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.4.1\n   - Run tests: npm test or\
        \ pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n\
        \   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read\
        \ documentation for task T6.4.1\n   - Verify every claimed feature has actual implementation\n   - Check for lies:\
        \ Features documented but not implemented\n   - Update docs if implementation differs from claims\n\n5. **Runtime\
        \ Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T6.4.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T6.4.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T6.4.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T6.4.1
      milestone_id: M-REM-1
      original_epic: E6
      original_task_id: T6.4.1
      priority: high
      status: pending
      title: '[REM] Verify: Live feature flag store with kill switch'
    - dependencies: []
      description: "VERIFY task T6.4.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Blue/green\
        \ worker manager & front-end proxy\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T6.4.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.4.2\n   - Run tests: npm test or\
        \ pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n\
        \   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read\
        \ documentation for task T6.4.2\n   - Verify every claimed feature has actual implementation\n   - Check for lies:\
        \ Features documented but not implemented\n   - Update docs if implementation differs from claims\n\n5. **Runtime\
        \ Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T6.4.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T6.4.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T6.4.2 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T6.4.2
      milestone_id: M-REM-1
      original_epic: E6
      original_task_id: T6.4.2
      priority: high
      status: pending
      title: '[REM] Verify: Blue/green worker manager & front-end proxy'
    - dependencies: []
      description: "VERIFY task T6.4.3 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Worker entrypoint\
        \ with DRY_RUN safeguards\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T6.4.3\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.4.3\n   - Run tests: npm test or\
        \ pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n\
        \   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read\
        \ documentation for task T6.4.3\n   - Verify every claimed feature has actual implementation\n   - Check for lies:\
        \ Features documented but not implemented\n   - Update docs if implementation differs from claims\n\n5. **Runtime\
        \ Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T6.4.3\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T6.4.3 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T6.4.3 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T6.4.3
      milestone_id: M-REM-1
      original_epic: E6
      original_task_id: T6.4.3
      priority: high
      status: pending
      title: '[REM] Verify: Worker entrypoint with DRY_RUN safeguards'
    - dependencies: []
      description: "VERIFY task T6.4.4 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Canary upgrade\
        \ harness & shadow validation\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T6.4.4\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.4.4\n   - Run tests: npm test or\
        \ pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n\
        \   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read\
        \ documentation for task T6.4.4\n   - Verify every claimed feature has actual implementation\n   - Check for lies:\
        \ Features documented but not implemented\n   - Update docs if implementation differs from claims\n\n5. **Runtime\
        \ Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T6.4.4\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T6.4.4 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T6.4.4 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T6.4.4
      milestone_id: M-REM-1
      original_epic: E6
      original_task_id: T6.4.4
      priority: high
      status: pending
      title: '[REM] Verify: Canary upgrade harness & shadow validation'
    - dependencies: []
      description: "VERIFY task T6.4.7 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Automatic\
        \ rollback monitors & kill-switch reset\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T6.4.7\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.4.7\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T6.4.7\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T6.4.7\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T6.4.7 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T6.4.7 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T6.4.7
      milestone_id: M-REM-1
      original_epic: E6
      original_task_id: T6.4.7
      priority: high
      status: pending
      title: '[REM] Verify: Automatic rollback monitors & kill-switch reset'
    - dependencies: []
      description: "VERIFY task T6.4.8 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Observability\
        \ & resource budgets during upgrade\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T6.4.8\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.4.8\n   - Run tests: npm test or\
        \ pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n\
        \   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read\
        \ documentation for task T6.4.8\n   - Verify every claimed feature has actual implementation\n   - Check for lies:\
        \ Features documented but not implemented\n   - Update docs if implementation differs from claims\n\n5. **Runtime\
        \ Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T6.4.8\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T6.4.8 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T6.4.8 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T6.4.8
      milestone_id: M-REM-1
      original_epic: E6
      original_task_id: T6.4.8
      priority: high
      status: pending
      title: '[REM] Verify: Observability & resource budgets during upgrade'
    - dependencies: []
      description: "VERIFY task T8.1.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Lock MCP\
        \ schemas to Zod shapes (SAFE: guardrail)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T8.1.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T8.1.1\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T8.1.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T8.1.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T8.1.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T8.1.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T8.1.1
      milestone_id: M-REM-1
      original_epic: E8
      original_task_id: T8.1.1
      priority: high
      status: pending
      title: '[REM] Verify: Lock MCP schemas to Zod shapes (SAFE: guardrail)'
    - dependencies: []
      description: "VERIFY task T8.1.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Implement\
        \ command allow-list in guardrails (SAFE: additive s\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T8.1.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T8.1.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T8.1.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T8.1.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T8.1.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T8.1.2 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T8.1.2
      milestone_id: M-REM-1
      original_epic: E8
      original_task_id: T8.1.2
      priority: high
      status: pending
      title: '[REM] Verify: Implement command allow-list in guardrails (SAFE: additive s'
    - dependencies: []
      description: "VERIFY task T8.1.3 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Thread correlation\
        \ IDs through state transitions (SAFE: obse\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T8.1.3\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T8.1.3\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T8.1.3\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T8.1.3\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T8.1.3 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T8.1.3 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T8.1.3
      milestone_id: M-REM-1
      original_epic: E8
      original_task_id: T8.1.3
      priority: high
      status: pending
      title: '[REM] Verify: Thread correlation IDs through state transitions (SAFE: obse'
    - dependencies: []
      description: "VERIFY task T8.2.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Implement\
        \ compact evidence-pack prompt mode (SAFE: new funct\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T8.2.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T8.2.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T8.2.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T8.2.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T8.2.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T8.2.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T8.2.1
      milestone_id: M-REM-1
      original_epic: E8
      original_task_id: T8.2.1
      priority: high
      status: pending
      title: '[REM] Verify: Implement compact evidence-pack prompt mode (SAFE: new funct'
    - dependencies: []
      description: "VERIFY task T8.2.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Finalize\
        \ Claude\u2194Codex coordinator failover (SAFE: expose exi\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n\
        \   - Locate all files modified/created for task T8.2.2\n   - Verify code is not empty, stub, or placeholder\n   -\
        \ Check for TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for\
        \ task T8.2.2\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior,\
        \ not just run code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build:\
        \ npm run build or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n\
        4. **Documentation Matches Code**:\n   - Read documentation for task T8.2.2\n   - Verify every claimed feature has\
        \ actual implementation\n   - Check for lies: Features documented but not implemented\n   - Update docs if implementation\
        \ differs from claims\n\n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide\
        \ runtime evidence: screenshot/logs showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check\
        \ resource usage: Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T8.2.2\n   - Check for superficial completion (empty metrics, unused infrastructure)\n   - Verify no test\
        \ manipulation (tests changed to pass without fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n\
        \   - Is task T8.2.2 actually integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are\
        \ there any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement\
        \ it\n- Failing tests \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation\
        \ issues\n- Doc mismatches \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192\
        \ Actually run it and capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n-\
        \ \u2705 Code exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation\
        \ matches implementation\n- \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task\
        \ T8.2.2 was marked \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T8.2.2
      milestone_id: M-REM-1
      original_epic: E8
      original_task_id: T8.2.2
      priority: high
      status: pending
      title: "[REM] Verify: Finalize Claude\u2194Codex coordinator failover (SAFE: expose exi"
    - dependencies: []
      description: "VERIFY task T9.1.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Stable prompt\
        \ headers with provider caching (SAFE: additive \n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T9.1.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T9.1.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T9.1.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T9.1.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T9.1.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T9.1.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T9.1.1
      milestone_id: M-REM-1
      original_epic: E9
      original_task_id: T9.1.1
      priority: high
      status: pending
      title: '[REM] Verify: Stable prompt headers with provider caching (SAFE: additive '
    - dependencies: []
      description: "VERIFY task T9.1.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Batch queue\
        \ for non-urgent prompts (SAFE: new queueing syste\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T9.1.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T9.1.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T9.1.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T9.1.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T9.1.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T9.1.2 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T9.1.2
      milestone_id: M-REM-1
      original_epic: E9
      original_task_id: T9.1.2
      priority: high
      status: pending
      title: '[REM] Verify: Batch queue for non-urgent prompts (SAFE: new queueing syste'
    - dependencies: []
      description: "VERIFY task T9.2.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Strict output\
        \ DSL validation (SAFE: validation layer only)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T9.2.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T9.2.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T9.2.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T9.2.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T9.2.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T9.2.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T9.2.1
      milestone_id: M-REM-1
      original_epic: E9
      original_task_id: T9.2.1
      priority: high
      status: pending
      title: '[REM] Verify: Strict output DSL validation (SAFE: validation layer only)'
    - dependencies: []
      description: "VERIFY task T9.2.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Idempotency\
        \ keys for mutating tools (SAFE: caching layer)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T9.2.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T9.2.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T9.2.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T9.2.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T9.2.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T9.2.2 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T9.2.2
      milestone_id: M-REM-1
      original_epic: E9
      original_task_id: T9.2.2
      priority: high
      status: pending
      title: '[REM] Verify: Idempotency keys for mutating tools (SAFE: caching layer)'
    - dependencies: []
      description: "VERIFY task T9.3.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: OpenTelemetry\
        \ spans for all operations (SAFE: tracing wrappe\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T9.3.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T9.3.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T9.3.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T9.3.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T9.3.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T9.3.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T9.3.1
      milestone_id: M-REM-1
      original_epic: E9
      original_task_id: T9.3.1
      priority: high
      status: pending
      title: '[REM] Verify: OpenTelemetry spans for all operations (SAFE: tracing wrappe'
    - dependencies: []
      description: "VERIFY task T9.4.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: SQLite FTS5\
        \ index for code search (SAFE: new index)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T9.4.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T9.4.1\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T9.4.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T9.4.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T9.4.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T9.4.1 was marked \"\
        done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T9.4.1
      milestone_id: M-REM-1
      original_epic: E9
      original_task_id: T9.4.1
      priority: high
      status: pending
      title: '[REM] Verify: SQLite FTS5 index for code search (SAFE: new index)'
    - dependencies: []
      description: "VERIFY task T10.1.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Cost telemetry\
        \ and budget alerts\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T10.1.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T10.1.1\n   - Run tests: npm test or\
        \ pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n\
        \   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read\
        \ documentation for task T10.1.1\n   - Verify every claimed feature has actual implementation\n   - Check for lies:\
        \ Features documented but not implemented\n   - Update docs if implementation differs from claims\n\n5. **Runtime\
        \ Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T10.1.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T10.1.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T10.1.1 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T10.1.1
      milestone_id: M-REM-1
      original_epic: E10
      original_task_id: T10.1.1
      priority: high
      status: pending
      title: '[REM] Verify: Cost telemetry and budget alerts'
    - dependencies: []
      description: "VERIFY task T12.0.3 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Document\
        \ synthetic tenant characteristics\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T12.0.3\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T12.0.3\n   - Run tests: npm test or\
        \ pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n\
        \   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read\
        \ documentation for task T12.0.3\n   - Verify every claimed feature has actual implementation\n   - Check for lies:\
        \ Features documented but not implemented\n   - Update docs if implementation differs from claims\n\n5. **Runtime\
        \ Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T12.0.3\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T12.0.3 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T12.0.3 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T12.0.3
      milestone_id: M-REM-1
      original_epic: E12
      original_task_id: T12.0.3
      priority: high
      status: pending
      title: '[REM] Verify: Document synthetic tenant characteristics'
    - dependencies: []
      description: "VERIFY task T12.1.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Validate\
        \ feature store joins against historical weather base\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   -\
        \ Locate all files modified/created for task T12.1.2\n   - Verify code is not empty, stub, or placeholder\n   - Check\
        \ for TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task\
        \ T12.1.2\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not\
        \ just run code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T12.1.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T12.1.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T12.1.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T12.1.2 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T12.1.2
      milestone_id: M-REM-1
      original_epic: E12
      original_task_id: T12.1.2
      priority: high
      status: pending
      title: '[REM] Verify: Validate feature store joins against historical weather base'
    - dependencies: []
      description: "VERIFY task T12.2.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Backtest\
        \ weather-aware model vs control across top tenants\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T12.2.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T12.2.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T12.2.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T12.2.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T12.2.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T12.2.1 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T12.2.1
      milestone_id: M-REM-1
      original_epic: E12
      original_task_id: T12.2.1
      priority: high
      status: pending
      title: '[REM] Verify: Backtest weather-aware model vs control across top tenants'
    - dependencies: []
      description: "VERIFY task T12.PoC.3 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Create\
        \ PoC demo results and proof brief\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T12.PoC.3\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T12.PoC.3\n   - Run tests: npm test\
        \ or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n   - Check test\
        \ coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py (0\
        \ errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T12.PoC.3\n   - Verify every claimed feature has actual implementation\n   - Check\
        \ for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\n5.\
        \ **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T12.PoC.3\n   - Check for\
        \ superficial completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to\
        \ pass without fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T12.PoC.3\
        \ actually integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code\
        \ paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests\
        \ \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T12.PoC.3 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T12.PoC.3
      milestone_id: M-REM-1
      original_epic: E12
      original_task_id: T12.PoC.3
      priority: high
      status: pending
      title: '[REM] Verify: Create PoC demo results and proof brief'
    - dependencies: []
      description: "VERIFY task T13.1.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Validate\
        \ 90-day tenant data coverage across sales, spend, an\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   -\
        \ Locate all files modified/created for task T13.1.1\n   - Verify code is not empty, stub, or placeholder\n   - Check\
        \ for TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task\
        \ T13.1.1\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not\
        \ just run code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T13.1.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T13.1.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T13.1.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T13.1.1 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T13.1.1
      milestone_id: M-REM-1
      original_epic: E13
      original_task_id: T13.1.1
      priority: high
      status: pending
      title: '[REM] Verify: Validate 90-day tenant data coverage across sales, spend, an'
    - dependencies: []
      description: "VERIFY task T13.1.3 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Implement\
        \ product taxonomy auto-classification with weather \n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T13.1.3\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T13.1.3\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T13.1.3\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T13.1.3\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T13.1.3 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T13.1.3 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T13.1.3
      milestone_id: M-REM-1
      original_epic: E13
      original_task_id: T13.1.3
      priority: high
      status: pending
      title: '[REM] Verify: Implement product taxonomy auto-classification with weather '
    - dependencies: []
      description: "VERIFY task T13.1.4 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Data quality\
        \ validation framework (verify data fitness for M\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T13.1.4\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T13.1.4\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T13.1.4\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T13.1.4\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T13.1.4 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T13.1.4 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T13.1.4
      milestone_id: M-REM-1
      original_epic: E13
      original_task_id: T13.1.4
      priority: high
      status: pending
      title: '[REM] Verify: Data quality validation framework (verify data fitness for M'
    - dependencies: []
      description: "VERIFY task T13.2.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Replace\
        \ heuristic MMM with LightweightMMM adstock+saturation\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   -\
        \ Locate all files modified/created for task T13.2.1\n   - Verify code is not empty, stub, or placeholder\n   - Check\
        \ for TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task\
        \ T13.2.1\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not\
        \ just run code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T13.2.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T13.2.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T13.2.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T13.2.1 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T13.2.1
      milestone_id: M-REM-1
      original_epic: E13
      original_task_id: T13.2.1
      priority: high
      status: pending
      title: '[REM] Verify: Replace heuristic MMM with LightweightMMM adstock+saturation'
    - dependencies: []
      description: "VERIFY task T13.2.3 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Replace\
        \ heuristic allocator with constraint-aware optimizer\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   -\
        \ Locate all files modified/created for task T13.2.3\n   - Verify code is not empty, stub, or placeholder\n   - Check\
        \ for TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task\
        \ T13.2.3\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not\
        \ just run code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T13.2.3\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T13.2.3\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T13.2.3 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T13.2.3 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T13.2.3
      milestone_id: M-REM-1
      original_epic: E13
      original_task_id: T13.2.3
      priority: high
      status: pending
      title: '[REM] Verify: Replace heuristic allocator with constraint-aware optimizer'
    - dependencies: []
      description: "VERIFY task T13.3.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Implement\
        \ DMA-first geographic aggregation with hierarchical\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T13.3.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T13.3.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T13.3.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T13.3.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T13.3.2 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T13.3.2 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T13.3.2
      milestone_id: M-REM-1
      original_epic: E13
      original_task_id: T13.3.2
      priority: high
      status: pending
      title: '[REM] Verify: Implement DMA-first geographic aggregation with hierarchical'
    - dependencies: []
      description: "VERIFY task T13.4.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Add modeling\
        \ reality critic to Autopilot\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T13.4.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T13.4.1\n   - Run tests: npm test or\
        \ pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n\
        \   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read\
        \ documentation for task T13.4.1\n   - Verify every claimed feature has actual implementation\n   - Check for lies:\
        \ Features documented but not implemented\n   - Update docs if implementation differs from claims\n\n5. **Runtime\
        \ Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T13.4.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T13.4.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T13.4.1 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T13.4.1
      milestone_id: M-REM-1
      original_epic: E13
      original_task_id: T13.4.1
      priority: high
      status: pending
      title: '[REM] Verify: Add modeling reality critic to Autopilot'
    - dependencies: []
      description: "VERIFY task T13.5.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Train weather-aware\
        \ allocation model on top of MMM baseline\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T13.5.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T13.5.1\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T13.5.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T13.5.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to pass without\
        \ fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T13.5.1 actually\
        \ integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code paths\
        \ that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192\
        \ Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T13.5.1 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T13.5.1
      milestone_id: M-REM-1
      original_epic: E13
      original_task_id: T13.5.1
      priority: high
      status: pending
      title: '[REM] Verify: Train weather-aware allocation model on top of MMM baseline'
    - dependencies: []
      description: "VERIFY task T-MLR-0.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Create\
        \ ModelingReality critic with quantitative thresholds\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   -\
        \ Locate all files modified/created for task T-MLR-0.1\n   - Verify code is not empty, stub, or placeholder\n   -\
        \ Check for TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for\
        \ task T-MLR-0.1\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior,\
        \ not just run code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build:\
        \ npm run build or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n\
        4. **Documentation Matches Code**:\n   - Read documentation for task T-MLR-0.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but not implemented\n   - Update docs if implementation\
        \ differs from claims\n\n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide\
        \ runtime evidence: screenshot/logs showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check\
        \ resource usage: Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T-MLR-0.1\n   - Check for superficial completion (empty metrics, unused infrastructure)\n   - Verify no\
        \ test manipulation (tests changed to pass without fixing bugs)\n   - Check for documentation-code mismatches\n\n\
        7. **Integration**:\n   - Is task T-MLR-0.1 actually integrated into the system?\n   - Can you demonstrate it working\
        \ in context?\n   - Are there any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors\
        \ \u2192 Fix compilation issues\n- Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial completion \u2192 Do the real work\n\
        \n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime evidence provided (screenshot/logs)\n\
        - \u2705 Adversarial detector: APPROVED\n- \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T-MLR-0.1 was marked \"done\" but needs verification\n**PRIORITY**: Must verify\
        \ before claiming remediation complete\n\n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n   npm run build 2>&1\n\
        \   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification report\n\n2. **TEST\
        \ Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"X/X passing\" (all\
        \ tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n\
        \   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in verification report\n\
        \n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n   - Log file from\
        \ feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration video/recording\n\
        \n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation sections\n \
        \  - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0 errors required)\n\
        - [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities required)\n- [\
        \ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\n**NOTE**:\
        \ Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection. Do NOT assume\
        \ quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T-MLR-0.1
      milestone_id: M-REM-1
      original_epic: E-ML-REMEDIATION
      original_task_id: T-MLR-0.1
      priority: high
      status: pending
      title: '[REM] Verify: Create ModelingReality critic with quantitative thresholds'
    - dependencies: []
      description: "VERIFY task T-MLR-0.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Update\
        \ all ML task exit criteria with objective metrics\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T-MLR-0.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T-MLR-0.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T-MLR-0.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T-MLR-0.2\n   - Check for\
        \ superficial completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to\
        \ pass without fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T-MLR-0.2\
        \ actually integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code\
        \ paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests\
        \ \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T-MLR-0.2 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T-MLR-0.2
      milestone_id: M-REM-1
      original_epic: E-ML-REMEDIATION
      original_task_id: T-MLR-0.2
      priority: high
      status: pending
      title: '[REM] Verify: Update all ML task exit criteria with objective metrics'
    - dependencies: []
      description: "VERIFY task T-MLR-1.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Debug\
        \ and fix weather multiplier logic in data generator\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T-MLR-1.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T-MLR-1.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T-MLR-1.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T-MLR-1.1\n   - Check for\
        \ superficial completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to\
        \ pass without fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T-MLR-1.1\
        \ actually integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code\
        \ paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests\
        \ \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T-MLR-1.1 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T-MLR-1.1
      milestone_id: M-REM-1
      original_epic: E-ML-REMEDIATION
      original_task_id: T-MLR-1.1
      priority: high
      status: pending
      title: '[REM] Verify: Debug and fix weather multiplier logic in data generator'
    - dependencies: []
      description: "VERIFY task T-MLR-1.3 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Create\
        \ validation tests for synthetic data quality\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T-MLR-1.3\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T-MLR-1.3\n   -\
        \ Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T-MLR-1.3\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T-MLR-1.3\n   - Check for\
        \ superficial completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to\
        \ pass without fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T-MLR-1.3\
        \ actually integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code\
        \ paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests\
        \ \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T-MLR-1.3 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T-MLR-1.3
      milestone_id: M-REM-1
      original_epic: E-ML-REMEDIATION
      original_task_id: T-MLR-1.3
      priority: high
      status: pending
      title: '[REM] Verify: Create validation tests for synthetic data quality'
    - dependencies: []
      description: "VERIFY task T-MLR-2.1 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Implement\
        \ proper train/val/test splitting with no leakage\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T-MLR-2.1\n   - Verify code is not empty, stub, or placeholder\n   - Check for\
        \ TODO/FIXME comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T-MLR-2.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build\
        \ or python build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T-MLR-2.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T-MLR-2.1\n   - Check for\
        \ superficial completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to\
        \ pass without fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T-MLR-2.1\
        \ actually integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code\
        \ paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests\
        \ \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T-MLR-2.1 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T-MLR-2.1
      milestone_id: M-REM-1
      original_epic: E-ML-REMEDIATION
      original_task_id: T-MLR-2.1
      priority: high
      status: pending
      title: '[REM] Verify: Implement proper train/val/test splitting with no leakage'
    - dependencies: []
      description: "VERIFY task T-MLR-2.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Implement\
        \ LightweightMMM with weather features\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T-MLR-2.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T-MLR-2.2\n   -\
        \ Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T-MLR-2.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T-MLR-2.2\n   - Check for\
        \ superficial completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to\
        \ pass without fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T-MLR-2.2\
        \ actually integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code\
        \ paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests\
        \ \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T-MLR-2.2 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T-MLR-2.2
      milestone_id: M-REM-1
      original_epic: E-ML-REMEDIATION
      original_task_id: T-MLR-2.2
      priority: high
      status: pending
      title: '[REM] Verify: Implement LightweightMMM with weather features'
    - dependencies: []
      description: "VERIFY task T-MLR-2.5 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Compare\
        \ models to baseline (naive/seasonal/linear)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T-MLR-2.5\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T-MLR-2.5\n   -\
        \ Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T-MLR-2.5\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T-MLR-2.5\n   - Check for\
        \ superficial completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to\
        \ pass without fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T-MLR-2.5\
        \ actually integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code\
        \ paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests\
        \ \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T-MLR-2.5 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T-MLR-2.5
      milestone_id: M-REM-1
      original_epic: E-ML-REMEDIATION
      original_task_id: T-MLR-2.5
      priority: high
      status: pending
      title: '[REM] Verify: Compare models to baseline (naive/seasonal/linear)'
    - dependencies: []
      description: "VERIFY task T-MLR-3.2 was completed correctly with quality.\n\n**ORIGINAL TASK**: [REM] Verify: Write\
        \ comprehensive ML validation documentation\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T-MLR-3.2\n   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T-MLR-3.2\n   -\
        \ Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check behavior, not just run code)\n\
        \   - Check test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python\
        \ build.py (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation Matches\
        \ Code**:\n   - Read documentation for task T-MLR-3.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n   - Check resource usage: Memory leaks? CPU\
        \ spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task T-MLR-3.2\n   - Check for\
        \ superficial completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation (tests changed to\
        \ pass without fixing bugs)\n   - Check for documentation-code mismatches\n\n7. **Integration**:\n   - Is task T-MLR-3.2\
        \ actually integrated into the system?\n   - Can you demonstrate it working in context?\n   - Are there any dead code\
        \ paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests\
        \ \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n- Doc mismatches\
        \ \u2192 Update docs to match code OR implement missing features\n- No runtime evidence \u2192 Actually run it and\
        \ capture evidence\n- Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and\
        \ is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task T-MLR-3.2 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 errors\"\n   - Provide output in verification\
        \ report\n\n2. **TEST Evidence**:\n   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must show\
        \ \"X/X passing\" (all tests pass)\n   - Provide output in verification report\n\n3. **AUDIT Evidence**:\n   ```bash\n\
        \   npm audit 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide output in\
        \ verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n   - Screenshot of feature running in browser/CLI\n\
        \   - Log file from feature execution\n   - Artifact created by feature (JSON file, report, etc.)\n   - Demonstration\
        \ video/recording\n\n5. **DOCUMENTATION Evidence**:\n   - List files modified/created\n   - Quote relevant documentation\
        \ sections\n   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [ ] Build output collected (0\
        \ errors required)\n- [ ] Test output collected (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n- [ ] Documentation verified (no mismatches)\n\
        \n**NOTE**: Without ALL evidence above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      domain: product
      epic_id: E-REMEDIATION
      exit_criteria:
      - Implementation verified
      - Tests verified
      - Quality gate APPROVED
      - Runtime verification PASSED
      - Critical issues fixed
      id: REM-T-MLR-3.2
      milestone_id: M-REM-1
      original_epic: E-ML-REMEDIATION
      original_task_id: T-MLR-3.2
      priority: high
      status: pending
      title: '[REM] Verify: Write comprehensive ML validation documentation'
    title: '[CRITICAL] Core Infrastructure Audit'
  - id: M-MLR-2
    status: pending
    tasks:
    - dependencies:
      - T-MLR-2.2
      description: Train models on all 20 synthetic tenants with cross-validation
      domain: product
      exit_criteria: []
      id: T-MLR-2.3
      status: pending
      title: Train models on all 20 synthetic tenants with cross-validation
    title: M-MLR-2
  - id: M-MLR-3
    status: pending
    tasks:
    - dependencies:
      - T-MLR-3.2
      description: Package all evidence artifacts for review
      domain: product
      exit_criteria: []
      id: T-MLR-3.3
      status: blocked
      title: Package all evidence artifacts for review
    title: M-MLR-3
  - id: M-MLR-4
    status: pending
    tasks:
    - dependencies:
      - T-MLR-0.1
      - T-MLR-3.3
      description: Deploy ModelingReality_v2 critic to production
      domain: product
      exit_criteria: []
      id: T-MLR-4.1
      status: pending
      title: Deploy ModelingReality_v2 critic to production
    - dependencies:
      - T-MLR-4.1
      description: Update autopilot policy to require critic approval
      domain: product
      exit_criteria: []
      id: T-MLR-4.2
      status: done
      title: Update autopilot policy to require critic approval
    - dependencies:
      - T-MLR-4.2
      description: Create meta-critic to review past completed ML tasks
      domain: product
      exit_criteria: []
      id: T-MLR-4.3
      status: blocked
      title: Create meta-critic to review past completed ML tasks
    - dependencies:
      - T-MLR-4.3
      description: Document lessons learned and update contributor guide
      domain: product
      exit_criteria: []
      id: T-MLR-4.4
      status: pending
      title: Document lessons learned and update contributor guide
    title: M-MLR-4
  priority: critical
  status: in_progress
  title: '[CRITICAL] Quality Remediation - Audit All Completed Work'
- domain: modeling
  id: E-ML-REMEDIATION
  milestones:
  - id: M-MLR-0
    status: pending
    tasks:
    - dependencies: []
      description: "Create critic that enforces quantitative thresholds: R\xB2 > 0.50, correct\nelasticity signs, baseline\
        \ comparison required, no subjective judgment.\n"
      domain: modeling
      exit_criteria:
      - artifact:tools/wvo_mcp/src/critics/modeling_reality_v2.ts
      - "test:Critic FAILS when R\xB2 < 0.50"
      - test:Critic FAILS when no baseline comparison
      - test:Critic FAILS when weather elasticity signs wrong
      - metric:critic_strictness = 1.0
      - critic:tests
      id: T-MLR-0.1
      status: done
      title: Create ModelingReality critic with quantitative thresholds
    - dependencies: []
      domain: modeling
      exit_criteria:
      - artifact:state/roadmap.yaml (T12.*, T13.* updated)
      - verification:All ML tasks have "metric:r2 > 0.50"
      - verification:All ML tasks have "metric:beats_baseline > 1.10"
      - verification:All ML tasks have "critic:modeling_reality_v2"
      id: T-MLR-0.2
      status: done
      title: Update all ML task exit criteria with objective metrics
    - dependencies: []
      domain: modeling
      exit_criteria:
      - artifact:docs/ML_QUALITY_STANDARDS.md
      - verification:Numeric thresholds for all metrics
      - verification:Baseline comparison requirements
      - review:External ML practitioner peer review
      id: T-MLR-0.3
      status: pending
      title: Document world-class quality standards for ML
    title: 'Foundation: Truth & Accountability'
  - id: M-MLR-1
    status: pending
    tasks:
    - dependencies: []
      domain: modeling
      exit_criteria:
      - artifact:scripts/weather/generate_synthetic_tenants_v2.py
      - "test:Extreme correlation = 0.85 \xB1 0.05"
      - "test:High correlation = 0.70 \xB1 0.05"
      - "test:Medium correlation = 0.40 \xB1 0.05"
      - test:None correlation < 0.10
      - critic:data_quality
      id: T-MLR-1.1
      status: done
      title: Debug and fix weather multiplier logic in data generator
    - dependencies: []
      domain: modeling
      exit_criteria:
      - artifact:storage/seeds/synthetic_v2/*.parquet (20 files)
      - metric:total_rows = 219000
      - metric:date_range = 2022-01-01 to 2024-12-31
      - metric:weather_correlations_within_target >= 0.90
      - test:pytest tests/data_gen/test_synthetic_v2_quality.py
      - critic:data_quality
      id: T-MLR-1.2
      status: pending
      title: Generate 3 years of synthetic data for 20 tenants
    - dependencies: []
      domain: modeling
      exit_criteria:
      - artifact:tests/data_gen/test_synthetic_v2_quality.py
      - test:20/20 tenant tests pass
      - artifact:experiments/data_validation/correlation_plots.pdf
      - critic:tests
      id: T-MLR-1.3
      status: done
      title: Create validation tests for synthetic data quality
    title: 'Phase 1: Fix Synthetic Data (2 weeks)'
  - id: M-MLR-2
    status: pending
    tasks:
    - dependencies: []
      domain: modeling
      exit_criteria:
      - artifact:shared/libs/modeling/time_series_split.py
      - test:Validation after training (no date overlap)
      - test:Test after validation (no date overlap)
      - test:Split percentages 70/15/15
      - critic:leakage
      id: T-MLR-2.1
      status: done
      title: Implement proper train/val/test splitting with no leakage
    - dependencies: []
      domain: modeling
      exit_criteria:
      - artifact:apps/model/mmm_lightweight_weather.py
      - test:Adstock transformation applied
      - test:Hill saturation curves applied
      - test:Weather interaction terms included
      - test:pytest tests/model/test_mmm_lightweight.py (12/12 pass)
      - critic:academic_rigor
      id: T-MLR-2.2
      status: done
      title: Implement LightweightMMM with weather features
    - dependencies: []
      domain: modeling
      exit_criteria:
      - artifact:experiments/mmm_v2/validation_report.json
      - metric:weather_sensitive_r2_pass_rate >= 0.80
      - metric:weather_elasticity_sign_correct = 1.0
      - metric:no_overfitting_detected = true
      - critic:modeling_reality_v2
      - critic:academic_rigor
      id: T-MLR-2.4
      status: pending
      title: Validate model performance against objective thresholds
    - dependencies: []
      domain: modeling
      exit_criteria:
      - artifact:experiments/mmm_v2/baseline_comparison.json
      - metric:beats_naive_by >= 1.10
      - metric:beats_seasonal_by >= 1.05
      - metric:beats_linear_by >= 1.05
      - artifact:experiments/mmm_v2/baseline_comparison_plots.pdf
      - critic:modeling_reality_v2
      id: T-MLR-2.5
      status: done
      title: Compare models to baseline (naive/seasonal/linear)
    - dependencies: []
      domain: modeling
      exit_criteria:
      - artifact:experiments/mmm_v2/robustness_report.json
      - test:Model handles extreme weather without crash
      - test:Model handles missing data gracefully
      - test:Zero ad spend predicts organic baseline
      - test:pytest tests/model/test_mmm_robustness.py (15/15 pass)
      - critic:modeling_reality_v2
      id: T-MLR-2.6
      status: pending
      title: Run robustness tests (outliers, missing data, edge cases)
    title: 'Phase 2: Rigorous MMM Training (3 weeks)'
  - id: M-MLR-3
    status: pending
    tasks:
    - dependencies: []
      domain: modeling
      exit_criteria:
      - artifact:experiments/mmm_v2/validation_notebook.ipynb
      - test:Notebook runs end-to-end without errors
      - test:Output matches claimed metrics
      - artifact:experiments/mmm_v2/validation_notebook.html
      - critic:academic_rigor
      id: T-MLR-3.1
      status: pending
      title: Create reproducible validation notebook
    - dependencies: []
      domain: modeling
      exit_criteria:
      - artifact:docs/ML_VALIDATION_COMPLETE.md
      - verification:Links to reproducible notebook
      - verification:Includes limitations section
      - verification:Includes baseline comparisons
      - review:External ML practitioner peer review
      id: T-MLR-3.2
      status: done
      title: Write comprehensive ML validation documentation
    title: 'Phase 3: Reproducibility & Documentation (1 week)'
  status: pending
  title: ML Model Remediation - From Prototype to Production
- domain: product
  id: E-PHASE0
  milestones:
  - id: M0.1
    status: done
    tasks:
    - dependencies: []
      description: Wire apps/validation/incrementality.py into ingestion runs with nightly job execution
      domain: product
      exit_criteria:
      - artifact:state/analytics/experiments/geo_holdouts/*.json
      - artifact:state/telemetry/experiments/geo_holdout_runs.jsonl
      - critic:data_quality
      id: T0.1.1
      status: done
      title: Implement geo holdout plumbing
    - dependencies: []
      description: Plan API surfaces experiment payloads; Plan UI renders lift/confidence cards with download
      domain: product
      exit_criteria:
      - artifact:apps/api/schemas/plan.py
      - artifact:apps/web/src/pages/plan.tsx
      - critic:tests
      - critic:design_system
      id: T0.1.2
      status: done
      title: Build lift & confidence UI surfaces
    - dependencies: []
      description: Quantile calibration metrics with summary published to docs
      domain: product
      exit_criteria:
      - artifact:docs/modeling/forecast_calibration_report.md
      - artifact:state/telemetry/calibration/*.json
      - critic:forecast_stitch
      id: T0.1.3
      status: done
      title: Generate forecast calibration report
    title: Measurement & Confidence Foundations
  status: done
  title: 'Phase 0: Measurement & Confidence'
- domain: product
  id: E-PHASE1
  milestones:
  - id: M1.1
    status: done
    tasks:
    - dependencies: []
      description: Implement GET/POST /onboarding/progress routes with telemetry instrumentation
      domain: product
      exit_criteria:
      - artifact:apps/api/routes/onboarding.py
      - artifact:apps/web/src/hooks/useOnboardingProgress.ts
      - critic:tests
      id: T1.1.3
      status: done
      title: Wire onboarding progress API
    title: Experience Delivery MVP
  status: done
  title: 'Phase 1: Experience Delivery'
- description: Stand up weather + marketing ingestion, harmonise geo/time, and validate data quality.
  domain: product
  id: E1
  milestones:
  - id: M1.1
    status: done
    tasks:
    - dependencies: []
      description: Interactive scenario flows with API endpoints for scenario snapshots and storybook coverage
      domain: product
      exit_criteria:
      - critic:build
      - critic:tests
      - doc:docs/INGESTION.md
      id: T1.1.1
      status: done
      title: Design Open-Meteo + Shopify connectors and data contracts
    - dependencies: []
      description: Map + chart overlays with export service (PPT/CSV)
      domain: product
      exit_criteria:
      - critic:data_quality
      - critic:org_pm
      - artifact:experiments/ingest/dq_report.json
      id: T1.1.2
      status: done
      title: Implement ingestion Prefect flow with checkpointing
    title: Connector scaffolding
  - id: M1.2
    status: done
    tasks:
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:forecast_stitch
      - doc:docs/weather/blending.md
      id: T1.2.1
      status: done
      title: Blend historical + forecast weather, enforce timezone alignment
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:leakage
      - critic:tests
      id: T1.2.2
      status: done
      title: Add leakage guardrails to feature builder
    title: Weather harmonisation
  status: done
  title: "Epic 1 \u2014 Ingest & Weather Foundations"
- domain: mcp
  id: E10
  milestones:
  - id: M10.1
    status: done
    tasks:
    - dependencies: []
      domain: mcp
      exit_criteria:
      - Provider cost telemetry recorded in state/telemetry/operations.jsonl
      - Budget thresholds configurable per environment
      - Alert surfaced via state/context.md and orchestration logs
      id: T10.1.1
      status: done
      title: Cost telemetry and budget alerts
    title: Usage telemetry & guardrails
  status: done
  title: "PHASE-6-COST \u2014 Usage-Based Optimisations"
- description: Auto-detect hardware, adapt workloads, and guarantee great performance on constrained machines.
  domain: product
  id: E11
  milestones:
  - id: M11.1
    status: done
    tasks:
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:build
      - doc:docs/ROADMAP.md
      id: T11.1.1
      status: done
      title: Implement hardware probe & profile persistence
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:tests
      - artifact:state/device_profiles.json
      id: T11.1.2
      status: done
      title: Adaptive scheduling for heavy tasks
    title: Capability Detection
  - id: M11.2
    status: done
    tasks:
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:design_system
      - doc:docs/WEB_DESIGN_SYSTEM.md
      id: T11.2.1
      status: done
      title: Design system elevation (motion, typography, theming)
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:exec_review
      - artifact:docs/UX_CRITIQUE.md
      id: T11.2.2
      status: done
      title: Award-level experience audit & remediation
    - dependencies: []
      domain: product
      exit_criteria:
      - artifact:apps/web/styles/themes/calm.ts
      - artifact:apps/web/styles/themes/aero.ts
      - critic:design_system
      id: T11.2.3
      status: done
      title: Extend calm/aero theme tokens to Automations and Experiments surfaces
    - dependencies: []
      domain: product
      exit_criteria:
      - artifact:apps/web/styles/tokens/gradients.md
      - critic:design_system
      id: T11.2.4
      status: done
      title: Refactor landing/marketing gradients into reusable tokens
    - dependencies: []
      domain: product
      exit_criteria:
      - artifact:apps/web/components/buttons/RetryButton.tsx
      - unknown
      - critic:design_system
      id: T11.2.5
      status: done
      title: Centralize retry button styles in shared component once App Router lands
    - dependencies: []
      domain: product
      exit_criteria:
      - artifact:apps/web/styles/mixins/panel.css
      - critic:design_system
      id: T11.2.6
      status: done
      title: Formalize shared panel mixin (border + shadow) to reduce overrides
    title: Falcon Design System & Award-ready UX
  status: done
  title: Resource-Aware Intelligence & Personalisation
- description: Prioritise end-to-end weather ingestion QA, model backtests, and operational readiness so WeatherVane shiproom
    can demo weather insights with confidence.
  domain: product
  id: E12
  milestones:
  - id: M12.0
    status: pending
    tasks:
    - dependencies: []
      description: Create 4 simulated tenants with Shopify products, Meta/Google ads spend, Klaviyo data, and weather-driven
        demand patterns
      domain: product
      exit_criteria:
      - artifact:storage/seeds/synthetic/*.parquet
      - artifact:state/analytics/synthetic_tenant_profiles.json
      - critic:data_quality
      id: T12.0.1
      status: blocked
      title: Generate synthetic multi-tenant dataset with weather-sensitive products
    - dependencies: []
      description: Confirm data volume, coverage, completeness; measure weather elasticity for each tenant
      domain: product
      exit_criteria:
      - artifact:state/analytics/synthetic_data_validation.json
      - artifact:docs/DATA_GENERATION.md
      - critic:data_quality
      id: T12.0.2
      status: pending
      title: Validate synthetic data quality and weather correlation
    - dependencies: []
      description: Create data dictionary with tenant profiles, weather sensitivity, expected model behaviors
      domain: product
      exit_criteria:
      - artifact:docs/SYNTHETIC_TENANTS.md
      - artifact:state/analytics/tenant_weather_profiles.json
      - critic:data_quality
      id: T12.0.3
      status: done
      title: Document synthetic tenant characteristics
    title: Synthetic Multi-Tenant Dataset Generation
  - id: M12.1
    status: pending
    tasks:
    - dependencies: []
      domain: product
      exit_criteria:
      - artifact:state/telemetry/weather_ingestion.json
      - critic:data_quality
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      - critic:causal
      id: T12.1.1
      status: pending
      title: Run smoke-context and weather ingestion regression suite nightly
    - dependencies: []
      domain: product
      exit_criteria:
      - artifact:experiments/weather/feature_backfill_report.md
      - critic:forecast_stitch
      - critic:tests
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      - critic:data_quality
      - critic:causal
      id: T12.1.2
      status: done
      title: Validate feature store joins against historical weather baselines
    title: Weather ingestion + feature QA
  - id: M12.2
    status: pending
    tasks:
    - dependencies: []
      domain: product
      exit_criteria:
      - artifact:experiments/weather/model_backtest_summary.md
      - critic:causal
      - critic:allocator
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      id: T12.2.1
      status: done
      title: Backtest weather-aware model vs control across top tenants
    - dependencies: []
      domain: product
      exit_criteria:
      - artifact:apps/web/pages/ops/weather-capabilities.tsx
      - critic:org_pm
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      - critic:causal
      id: T12.2.2
      status: pending
      title: Publish weather capability runbook and monitoring dashboards
    title: Weather model capability sign-off
  - id: M12.3
    status: pending
    tasks:
    - dependencies: []
      description: Train multi-channel MMM using validated 90-day data with weather features integrated
      domain: product
      exit_criteria:
      - artifact:experiments/mcp/mmm_weather_model.json
      - critic:causal
      - critic:academic_rigor
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      id: T12.3.1
      status: pending
      title: Train weather-aware MMM on validated 90-day tenant data
    - dependencies: []
      description: Quantify how demand elasticity varies by weather (temperature sensitivity, rain impact, seasonal patterns)
      domain: product
      exit_criteria:
      - artifact:docs/models/weather_elasticity_analysis.md
      - critic:causal
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      id: T12.3.2
      status: pending
      title: Implement weather sensitivity elasticity estimation
    - dependencies: []
      description: Deploy MMM with weather features as production inference service
      domain: product
      exit_criteria:
      - artifact:apps/api/routes/models/weather_mixin.py
      - artifact:apps/worker/models/mmm_weather_inference.py
      - critic:tests
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      - critic:causal
      id: T12.3.3
      status: pending
      title: Ship production MMM inference service with real-time weather scoring
    title: Weather-Aware MMM Model Training
  - id: M12.UXExcellence
    status: pending
    tasks:
    - dependencies:
      - T12.0.1
      description: "Configure screenshot_session, screenshot_capture_multiple tools for visual design iteration. Document\
        \ workflow for Build\u2192Screenshot\u2192Critique\u2192Refine loop. PRIORITY NOTE - Execute only after T12.0.1 (data\
        \ generation) completes."
      domain: product
      exit_criteria:
      - artifact:state/screenshot_config.yaml
      - artifact:docs/DESIGN_WORKFLOW.md
      - critic:tests
      id: T12.UX.1
      status: pending
      title: Setup Playwright screenshot workflow for design iteration
    - dependencies:
      - T12.0.1
      description: "Document process for researching award-winning UIs (Awwwards, FWA, Stripe, Linear, Observable). Create\
        \ reference library of world-class design patterns to inform WeatherVane UI decisions. Extract principles from Dieter\
        \ Rams, M\xFCller-Brockmann, Vignelli, Paul Rand."
      domain: product
      exit_criteria:
      - artifact:docs/DESIGN_INSPIRATION.md
      - artifact:state/design_references.json
      id: T12.UX.2
      status: pending
      title: Create design research process and inspiration library
    - dependencies:
      - T12.0.1
      description: Create checklist for trivial delights (micro-interactions, loading states, empty states, witty copy) and
        non-trivial delights (anticipatory UX, intelligent recovery, time-saving magic). Validation - Would user screenshot
        and share? Would Don Norman/Kathy Sierra/Julie Zhuo approve?
      domain: product
      exit_criteria:
      - artifact:docs/SURPRISE_DELIGHT_CHECKLIST.md
      - artifact:tools/wvo_mcp/src/critics/ux_delight_scorer.ts
      id: T12.UX.3
      status: pending
      title: Establish surprise & delight checklist and validation criteria
    - dependencies:
      - T12.UX.1
      - T12.UX.2
      - T12.UX.3
      description: Enhance design_system critic to detect generic gradients, stock layouts, template components. Enforce heritage
        design principles. Integrate with screenshot workflow to compare against world-class references. Block merge if AI
        aesthetic detected.
      domain: product
      exit_criteria:
      - artifact:tools/wvo_mcp/src/critics/design_system_enhanced.ts
      - critic:design_system
      id: T12.UX.4
      status: pending
      title: Configure design_system critic for zero-AI-aesthetic enforcement
    title: Design Excellence Infrastructure (Lower Priority - After Data Gen)
  - id: M12.Demo
    status: pending
    tasks:
    - dependencies:
      - T12.UX.1
      - T12.UX.2
      - T12.UX.3
      - T12.UX.4
      description: Create web UI that lets stakeholders toggle weather on/off and see impact on predicted revenue and ROAS
        for each synthetic tenant. Interactive proof that weather matters for demand forecasting. DESIGN EXCELLENCE REQUIRED
        - Follow M12.UXExcellence standards (Playwright iteration, world-class inspiration, surprise & delight, zero AI aesthetic).
        Use screenshot_session for visual QA. Validate with design_system critic.
      domain: product
      exit_criteria:
      - artifact:apps/web/src/pages/demo-weather-analysis.tsx
      - critic:design_system
      - artifact:state/artifacts/screenshots/demo_ui_iteration/
      id: T12.Demo.1
      notes: 'Auto-unblocked by CriticAvailabilityGuardian: Critics design_system are offline. Proceeding with implementation;
        gather QA evidence for eventual review. UPDATED - Now depends on M12.UXExcellence infrastructure.'
      status: pending
      title: Build interactive demo UI showing weather impact on ROAS
    - dependencies: []
      description: Record 5-min demo video showing weather-aware model in action. Create 1-page brief for executives explaining
        business impact (revenue upside, forecast accuracy improvement, ROAS optimization potential).
      domain: product
      exit_criteria:
      - artifact:docs/WEATHER_DEMO_BRIEF.md
      - artifact:state/artifacts/stakeholder/weather_demo_script.md
      id: T12.Demo.2
      status: pending
      title: Record demo video and create stakeholder brief
    title: Executive Demo & Stakeholder Sign-Off
  - id: M12.PoC
    status: pending
    tasks:
    - dependencies: []
      description: Train a baseline weather-aware regression model on each synthetic tenant (high/extreme/medium/none sensitivity)
        to validate the weather correlation detection works across different product types and sensitivity profiles.
      domain: product
      exit_criteria:
      - artifact:experiments/mcp/weather_poc_model.pkl
      - artifact:experiments/mcp/weather_poc_metrics.json
      - critic:academic_rigor
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      id: T12.PoC.1
      status: pending
      title: Train weather-aware model on synthetic tenant data
    - dependencies: []
      description: "Test PoC model on final 30 days of each synthetic tenant. Verify that: (1) High/Extreme tenants show strong\
        \ weather effects, (2) None tenant shows no weather effect, (3) Model R\xB2 > 0.6 on validation set"
      domain: product
      exit_criteria:
      - artifact:experiments/mcp/weather_poc_validation.json
      - critic:causal
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      id: T12.PoC.2
      status: pending
      title: Validate PoC model predictions on hold-out data
    - dependencies: []
      description: 'Create executive brief demonstrating that weather-aware modeling works: show before/after model performance,
        weather elasticity coefficients, and prediction examples. Use this to get stakeholder buy-in before full MMM training.'
      domain: product
      exit_criteria:
      - artifact:docs/WEATHER_PROOF_OF_CONCEPT.md
      - artifact:experiments/mcp/poc_demo_charts.json
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      id: T12.PoC.3
      status: done
      title: Create PoC demo results and proof brief
    title: Proof of Concept & Model Testing
  status: pending
  title: "Epic 12 \u2014 Weather Model Production Validation"
- description: Close the execution gap between sophisticated modeling plans and the current codebase, while embedding Autopilot
    self-critique so these regressions cannot hide in the future.
  domain: product
  id: E13
  milestones:
  - id: M13.1
    status: in_progress
    tasks:
    - dependencies: []
      description: Ensure the Shopify/ads/weather ingestion flows actually populate product_daily with geocoded spend and
        90+ days of history so MMM inputs are real, not theoretical.
      domain: product
      exit_criteria:
      - artifact:experiments/features/weather_join_validation.json
      - critic:data_quality passes with weather join metrics captured
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      - critic:data_quality
      - critic:causal
      id: T13.1.1
      status: done
      title: Validate 90-day tenant data coverage across sales, spend, and weather
    - dependencies: []
      description: Bake data completeness checks into Autopilot so future weather-awareness regressions trigger automated
        investigations.
      domain: product
      exit_criteria:
      - critic:weather_coverage autop-run nightly with failure escalation to Atlas
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      - critic:data_quality
      - critic:causal
      id: T13.1.2
      status: pending
      title: Autopilot guardrail for ingestion + weather drift
    - dependencies: []
      description: 'Auto-classify products from Shopify/Meta/Google using LLM (Claude/GPT-4) to tag products with weather
        affinity and category hierarchy. This enables product-level modeling (not just brand-level) and cold-start for new
        brands.

        '
      domain: product
      exit_criteria:
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      - critic:data_quality
      - critic:causal
      id: T13.1.3
      status: done
      title: Implement product taxonomy auto-classification with weather affinity
    - dependencies: []
      description: 'Implement data quality checks to verify data is ready for ML training. Prevents training models on insufficient/corrupted
        data.

        '
      domain: product
      exit_criteria:
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      - critic:causal
      id: T13.1.4
      status: done
      title: Data quality validation framework (verify data fitness for ML)
    title: Data Backbone Verified
  - id: M13.2
    status: pending
    tasks:
    - dependencies: []
      description: Integrate the existing LightweightMMM wrapper so allocations use Bayesian adstock/saturation estimates
        instead of covariance heuristics.
      domain: product
      exit_criteria:
      - critic:model_fit passes with synthetic recovery tests
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      - critic:causal
      id: T13.2.1
      status: done
      title: Replace heuristic MMM with LightweightMMM adstock+saturation fit
    - dependencies: []
      description: Establish out-of-sample evaluation so MMM recommendations are validated continuously.
      domain: product
      exit_criteria:
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      - critic:causal
      id: T13.2.2
      status: pending
      title: Build MMM backtesting + regression suite
    - dependencies: []
      description: "Replace heuristic allocation rules (\xB110% budget adjustments) with proper constrained optimization.\
        \ Use cvxpy or OR-Tools to maximize ROAS subject to budget, inventory, and platform constraints.\n"
      domain: product
      exit_criteria:
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      - critic:causal
      id: T13.2.3
      status: done
      title: Replace heuristic allocator with constraint-aware optimizer
    title: MMM Upgrade & Backtests
  - id: M13.3
    status: pending
    tasks:
    - dependencies: []
      description: Adopt causal estimators appropriate for non-manipulable treatments so weather impact claims are statistically
        defensible.
      domain: product
      exit_criteria:
      - critic:causal passes with new methodology notes in docs/CAUSAL_LIMITATIONS.md
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      - critic:causal
      id: T13.3.1
      status: pending
      title: Swap uplift propensity scoring with DID/synthetic control for weather shocks
    - dependencies: []
      description: Resolve the open question on geographic granularity by codifying DMA-first modeling with automatic fallback.
      domain: product
      exit_criteria:
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      - critic:causal
      id: T13.3.2
      status: done
      title: Implement DMA-first geographic aggregation with hierarchical fallback
    title: Causal & Geography Alignment
  - id: M13.4
    status: pending
    tasks:
    - dependencies: []
      description: Teach Autopilot to generate the sort of gap analysis we just performed so future discrepancies surface
        automatically.
      domain: product
      exit_criteria:
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      id: T13.4.1
      status: done
      title: Add modeling reality critic to Autopilot
    - dependencies: []
      description: "Provide a repeatable process\u2014documentation, scheduling, and telemetry\u2014for leadership to review\
        \ modeling execution against strategy."
      domain: product
      exit_criteria:
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      id: T13.4.2
      status: pending
      title: Meta-evaluation playbook for modeling roadmap
    title: Autopilot Meta-Critique Loop
  - id: M13.5
    status: pending
    tasks:
    - dependencies: []
      description: Build allocation optimization model that incorporates weather-driven demand elasticity from MMM training
      domain: product
      exit_criteria:
      - artifact:experiments/allocation/weather_aware_model.json
      - critic:allocator
      - critic:causal
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      id: T13.5.1
      status: done
      title: Train weather-aware allocation model on top of MMM baseline
    - dependencies: []
      description: Add constraints that adjust budget allocation based on weather forecasts (e.g., reduce spend on low-demand
        weather days)
      domain: product
      exit_criteria:
      - artifact:experiments/allocation/constraint_validation.json
      - critic:tests
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      - critic:causal
      id: T13.5.2
      status: pending
      title: Implement weather-responsive budget allocation constraints
    - dependencies: []
      description: Ship weather-aware allocation as the primary recommendation engine
      domain: product
      exit_criteria:
      - artifact:apps/api/routes/allocate.py updated with weather model
      - artifact:apps/worker/allocation_service.py deployed
      - critic:tests
      - critic:allocator
      - metric:r2 > 0.50
      - metric:beats_baseline > 1.10
      - critic:modeling_reality_v2
      id: T13.5.3
      status: pending
      title: Deploy weather-aware allocator to production
    title: Weather-Aware Allocation Model Deployment
  status: pending
  title: "Epic 13 \u2014 Weather-Aware Modeling Reality"
- description: Ship lagged features, baseline models, and evaluation harness.
  domain: product
  id: E2
  milestones:
  - id: M2.1
    status: done
    tasks:
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:build
      - critic:tests
      - critic:data_quality
      id: T2.1.1
      status: done
      title: Build lag/rolling feature generators with deterministic seeds
    title: Feature pipeline
  - id: M2.2
    status: done
    tasks:
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:causal
      - critic:academic_rigor
      - doc:docs/models/baseline.md
      id: T2.2.1
      status: done
      title: Train weather-aware GAM baseline and document methodology
    title: Baseline modeling
  status: done
  title: "Epic 2 \u2014 Features & Modeling Baseline"
- description: Allocator robustness checks, dashboards, and UI polish.
  domain: product
  id: E3
  milestones:
  - id: M3.1
    status: done
    tasks:
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:allocator
      - critic:cost_perf
      - artifact:experiments/policy/regret.json
      id: T3.1.1
      status: done
      title: Implement budget allocator stress tests and regret bounds
    title: Allocator guardrails
  - id: M3.2
    status: done
    tasks:
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:design_system
      id: T3.2.1
      status: done
      title: Run design system critic and ensure accessibility coverage
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:design_system
      - critic:exec_review
      - doc:docs/UX_CRITIQUE.md
      - artifact:docs/product/UX_CRITIQUE.md
      id: T3.2.2
      status: done
      title: Elevate dashboard storytelling & UX
    title: Dashboard + UX review
  - id: M3.3
    status: done
    tasks:
    - dependencies: []
      domain: product
      exit_criteria:
      - artifact:docs/orchestration/multi_agent_charter.md
      - critic:manager_self_check
      - critic:org_pm
      id: T3.3.1
      status: done
      title: Draft multi-agent charter & delegation mesh (AutoGen/Swarm patterns)
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:integration_fury
      - critic:manager_self_check
      - doc:docs/orchestration/consensus_engine.md
      id: T3.3.2
      status: done
      title: Implement hierarchical consensus & escalation engine
    - dependencies: []
      domain: product
      exit_criteria:
      - artifact:experiments/orchestration/simulation_report.md
      - critic:tests
      - critic:health_check
      id: T3.3.3
      status: done
      title: Build closed-loop simulation harness for autonomous teams
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:prompt_budget
      - critic:exec_review
      - artifact:state/analytics/orchestration_metrics.json
      id: T3.3.4
      status: done
      title: Instrument dynamic staffing telemetry & learning pipeline
    title: Autonomous Orchestration Blueprints
  - id: M3.4
    status: done
    tasks:
    - dependencies: []
      domain: product
      exit_criteria:
      - artifact:apps/web/pages/plan.tsx
      - unknown
      - critic:design_system
      - unknown
      id: T3.4.1
      status: done
      title: Implement Plan overview page with weather-driven insights
    - dependencies: []
      domain: product
      exit_criteria:
      - artifact:apps/web/pages/dashboard.tsx
      - unknown
      - critic:design_system
      - unknown
      id: T3.4.2
      status: done
      title: Build WeatherOps dashboard with allocator + weather KPIs
    - dependencies: []
      domain: product
      exit_criteria:
      - artifact:apps/web/pages/experiments.tsx
      - unknown
      - critic:design_system
      - unknown
      id: T3.4.3
      status: done
      title: Ship Experiments hub UI for uplift & incrementality reviews
    - dependencies: []
      domain: product
      exit_criteria:
      - artifact:apps/web/pages/reports.tsx
      - unknown
      - critic:design_system
      - unknown
      id: T3.4.4
      status: done
      title: Deliver storytelling Reports view with weather + spend narratives
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:design_system
      - doc:docs/product/acceptance_report.md
      - artifact:state/critics/designsystem.json
      id: T3.4.5
      status: done
      title: Conduct design_system + UX acceptance review across implemented pages
    - dependencies: []
      description: "Requirements:\n  - Show operators exactly what WeatherVane changed or recommends changing, with clear\
        \ \u201Cwhat/why/next\u201D messaging.\n  - Remove jargon (\u201Cguardrail\u201D, \u201Ctriage\u201D) in favour of\
        \ user-facing language (e.g., \u201COverspend alert\u201D, \u201CWeather action\u201D).\n  - Keep first view scannable:\
        \ one hero recommendation, secondary cards, optional detail drill-down.\nStandards:\n  - Copy: conversational, action-oriented,\
        \ no internal terminology.\n  - UX: minimalist layout, responsive to desktop/tablet, accessible (WCAG AA).\n  - Engineering:\
        \ Playwright smoke must pass; analytics instrumentation preserved; Vitest coverage for helpers.\nImplementation Plan:\n\
        \  - Draft/record design brief in docs/UX_CRITIQUE.md.\n  - Refactor hero + summary components around new copy and\
        \ layout.\n  - Update analytics helpers/tests, run Vitest + Playwright.\n  - Capture iteration in state/context.md\
        \ with screenshots and critic notes.\nDeliverables:\n  - Updated React/CSS modules under apps/web/src/pages/dashboard.tsx\
        \ and styles.\n  - Revised helper libraries/tests (apps/web/src/lib/**, tests/web/**).\n  - Playwright report + screenshots\
        \ stored under state/artifacts/ui/weatherops.\nIntegration Points:\n  - API suggestion telemetry (shared/services/dashboard_analytics_ingestion.py)\
        \ ensuring copy aligns with payload fields.\n  - Analytics events (`trackDashboardEvent`) and downstream dashboards;\
        \ coordinate with data/ML owners if field names change.\n  - Worker-generated suggestion summaries (apps/worker/flows/poc_pipeline.py)\
        \ to maintain consistency across channels.\nEvidence:\n  - Playwright run ID + html report.\n  - design_system + exec_review\
        \ critic outputs (once available).\n  - Context entry summarising decisions, open questions, next iteration."
      domain: product
      exit_criteria:
      - unknown
      - unknown
      - critic:design_system
      - critic:exec_review
      - unknown
      id: T3.4.6
      status: done
      title: Rewrite WeatherOps dashboard around plain-language decisions
    - dependencies: []
      description: "Requirements:\n  - Explain every autonomous change in plain language (what changed, when, why, impact).\n\
        \  - Provide explicit approval/rollback affordances for humans and highlight pending reviews.\n  - Surface evidence\
        \ (metrics, weather context, spend forecasts) inline or a click away.\nStandards:\n  - Copy: transparent, confidence-building,\
        \ avoids \u201Caudit/guardrail\u201D jargon.\n  - UX: timeline or table must prioritise newest changes, support filtering;\
        \ accessible controls for approvals.\n  - Engineering: tests updated (Vitest, Playwright), telemetry preserved, change\
        \ log data schema documented.\n  - ML context (if applicable): explain model confidence/reason codes clearly.\nImplementation\
        \ Plan:\n  - Extend docs/UX_CRITIQUE.md with Automations brief, list user questions + acceptance metrics.\n  - Redesign\
        \ components/layout in apps/web/src/pages/automations.tsx; integrate evidence panels.\n  - Update helpers/tests, run\
        \ Vitest + Playwright, capture critics.\n  - Log iterations in state/context.md with before/after screenshots and\
        \ open questions.\nDeliverables:\n  - Updated Automations page/components/styles.\n  - Supporting helper modules/tests\
        \ (automationInsights, validation, etc.).\n  - Evidence artifacts (Playwright report, screenshots, context notes).\n\
        Integration Points:\n  - Automation audit APIs/events (apps/api/services/dashboard_service.py, shared schemas) so\
        \ reason codes remain synchronized.\n  - Worker automation execution logs (`apps/worker/flows/**`) and telemetry exports\
        \ consumed by directors/Dana.\n  - Notification channels or forthcoming approval workflows (e.g., Slack/email) to\
        \ ensure new statuses map correctly.\nEvidence:\n  - Playwright run + report stored under state/artifacts/ui/automations.\n\
        \  - design_system + exec_review critic confirmation.\n  - Context log summarising decisions, trade-offs, next steps."
      domain: product
      exit_criteria:
      - unknown
      - unknown
      - critic:design_system
      - critic:exec_review
      - unknown
      id: T3.4.7
      status: done
      title: Reimagine Automations change log as a trust-first narrative
    title: Experience Implementation
  status: done
  title: "Epic 3 \u2014 Allocation & UX"
- description: Maintain velocity while hardening performance and delivery processes.
  domain: product
  id: E4
  milestones:
  - id: M4.1
    status: done
    tasks:
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:allocator
      - artifact:experiments/allocator/saturation_report.json
      id: T4.1.10
      status: done
      title: Cross-market saturation optimization (fairness-aware)
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:causal
      - artifact:experiments/causal/uplift_report.json
      id: T4.1.3
      status: done
      title: Causal uplift modeling & incremental lift validation
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:forecast_stitch
      - artifact:experiments/forecast/ensemble_metrics.json
      id: T4.1.4
      status: done
      title: Multi-horizon ensemble forecasting
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:allocator
      - unknown
      id: T4.1.5
      status: done
      title: Non-linear allocation optimizer with constraints (ROAS, spend caps)
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:allocator
      - artifact:experiments/allocator/hf_response.json
      id: T4.1.6
      status: done
      title: High-frequency spend response modeling (intraday adjustments)
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:allocator
      - unknown
      id: T4.1.7
      status: done
      title: Marketing mix budget solver (multi-channel, weather-aware)
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:allocator
      - artifact:experiments/rl/shadow_mode.json
      id: T4.1.8
      status: done
      title: Reinforcement-learning shadow mode (safe exploration)
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:design_system
      - artifact:experiments/creative/response_scores.json
      id: T4.1.9
      status: done
      title: Creative-level response modeling with brand safety guardrails
    title: Optimization sprint
  status: done
  title: "Epic 4 \u2014 Operational Excellence"
- description: Enable WeatherVane to programmatically create, update, monitor, and rollback ads across major platforms.
  domain: product
  id: E5
  milestones:
  - id: M5.1
    status: done
    tasks:
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:allocator
      - unknown
      id: T5.1.1
      status: done
      title: Implement Meta Marketing API client (creative + campaign management)
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:security
      - artifact:experiments/meta/sandbox_run.json
      id: T5.1.2
      status: done
      title: Meta sandbox and dry-run executor with credential vaulting
    title: Meta Ads Command Pipeline
  - id: M5.2
    status: done
    tasks:
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:allocator
      - unknown
      id: T5.2.1
      status: done
      title: Google Ads API integration (campaign create/update, shared budgets)
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:allocator
      - artifact:experiments/allocator/spend_guardrails.json
      id: T5.2.2
      status: done
      title: Budget reconciliation & spend guardrails across platforms
    title: Google Ads Execution & Budget Sync
  - id: M5.3
    status: done
    tasks:
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:tests
      - artifact:state/ad_push_diffs.json
      id: T5.3.1
      status: done
      title: Dry-run & diff visualizer for ad pushes (pre-flight checks)
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:manager_self_check
      - artifact:experiments/allocator/rollback_sim.json
      id: T5.3.2
      status: done
      title: Automated rollback + alerting when performance/regression detected
    title: QA, Rollback & Safety Harness
  status: done
  title: Ad Platform Execution & Automation
- description: Validate and harden the dual-provider MCP orchestrator for autonomous operation while preserving 100% run safety.
    All milestones under this epic must enforce the blue/green upgrade guardrails defined in docs/MCP_ORCHESTRATOR.md#1113-tight-integration-playbook-steps-0-15.
  domain: mcp
  id: E6
  milestones:
  - id: M6.1
    status: done
    tasks:
    - dependencies: []
      domain: mcp
      exit_criteria:
      - critic:tests
      - artifact:tests/test_mcp_tools.py
      - 'Guardrail: integration suite enforces blue/green safety invariants (no unhandled throws, DRY_RUN parity)'
      id: T6.1.1
      status: done
      title: MCP server integration tests (all 25 tools across both providers)
    - dependencies: []
      domain: mcp
      exit_criteria:
      - critic:manager_self_check
      - artifact:experiments/mcp/failover_test.json
      - 'Guardrail: circuit-breaker rollback and DISABLE_NEW kill switch verified under simulated failures'
      id: T6.1.2
      status: done
      title: Provider failover testing (token limit simulation & automatic switching)
    - dependencies: []
      domain: mcp
      exit_criteria:
      - critic:tests
      - artifact:tests/test_state_persistence.py
      - 'Guardrail: recovery flow preserves upgrade locks and safety state without manual intervention'
      id: T6.1.3
      status: done
      title: State persistence testing (checkpoint recovery across sessions)
    - dependencies: []
      domain: mcp
      exit_criteria:
      - critic:manager_self_check
      - artifact:state/quality/assessment_log.json
      - 'Guardrail: quality checks confirm run-safety metrics from blue/green playbook remain green'
      id: T6.1.4
      status: done
      title: Quality framework validation (10 dimensions operational)
    title: Core Infrastructure Validation
  - id: M6.2
    status: done
    tasks:
    - dependencies: []
      domain: mcp
      exit_criteria:
      - critic:security
      - doc:docs/SECURITY_AUDIT.md
      - 'Guardrail: audit verifies secrets handling inside blue/green upgrade flow and DRY_RUN constraints'
      id: T6.2.1
      status: done
      title: Credentials security audit (auth.json, API keys, token rotation)
    - dependencies: []
      domain: mcp
      exit_criteria:
      - critic:tests
      - artifact:experiments/mcp/error_recovery.json
      - 'Guardrail: automated rollback path exercised with observation window + DISABLE_NEW reset'
      id: T6.2.2
      status: done
      title: Error recovery testing (graceful degradation, retry logic)
    - dependencies: []
      domain: mcp
      exit_criteria:
      - critic:data_quality
      - artifact:shared/contracts/*.schema.json
      - 'Guardrail: dual-write / expand-cutover-contract workflow logged for 100% safe migrations'
      id: T6.2.3
      status: done
      title: Schema validation enforcement (all data contracts validated)
    - dependencies: []
      domain: mcp
      exit_criteria:
      - critic:allocator
      - unknown
      - 'Guardrail: rate-limit handling respects worker timeouts and prevents cascading failures during upgrades'
      id: T6.2.4
      status: done
      title: API rate limiting & exponential backoff (Open-Meteo, Shopify, Ads APIs)
    title: Security & Reliability Hardening
  - id: M6.3
    status: done
    tasks:
    - dependencies: []
      domain: mcp
      exit_criteria:
      - critic:cost_perf
      - artifact:experiments/mcp/performance_benchmarks.json
      - 'Guardrail: benchmarks include worker swap scenarios and confirm resource limits (timeouts, RSS) hold'
      id: T6.3.1
      status: done
      title: Performance benchmarking (MCP overhead, checkpoint size, token usage)
    - dependencies: []
      domain: mcp
      exit_criteria:
      - critic:manager_self_check
      - artifact:state/telemetry/metrics_summary.json
      - "Guardrail: telemetry captures Step 0\u201315 safety signals with alerting on breaches"
      id: T6.3.2
      status: done
      title: Enhanced observability export (structured logs, metrics dashboards)
    - dependencies: []
      domain: mcp
      exit_criteria:
      - critic:manager_self_check
      - artifact:experiments/mcp/autopilot_e2e.json
      - 'Guardrail: autonomous loop validates automatic promotion + rollback without manual resets'
      id: T6.3.3
      status: done
      title: Autopilot loop end-to-end testing (full autonomous cycle validation)
    title: Observability & Performance
  - id: M6.4
    status: pending
    tasks:
    - dependencies: []
      description: "Define upgrade preflight: clean git, version sanity, \u2265500MB disk, SQLite lock probe, and single-flight\
        \ upgrade.lock. Gate promotion through build \u2192 unit \u2192 selfchecks \u2192 canary, aborting with {error:\"\
        upgrade_aborted\"} on any failure."
      domain: mcp
      exit_criteria:
      - state/upgrade.lock created before work and removed on exit
      - "Preflight script validates git status, Node/npm versions, disk \u2265500MB, sandbox availability"
      - Four-step gate recorded in logs; any failure returns {error:"upgrade_aborted"}
      id: T6.4.0
      status: done
      title: Upgrade invariants & preflight guardrails
    - dependencies: []
      description: "Replace environment toggles with a SQLite-backed `settings` table, seed\ndefaults, and hot-refresh cached\
        \ flags (\u2264500 ms poll). Include a\n`DISABLE_NEW` global kill switch that forces legacy behaviour instantly.\n"
      domain: mcp
      exit_criteria:
      - settings table created with defaults + DISABLE_NEW
      - LiveFlags poller refreshes in-memory cache during runtime
      - Integration test flips PROMPT_MODE without restart
      id: T6.4.1
      status: done
      title: Live feature flag store with kill switch
    - dependencies: []
      description: 'Keep the MCP front-end process stable while managing active and canary

        worker children over IPC. Ensure requests route through a proxy that can

        atomically switch to the validated canary without disconnecting clients.

        '
      domain: mcp
      exit_criteria:
      - WorkerManager exposes startActive/startCanary/switchToCanary
      - Front-end tool handlers call workers.getActive().call(...)
      - RPC protocol enforces ready handshake, 30s timeouts, and structured {ok,error} results
      - Test demonstrates zero-downtime swap between worker binaries
      id: T6.4.2
      status: done
      title: Blue/green worker manager & front-end proxy
    - dependencies: []
      description: 'Implement a dedicated worker entry that routes RPCs, enforces DRY_RUN=1

        by opening the state DB read-only, refuses mutating calls, and confirms

        legacy behaviour when DRY_RUN=0.

        '
      domain: mcp
      exit_criteria:
      - Route function covers health/plan/dispatch/runTool/verify/report.mo
      - SQLite opened via file:state/state.db?mode=ro when DRY_RUN=1
      - applyPatch/mutate operations rejected while DRY_RUN=1
      - tests/test_worker_dry_run.py captures read-only guarantees
      id: T6.4.3
      status: done
      title: Worker entrypoint with DRY_RUN safeguards
    - dependencies: []
      description: 'Automate the upgrade flow: create a separate git worktree, build/test new

        code, spawn a DRY_RUN canary, run shadow health/plan/report checks, then

        promote only if outputs match expectations.

        '
      domain: mcp
      exit_criteria:
      - scripts/mcp_safe_upgrade.sh orchestrates worktree build + tests
      - Shadow checks compare active vs canary outputs in logs
      - "Promotion flow documents gate order and staged routing (DRY \u2192 live) with metrics snapshots"
      - experiments/mcp/upgrade/<ts>/report.json recorded for each run
      id: T6.4.4
      status: done
      title: Canary upgrade harness & shadow validation
    - dependencies: []
      description: 'Gate compact prompt headers, sandbox pooling, scheduler WSJF mode,

        selective tests, danger gates, and MO engine behind live flags so they

        only activate after successful canary validation.

        '
      domain: mcp
      exit_criteria:
      - PROMPT_MODE, SANDBOX_MODE, SCHEDULER_MODE, SELECTIVE_TESTS, DANGER_GATES, MO_ENGINE read from LiveFlags
      - Regression fixtures cover legacy vs new mode per feature
      - docs/MCP_ORCHESTRATOR.md updated with flag toggle order
      id: T6.4.5
      status: pending
      title: Feature flag gating for compact prompts & sandbox pool
    - dependencies: []
      description: 'Ensure tool surfaces remain stable while routing to v1/v2 handlers based

        on flags. Provide an MCP admin tool or CLI to update settings atomically

        without restarts.

        '
      domain: mcp
      exit_criteria:
      - Tool handlers return 'disabled' until corresponding flag enabled
      - settings.update, upgrade.applyPatch, route.switch commands exposed with structured errors
      - Operator guide added under docs/MCP_AUTOMATION.md#live-flags
      id: T6.4.6
      status: pending
      title: Runtime tool registration & admin flag controls
    - dependencies: []
      description: 'Add health monitoring that reverts to the previous worker and resets

        flags when error rates spike post-promotion. Document on-call rollback

        steps and ensure DISABLE_NEW restores legacy behaviour.

        '
      domain: mcp
      exit_criteria:
      - Heartbeat every 2s with 3-strike circuit breaker routes back to standby
      - Error budget (5%/2min) and SLO monitors trigger automatic rollback
      - DISABLE_NEW flag automatically flipped during rollback
      - docs/MCP_ORCHESTRATOR.md includes rollback playbook
      id: T6.4.7
      status: done
      title: Automatic rollback monitors & kill-switch reset
    - dependencies: []
      description: 'Emit OTel spans (or structured JSON logs) for every worker call with

        timing, lane, task, and outcome metadata. Enforce concurrency, timeout,

        and RSS guards to prevent runaway resource usage.

        '
      domain: mcp
      exit_criteria:
      - Span/log attributes include method, lane, ok/error, duration, task.id
      - runTool/plan timeouts (30s/120s) & lane concurrency limits enforced
      - RSS watchdog throttles batch lane when >1.5x baseline
      id: T6.4.8
      status: done
      title: Observability & resource budgets during upgrade
    title: Zero-downtime self-upgrade
  status: done
  title: MCP Orchestrator Production Readiness
- description: Complete geocoding integration, weather feature joins, and data quality validation.
  domain: product
  id: E7
  milestones:
  - id: M7.1
    status: done
    tasks:
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:data_quality
      - unknown
      id: T7.1.1
      status: done
      title: Complete geocoding integration (city->lat/lon, cache strategy)
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:leakage
      - artifact:experiments/features/weather_join_validation.json
      id: T7.1.2
      status: done
      title: Weather feature join to model matrix (prevent future leakage)
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:data_quality
      - unknown
      id: T7.1.3
      status: done
      title: Data contract schema validation (Shopify, weather, ads)
    title: Geocoding & Weather Integration
  - id: M7.2
    status: done
    tasks:
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:data_quality
      - unknown
      id: T7.2.1
      status: done
      title: Incremental ingestion with deduplication & checkpointing
    - dependencies: []
      domain: product
      exit_criteria:
      - critic:data_quality
      - artifact:state/dq_monitoring.json
      id: T7.2.2
      status: done
      title: Data quality monitoring & alerting (anomaly detection)
    title: Pipeline Robustness
  status: done
  title: Data Pipeline Hardening
- description: "Critical production readiness tasks for MCP orchestrator. Complete before WeatherVane v1 launch while maintaining\
    \ the Step 0\u201315 run-safety guardrails (docs/MCP_ORCHESTRATOR.md#1113-tight-integration-playbook-steps-0-15)."
  domain: mcp
  id: E8
  milestones:
  - id: M8.1
    status: done
    tasks:
    - dependencies: []
      domain: mcp
      exit_criteria:
      - utils/schema.ts returns schema.shape with guardrail comment
      - MCP entrypoints register raw shapes only
      - Autopilot documentation updated to reflect guardrail
      - critic:build passes
      - 'Guardrail: validation confirms schema handling does not weaken blue/green safety gates'
      id: T8.1.1
      status: done
      title: 'Lock MCP schemas to Zod shapes (SAFE: guardrail)'
    - dependencies: []
      domain: mcp
      exit_criteria:
      - ALLOWED_COMMANDS constant defined
      - isCommandAllowed() enforced before execution
      - Deny-list kept as secondary check
      - critic:tests passes with new test_command_allowlist.py
      - critic:manager_self_check passes
      - 'Guardrail: allow-list integration verified against blue/green upgrade scenarios'
      id: T8.1.2
      status: done
      title: 'Implement command allow-list in guardrails (SAFE: additive security)'
    - dependencies: []
      domain: mcp
      exit_criteria:
      - All tool handlers generate correlationId
      - All state transitions include correlationId
      - Events in SQLite include correlation_id column populated
      - critic:manager_self_check passes
      - End-to-end trace visible in state/orchestrator.db
      - "Guardrail: correlation IDs trace compliance with Step 0\u201315 safety checks"
      id: T8.1.3
      status: done
      title: 'Thread correlation IDs through state transitions (SAFE: observability only)'
    title: MCP Compliance & Security
  - id: M8.2
    status: done
    tasks:
    - dependencies: []
      domain: mcp
      exit_criteria:
      - formatForPromptCompact() returns JSON evidence pack
      - unknown
      - All coordinator calls use compact mode
      - critic:build passes
      - critic:manager_self_check passes
      - unknown
      - 'Guardrail: compact mode flip integrated with Step 15 staged flag process'
      id: T8.2.1
      status: done
      title: 'Implement compact evidence-pack prompt mode (SAFE: new function, backward compatible)'
    - dependencies: []
      domain: mcp
      exit_criteria:
      - orchestrator_status tool shows coordinator type and availability
      - Telemetry includes coordinator field in execution logs
      - Documentation updated in IMPLEMENTATION_STATUS.md
      - critic:manager_self_check passes
      - Failover behavior visible and logged
      - 'Guardrail: failover reporting feeds SLO/error budget monitors for auto rollback'
      id: T8.2.2
      status: done
      title: "Finalize Claude\u2194Codex coordinator failover (SAFE: expose existing functionality)"
    title: Context & Performance Optimization
  status: done
  title: "PHASE-4-POLISH \u2014 MCP Production Hardening"
- description: Post-v1 performance improvements and production observability. High ROI optimizations that still honour the
    blue/green guardrail contract.
  domain: mcp
  id: E9
  milestones:
  - id: M9.1
    status: done
    tasks:
    - dependencies: []
      domain: mcp
      exit_criteria:
      - standardPromptHeader() returns deterministic header
      - All prompts include standard header
      - Header enables provider caching (verified with API logs)
      - critic:cost_perf shows token cache hit rate
      - critic:manager_self_check passes
      - "Guardrail: caching rollout assessed via Step 0\u201315 safety checks before staying live"
      id: T9.1.1
      status: done
      title: 'Stable prompt headers with provider caching (SAFE: additive optimization)'
    - dependencies: []
      domain: mcp
      exit_criteria:
      - Priority queue with 3 lanes operational
      - Semaphore limits enforced per lane
      - Interactive tasks always get priority
      - critic:tests passes
      - critic:manager_self_check passes
      - 'Guardrail: queue respects worker concurrency caps from blue/green playbook'
      id: T9.1.2
      status: done
      title: 'Batch queue for non-urgent prompts (SAFE: new queueing system)'
    title: Cost Optimization & Caching
  - id: M9.2
    status: pending
    tasks:
    - dependencies: []
      domain: mcp
      exit_criteria:
      - validateDiff() rejects non-diff outputs
      - validateJSON() rejects invalid JSON
      - Retry rate reduction measured
      - critic:tests passes
      - 'Guardrail: validation enforced in canary shadow runs before live promotion'
      id: T9.2.1
      status: done
      title: 'Strict output DSL validation (SAFE: validation layer only)'
    - dependencies: []
      domain: mcp
      exit_criteria:
      - Idempotency cache operational
      - Duplicate operations return cached results
      - 1-hour TTL enforced
      - critic:tests passes
      - 'Guardrail: cache respects DRY_RUN mode and avoids side effects during canary runs'
      id: T9.2.2
      status: done
      title: 'Idempotency keys for mutating tools (SAFE: caching layer)'
    title: Reliability & Quality Improvements
  - id: M9.3
    status: pending
    tasks:
    - dependencies: []
      domain: mcp
      exit_criteria:
      - All tool handlers instrumented
      - Spans exported to tracing backend
      - End-to-end traces visible
      - Performance insights available
      - critic:manager_self_check passes
      - "Guardrail: telemetry alerts on Step 0\u201315 safety breaches"
      id: T9.3.1
      status: done
      title: 'OpenTelemetry spans for all operations (SAFE: tracing wrapper)'
    - dependencies: []
      domain: mcp
      exit_criteria:
      - Sandbox pool with 3 pre-warmed containers
      - Test execution uses pooled sandboxes
      - 10x speedup measured
      - Fallback to non-pooled works
      - critic:tests passes
      - 'Guardrail: pool enforces DRY_RUN read-only mode during canary validation'
      id: T9.3.2
      status: pending
      title: 'Sandbox pooling for test execution (SAFE: new executor)'
    title: Production Observability
  - id: M9.4
    status: pending
    tasks:
    - dependencies: []
      domain: mcp
      exit_criteria:
      - code_fts virtual table created
      - Index populated on repo sync
      - Search performance <50ms
      - critic:tests passes
      id: T9.4.1
      status: done
      title: 'SQLite FTS5 index for code search (SAFE: new index)'
    - dependencies: []
      domain: mcp
      exit_criteria:
      - tsserver and pyright proxies running
      - lsp.definition and lsp.references tools work
      - Context assembler uses LSP for code slices
      - Context relevance measured and improved
      - critic:tests passes
      - "Guardrail: LSP tools routed through worker proxy with Step 0\u201315 safety enforcement"
      id: T9.4.2
      status: pending
      title: 'LSP proxy tools for symbol-aware context (SAFE: new tools)'
    title: Advanced Context & Search
  status: pending
  title: "PHASE-5-OPTIMIZATION \u2014 Performance & Observability"
