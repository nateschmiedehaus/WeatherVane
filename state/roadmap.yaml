schema_version: '2.0'
epics:
- id: E-AUTOPILOT-GUARDRAILS
  title: Autopilot Guardrails Completion
  status: pending
  domain: product
  milestones:
  - id: E-AUTOPILOT-PHASE-NEG1
    title: Phase -1 Enforcement Completion
    status: pending
    tasks:
    - id: AT-GUARD-STRATEGIZE
      title: 'STRATEGIZE: Define enforcement scope and link to Autopilot functionality'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Purpose, impacted Autopilot workflows, and success metrics documented
          in strategy artifact
      domain: product
      description: 'Capture the strategic framing for Phase -1: which Autopilot behaviours
        (state transitions, tool executions, evidence gates) must be protected, why
        enforcement failed previously, and what success looks like. Produce a strategy.md
        artifact and ledger entry.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/AT-GUARD-STRATEGIZE
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: META-QUALITY-GRAPH
      title: 'META: Ensure quality graph integration is enforced'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Analysis of quality graph bootstrap, live flag defaults, and telemetry
          gaps documented with remediation owners.
      - test: node tools/wvo_mcp/scripts/check_quality_graph.ts --workspace-root .
        expect: exit 0
      - file: state/quality_graph/task_vectors.jsonl
        expect: exists
      domain: mcp
      description: Diagnose why the quality graph memory has remained disabled (missing
        Python env bootstrap, flags left off, telemetry directories absent) and integrate
        the fixes into the Autopilot loop so hints and recordings are always available.
        Deliverables include an enforced bootstrap check, sane live-flag defaults,
        CI guardrails, and updated documentation.
      complexity_score: 5
      effort_hours: 3
      required_tools:
      - cmd_run
      - fs_read
      - fs_write
      evidence_path: state/evidence/META-QUALITY-GRAPH
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: META-POLICY-05
      title: Enforce automatic follow-up handling for Codex loops
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: CLAUDE.md, AGENTS.md, and WORK_PROCESS docs updated with follow-up
          enforcement policy
      - test: node tools/wvo_mcp/scripts/classify_follow_ups.ts --enforce
        expect: exit 0
      - prose: CLI emits JSON report and blocks unresolved follow-ups in CI
      - file: state/automation/follow_up_report.json
        expect: exists
      - prose: Example Codex session evidence demonstrates auto-created roadmap task
          for follow-up bullet
      domain: mcp
      description: "Close the gap between policy and practice by ensuring Codex agents\
        \ resolve or auto-task all follow-up bullets before closing STRATEGIZE\u2192\
        MONITOR loops. Includes meta-policy wording, classifier enforcement upgrades,\
        \ CI integration, and roadmap auto-task wiring consistent with Claude parity.\n"
      complexity_score: 6
      effort_hours: 3
      required_tools:
      - fs_read
      - fs_write
      - cmd_run
      evidence_path: state/evidence/META-POLICY-05
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: AT-GUARD-SPEC
      title: 'SPEC: Evidence-gated enforcement requirements'
      status: pending
      dependencies:
        depends_on:
        - AT-GUARD-STRATEGIZE
      exit_criteria:
      - prose: Acceptance criteria for enforcement, evidence collection, ledger, prompt
          attestation, and metrics defined and approved
      domain: product
      description: 'Write spec.md enumerating acceptance criteria for WorkProcessEnforcer
        wiring, phase ledger persistence, prompt signature checks, artifact validation,
        leases, and integrity script expectations.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/AT-GUARD-SPEC
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: AT-GUARD-PLAN
      title: 'PLAN: Implementation breakdown for Phase -1'
      status: pending
      dependencies:
        depends_on:
        - AT-GUARD-SPEC
      exit_criteria:
      - prose: Plan.md lists work items, owners, estimates, dependencies, and verification
          strategy
      domain: product
      description: "Create plan.md with sequenced tasks (code, tests, telemetry, docs)\
        \ required to ship Phase -1 enforcement and to gather Strategy\u2192Monitor\
        \ evidence.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/AT-GUARD-PLAN
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: AT-GUARD-THINK
      title: 'THINK: Risk analysis for enforcement rollout'
      status: pending
      dependencies:
        depends_on:
        - AT-GUARD-PLAN
      exit_criteria:
      - prose: Edge cases and mitigation strategies captured in edge_cases.md with
          ledger reference
      domain: product
      description: 'Document risks (multi-agent contention, ledger corruption, false
        positives, integrity flakiness) and mitigation steps before coding.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/AT-GUARD-THINK
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: AT-GUARD-IMPLEMENT
      title: 'IMPLEMENT: Wire WorkProcessEnforcer, ledger, attestation, leases'
      status: pending
      dependencies:
        depends_on:
        - AT-GUARD-THINK
      exit_criteria:
      - prose: Code merged with unit/integration tests; ledger entries created; prompt
          signature enforced
      domain: product
      description: 'Implement startCycle/advancePhase wiring, state graph and tool
        router guards, phase ledger, prompt attestation, artifact gating, lease handling,
        and metrics emission.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/AT-GUARD-IMPLEMENT
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: AT-GUARD-VERIFY
      title: 'VERIFY: Integrity suite and enforcement telemetry'
      status: pending
      dependencies:
        depends_on:
        - AT-GUARD-IMPLEMENT
      exit_criteria:
      - prose: run_integrity_tests.sh passes; telemetry shows process.validation spans
          & counters
      domain: product
      description: 'Run full integrity script, capture logs, coverage, ledger verification,
        and ensure metrics/traces are written.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/AT-GUARD-VERIFY
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: AT-GUARD-REVIEW
      title: 'REVIEW: Confirm Autopilot functionality post-enforcement'
      status: pending
      dependencies:
        depends_on:
        - AT-GUARD-VERIFY
      exit_criteria:
      - prose: Reviewer rubric completed; functionality smoke evidence attached; ledger/journal
          updated
      domain: product
      description: 'Conduct structured review validating Autopilot workflows still
        operate (planner dispatch, tool execution, state transitions) with evidence
        attached.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/AT-GUARD-REVIEW
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: AT-GUARD-PR
      title: 'PR: Document Phase -1 completion'
      status: pending
      dependencies:
        depends_on:
        - AT-GUARD-REVIEW
      exit_criteria:
      - prose: PR summary with evidence bundle, ledger hash, prompt signature verification
      domain: product
      description: 'Assemble PR summary, evidence, and ledger/prompt attestation outputs
        proving Phase -1 completion.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/AT-GUARD-PR
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: AT-GUARD-MONITOR
      title: 'MONITOR: Post-deployment telemetry for enforcement'
      status: pending
      dependencies:
        depends_on:
        - AT-GUARD-PR
      exit_criteria:
      - prose: Monitoring plan executed; metrics dashboard updated; discrepancies
          addressed or logged
      domain: product
      description: 'Run monitoring plan, ensure enforcement metrics stay stable, capture
        results in monitoring plan doc.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/AT-GUARD-MONITOR
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: ROADMAP-STRUCT
      title: Roadmap Structure Enhancement for Autopilot Efficiency
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Typed roadmap schemas defined with validation
      - prose: Machine-readable dependency graph implemented
      - prose: Success criteria structured for automated verification
      - prose: Autopilot metadata added (complexity, effort, tool requirements)
      - prose: plan_next tool enhanced to use new structure
      domain: product
      description: 'Enhance roadmap.yaml structure to be more machine-readable and
        autopilot-friendly. Current structure is human-optimized (YAML prose); autopilot
        needs typed schemas, explicit dependencies, machine-readable success criteria,
        and metadata for intelligent task selection.

        Improvements: 1. Define TypeScript schemas (TaskSchema, EpicSchema, MilestoneSchema)
        2. Add typed dependency relationships (depends_on, blocks, related_to, produces,
        consumes) 3. Structure acceptance criteria as testable predicates (not prose)
        4. Add autopilot metadata (complexity: 1-10, effort_hours, risk_level, tool_allowlist)
        5. Enhance task status (add: blocked, needs_review, ready_for_rollout, archived)
        6. Add cross-item integration metadata (related items, contract versions)

        Benefits: - Better plan_next intelligence (dependency resolution, effort estimation)
        - Automated validation (no missing dependencies, no circular deps) - Better
        progress tracking (granular status beyond pending/in_progress/done) - Future-proof
        for multi-agent coordination - System compatibility (structured data for tools/APIs)

        Implementation: - Create schemas in tools/wvo_mcp/src/roadmap/schemas.ts -
        Add roadmap validation script (scripts/validate_roadmap.ts) - Migrate existing
        roadmap.yaml to new structure (backwards compatible) - Update plan_next MCP
        tool to use typed structure - Add roadmap linter to CI

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/ROADMAP-STRUCT
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: ROADMAP-HYBRID
      title: 'Roadmap Hybrid Storage: YAML Source + SQLite Runtime Index'
      status: pending
      dependencies:
        depends_on:
        - ROADMAP-STRUCT
      exit_criteria:
      - prose: SQLite schema defined for roadmap entities (tasks, dependencies, metadata)
      - prose: "YAML\u2192SQLite loader implemented with incremental updates"
      - prose: Query performance benchmarked (>100x faster than YAML parsing)
      - prose: MCP tools migrated to use SQLite queries (plan_next, plan_update)
      - prose: YAML remains source of truth (git-friendly, human-editable)
      - prose: "Hot reload on YAML changes (file watcher \u2192 reload SQLite)"
      - test: npm run test:roadmap-hybrid
        expect: all tests pass
      - metric: plan_next query time
        expect: < 5ms (vs ~50ms YAML)
      domain: mcp
      description: "Implement hybrid storage for roadmap: keep YAML as git-friendly\
        \ source of truth, but build SQLite runtime index for fast queries. Best of\
        \ both worlds.\nProblem: YAML parsing is slow (~50ms for 500 tasks), no indexing,\
        \ no complex queries. But YAML is human-readable, git-friendly, and easy to\
        \ edit. SQLite is fast (<5ms queries), has indexes and SQL, but poor git diffs\
        \ and not human-editable.\nSolution: Hybrid approach - YAML as source, SQLite\
        \ as runtime index. On MCP server startup, load roadmap.yaml \u2192 build\
        \ SQLite database in memory or on disk. File watcher detects YAML changes\
        \ \u2192 reload SQLite. All queries use SQLite, all edits go through YAML.\n\
        Architecture: 1. Schema: SQLite tables (tasks, dependencies, exit_criteria,\
        \ metadata, tags) 2. Loader: YAML parser \u2192 populate SQLite (tools/wvo_mcp/src/roadmap/loader.ts)\
        \ 3. Query Interface: Replace direct YAML access with SQL queries 4. Hot Reload:\
        \ File watcher on state/roadmap.yaml \u2192 trigger reload 5. Validation:\
        \ Both YAML (structure) and SQLite (referential integrity)\nMigration Path:\
        \ - Phase 1: Build loader + SQLite schema (no breaking changes) - Phase 2:\
        \ Add parallel queries (YAML + SQLite, verify consistency) - Phase 3: Migrate\
        \ MCP tools to SQLite-only - Phase 4: Remove YAML parsing from query path\
        \ (keep for edits)\nBenefits: - Fast queries: O(1) lookups vs O(n) YAML scan\
        \ - Complex queries: SQL joins vs manual YAML traversal - Indexes: task_id,\
        \ status, dependencies, tags - Scalability: Handle 10,000+ tasks efficiently\
        \ - Git-friendly: YAML source of truth remains unchanged - Transactions: Atomic\
        \ updates to roadmap state\nExample Queries: - \"Find all ready tasks\" \u2192\
        \ SELECT * FROM tasks WHERE status='ready' AND all_blockers_done=1 - \"Calculate\
        \ WSJF ranking\" \u2192 SELECT *, (readiness * value) / effort AS wsjf FROM\
        \ tasks ORDER BY wsjf DESC - \"Find blockers for task X\" \u2192 SELECT *\
        \ FROM dependencies WHERE dependent_id='X' AND type='depends_on'\nTrade-offs:\
        \ - Complexity: Two representations to keep in sync - Memory: SQLite database\
        \ (~2-5MB for 500 tasks) - Startup: Initial load takes ~100ms (vs ~50ms pure\
        \ YAML) - Benefit: Ongoing queries 10-100x faster\nFuture Extensions: - Vector\
        \ embeddings for task similarity search - Full-text search on descriptions\
        \ - Historical state tracking (task status over time) - Multi-roadmap federation\
        \ (query across multiple roadmaps)\n"
      complexity_score: 7
      effort_hours: 8
      required_tools:
      - fs_read
      - cmd_run
      evidence_path: state/evidence/ROADMAP-HYBRID
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: ROADMAP-PROMPTS
      title: Update Work Process Prompts for Roadmap v2.0 Structure
      status: done
      dependencies:
        depends_on:
        - ROADMAP-STRUCT
      exit_criteria:
      - prose: CLAUDE.md updated to reference v2.0 structure
      - prose: AGENTS.md updated with v2.0 examples
      - prose: MCP tool descriptions updated (plan_next, plan_update)
      - test: 'grep -r "dependencies: \\[\\]" docs/autopilot/'
        expect: no matches
      domain: mcp
      description: 'Update work process docs and system prompts to reflect roadmap
        v2.0 structure. Replace v1 examples (flat dependencies, prose criteria) with
        v2.0 (typed dependencies, structured criteria).

        '
      complexity_score: 3
      effort_hours: 2
      required_tools:
      - fs_read
      - fs_write
      evidence_path: state/evidence/ROADMAP-PROMPTS
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: enforce
    - id: ROADMAP-EVIDENCE
      title: Integrate Evidence Tracking with Roadmap v2.0 Structure
      status: done
      dependencies:
        depends_on:
        - ROADMAP-STRUCT
      exit_criteria:
      - prose: Schema extended with evidence_path and work_process_phases fields
      - prose: Migration script adds evidence paths to existing tasks
      - prose: MCP tools auto-create evidence directories (strategize/, spec/, plan/,
          think/, implement/, verify/, review/, pr/, monitor/)
      - prose: Exit criteria include evidence file checks
      - test: npm run validate:roadmap-evidence
        expect: all tasks with status=done have complete evidence
      - file: state/evidence/{task_id}/verify/verification.md
        expect: exists for all done tasks
      domain: mcp
      description: "Integrate work process evidence tracking directly into roadmap\
        \ v2.0. Adds schema fields (`evidence_path`, `work_process_phases`, `evidence_enforcement`),\
        \ migration tooling, and an evidence validator so STRATEGIZE\u2192MONITOR\
        \ artifacts are auditable from the roadmap and CI can flag missing evidence.\
        \ mark verify phase complete 4. **plan_next**: Filter tasks by phase completeness\
        \ (e.g., only show tasks with verify done) 5. **critics_run**: Check evidence\
        \ completeness before marking task done\nExit Criteria Integration: ```yaml\
        \ exit_criteria:\n  - file: \"state/evidence/ROADMAP-STRUCT/strategize/strategy.md\"\
        \n    expect: \"exists\"\n  - file: \"state/evidence/ROADMAP-STRUCT/verify/verification.md\"\
        \n    expect: \"exists\"\n  - prose: \"All 9 phases completed with evidence\"\
        \n```\nMigration Strategy: 1. Add fields to schema (backwards compatible -\
        \ optional fields) 2. Scan state/evidence/* directories and populate evidence_path\
        \ 3. Detect which phases completed by checking directory contents 4. Update\
        \ existing tasks with evidence paths 5. Add validation script (validate:roadmap-evidence)\n\
        Benefits: - Automated work process compliance checking - Easy discovery of\
        \ evidence for any task - Phase completion dashboard (e.g., \"80% of tasks\
        \ have verify done\") - Better task filtering (show only tasks ready for review)\
        \ - Historical tracking (when did each phase complete?)\nEnforcement: - Tasks\
        \ cannot be marked \"done\" without complete evidence - Each phase directory\
        \ must contain required artifacts - WorkProcessEnforcer validates evidence\
        \ before transitions\nExample Enhanced Task: ```yaml - id: ROADMAP-STRUCT\n\
        \  status: done\n  evidence_path: \"state/evidence/ROADMAP-STRUCT\"\n  work_process_phases:\n\
        \    strategize: { completed: true, path: \"strategize/strategy.md\", date:\
        \ \"2025-10-29\" }\n    spec: { completed: true, path: \"spec/spec.md\", date:\
        \ \"2025-10-29\" }\n    # ... all 9 phases\n  exit_criteria:\n    - file:\
        \ \"state/evidence/ROADMAP-STRUCT/verify/verification.md\"\n      expect:\
        \ \"exists\"\n```\n"
      complexity_score: 6
      effort_hours: 6
      required_tools:
      - fs_read
      - fs_write
      - plan_update
      evidence_path: state/evidence/ROADMAP-EVIDENCE
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: WORK-PROCESS-FAILURES
      title: Comprehensive Failure Detection & Prevention System
      status: pending
      dependencies:
        depends_on:
        - ROADMAP-STRUCT
      exit_criteria:
      - prose: Pre-flight checks integrated
      - prose: Failure hunting protocol established
      - prose: Zero-skip policy enforced
      - test: npm run hunt:failures
        expect: scan completes
      domain: mcp
      description: 'Implement comprehensive failure detection, prevention, and hunting.
        Inspired by 4 test failures discovered late - need proactive, aggressive failure
        detection. Multi-layered: Prevention (pre-flight checks), Detection (CI/watch),
        Hunting (daily scans), Triage (auto-create tasks), Culture (zero-skip mandate).
        Target: detect <5min, fix <30min.

        '
      complexity_score: 10
      effort_hours: 16
      required_tools:
      - cmd_run
      - fs_read
      - critics_run
      evidence_path: state/evidence/WORK-PROCESS-FAILURES
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: enforce
    - id: AUTOPILOT-AUTONOMY-META
      title: Accelerate Full Autonomy with Excellence
      status: pending
      dependencies:
        depends_on:
        - WORK-PROCESS-FAILURES
        - ROADMAP-EVIDENCE
      exit_criteria:
      - prose: "Autonomy roadmap defined (current \u2192 full)"
      - prose: Quality gates locked (no regression)
      - prose: Self-improvement loop active
      - metric: autonomy_score
        expect: increase weekly
      domain: mcp
      description: "Meta-task: Structure autopilot development for full autonomy ASAP\
        \ with utmost quality. Critical path: 1) Evidence automation 2) Decision trees\
        \ 3) Self-correction 4) Self-improvement 5) Quality gates. Target: 0.60\u2192\
        0.95 autonomy score in 8 weeks while maintaining quality >0.95. Weekly goals,\
        \ daily dashboard, self-optimization protocol.\n"
      complexity_score: 9
      effort_hours: 16
      required_tools:
      - plan_next
      - plan_update
      - quality_standards
      evidence_path: state/evidence/AUTOPILOT-AUTONOMY-META
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: TOOL-EVAL-META
      title: MCP Tool Ecosystem Evaluation and Enhancement
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Current tool inventory documented with usage metrics
      - prose: Gap analysis complete (missing tools for autopilot workflows)
      - prose: Tool improvement proposals prioritized
      - prose: New tool designs specified for critical gaps
      - prose: Tool deprecation/consolidation recommendations
      domain: mcp
      description: "Evaluate the current state of MCP tools available to autopilot\
        \ (both current and post-improvement) to identify gaps, redundancies, and\
        \ opportunities for enhancement. Consider evolving work processes and autopilot\
        \ capabilities to anticipate future tool needs.\nCurrent State Assessment:\
        \ 1. Inventory all MCP tools (wvo_status, plan_next, fs_read, cmd_run, etc.)\
        \ 2. Measure usage frequency and context (which workflows use which tools)\
        \ 3. Identify tool gaps (workflows that lack appropriate tools) 4. Analyze\
        \ tool redundancy (overlapping functionality) 5. Evaluate tool ergonomics\
        \ (ease of use, clarity, error handling)\nFuture State Analysis: 1. Map tools\
        \ to future autopilot capabilities (post AT-GUARD, post instrumentation) 2.\
        \ Identify tools needed for new workflows (meta-evaluation, self-improvement)\
        \ 3. Design tools for emerging patterns (e.g., quality graph queries, roadmap\
        \ intelligence) 4. Consider multi-agent coordination needs (locks, leases,\
        \ handoffs) 5. Evaluate cross-cutting concerns (observability, error recovery,\
        \ rollback)\nTool Categories to Evaluate: - State Management (plan_next, plan_update,\
        \ context_write) - File System (fs_read, fs_write) - Execution (cmd_run, heavy_queue_enqueue)\
        \ - Quality (critics_run, quality_checklist) - Meta (wvo_status, provider_status,\
        \ state_metrics) - Observability (screenshot_session, artifact_record) - LSP\
        \ (lsp_definition, lsp_references, lsp_hover) - Flags (mcp_admin_flags)\n\
        Outputs: 1. Tool Usage Matrix (tool \xD7 workflow \xD7 frequency) 2. Gap Analysis\
        \ Report (missing tools, priority ranking) 3. Tool Enhancement Proposals (5-10\
        \ high-priority improvements) 4. New Tool Specifications (3-5 critical gaps)\
        \ 5. Deprecation Recommendations (redundant/unused tools)\nMeta Consideration:\
        \ Roadmap structure improvements (ROADMAP-STRUCT) apply to both autopilot\
        \ and WeatherVane roadmaps. Tool evaluation should consider how roadmap enhancements\
        \ enable new tool capabilities (e.g., tools that query roadmap metadata, suggest\
        \ next tasks, estimate effort).\nBenefits: - More efficient autopilot workflows\
        \ (right tools for each job) - Better developer experience (ergonomic, discoverable\
        \ tools) - Reduced tool sprawl (consolidate redundant functionality) - Future-proofed\
        \ tool ecosystem (anticipate emerging needs) - Better cross-cutting concerns\
        \ (observability, error handling)\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/TOOL-EVAL-META
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: E-AUTOPILOT-PHASE0
    title: Phase 0 Instrumentation
    status: pending
    tasks:
    - id: AT-INST-STRATEGIZE
      title: 'STRATEGIZE: Define instrumentation outcomes'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Strategy artifact links instrumentation goals to Autopilot observability
          needs
      domain: product
      description: 'Document why OTEL spans, metrics counters, and dashboards are
        required, including impacted Autopilot flows and success metrics.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/AT-INST-STRATEGIZE
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: AT-INST-SPEC
      title: 'SPEC: Telemetry & dashboard requirements'
      status: pending
      dependencies:
        depends_on:
        - AT-INST-STRATEGIZE
      exit_criteria:
      - prose: Spec covers span names, attributes, counters, JSONL outputs, dashboards/alerts
      domain: product
      description: 'Capture detailed specs for instrumentation and visualization deliverables.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/AT-INST-SPEC
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: AT-INST-PLAN
      title: 'PLAN: Implementation breakdown for instrumentation'
      status: pending
      dependencies:
        depends_on:
        - AT-INST-SPEC
      exit_criteria:
      - prose: Plan with tasks, estimates, dependencies, verification steps
      domain: product
      description: 'Outline implementation steps for spans, metrics, telemetry writers,
        dashboards, evidence capture.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/AT-INST-PLAN
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: AT-INST-THINK
      title: 'THINK: Risks & mitigations for instrumentation rollout'
      status: pending
      dependencies:
        depends_on:
        - AT-INST-PLAN
      exit_criteria:
      - prose: Edge cases logged (overhead, missing traces, noisy counters) with mitigations
      domain: product
      description: 'Analyze risks (performance, data volume, false positives) before
        coding.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/AT-INST-THINK
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: AT-INST-IMPLEMENT
      title: 'IMPLEMENT: Add spans, counters, telemetry outputs'
      status: pending
      dependencies:
        depends_on:
        - AT-INST-THINK
      exit_criteria:
      - prose: Code merged with tests; traces/metrics JSONL generated in local run
      domain: product
      description: 'Implement OTEL spans, counters, telemetry writers, and ensure
        they integrate with enforcement.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/AT-INST-IMPLEMENT
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: AT-INST-VERIFY
      title: 'VERIFY: Demonstrate instrumentation under load'
      status: pending
      dependencies:
        depends_on:
        - AT-INST-IMPLEMENT
      exit_criteria:
      - prose: Tests and manual run produce traces/metrics; artefacts stored and linked
      domain: product
      description: 'Run validation suite and manual scenario to confirm instrumentation
        outputs.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/AT-INST-VERIFY
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: AT-INST-REVIEW
      title: 'REVIEW: Evaluate instrumentation quality'
      status: pending
      dependencies:
        depends_on:
        - AT-INST-VERIFY
      exit_criteria:
      - prose: Reviewer confirms coverage, evidence linked, documentation updated
      domain: product
      description: 'Conduct review ensuring instrumentation meets quality bar and
        Autopilot functionality preserved.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/AT-INST-REVIEW
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: AT-INST-PR
      title: 'PR: Summarize Phase 0 instrumentation'
      status: pending
      dependencies:
        depends_on:
        - AT-INST-REVIEW
      exit_criteria:
      - prose: PR summary includes spans/metrics evidence, dashboards, ledger references
      domain: product
      description: 'Document instrumentation deliverables and evidence for PR.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/AT-INST-PR
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: AT-INST-MONITOR
      title: 'MONITOR: Ongoing telemetry validation'
      status: pending
      dependencies:
        depends_on:
        - AT-INST-PR
      exit_criteria:
      - prose: Monitoring plan executed, dashboards reviewed, follow-ups logged
      domain: product
      description: 'Operate the instrumentation post-deploy, verify dashboards, and
        log observations.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/AT-INST-MONITOR
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
- id: E-GENERAL
  title: E-GENERAL
  status: pending
  domain: product
  milestones:
  - id: E-GENERAL-backlog
    title: Backlog
    status: pending
    tasks:
    - id: CRIT-PERF-BUILD-4254a2
      title: '[Critic:build] Restore performance'
      status: blocked
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Critic build is underperforming and needs immediate remediation.


        Identity: Build Sentinel (engineering, authority blocking)

        Mission: Guarantee that core build processes remain reproducible and optimized
        across environments.

        Signature powers: Diagnoses build pipeline regressions and unstable toolchains.;
        Flags missing build artifacts or misconfigured dependencies before release.

        Autonomy guidance: Attempt automated patching of build scripts when safe;
        escalate infrastructure escalations beyond local fixes.


        No successful runs recorded in the last 6 observations; 6 consecutive failures
        detected.


        Observation window: 6 runs


        Consecutive failures: 6


        Failures: 6 | Successes: 0


        Assigned to: Director Dana


        Expectations:

        - Diagnose root causes for the critic''s repeated failures.

        - Patch critic configuration, training data, or underlying automation as needed.

        - Document findings in state/context.md and roadmap notes.

        - Close this task once the critic passes reliably.


        Latest output snippet:

        make[1]: *** [lint] Error 1'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-BUILD-4254a2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-BUILD-4254a2.1
      title: Research and design for [Critic:build] Restore performance
      status: pending
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Research phase: Understand requirements and design approach for
        [Critic:build] Restore performance'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-BUILD-4254a2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-BUILD-4254a2.2
      title: Implement [Critic:build] Restore performance
      status: pending
      dependencies:
        depends_on:
        - CRIT-PERF-BUILD-4254a2.1
      exit_criteria: []
      domain: product
      description: 'Implementation phase: Execute the plan for [Critic:build] Restore
        performance'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-BUILD-4254a2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-BUILD-4254a2.3
      title: Validate and test [Critic:build] Restore performance
      status: pending
      dependencies:
        depends_on:
        - CRIT-PERF-BUILD-4254a2.2
      exit_criteria: []
      domain: product
      description: 'Validation phase: Test and verify [Critic:build] Restore performance'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-BUILD-4254a2.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-GLOBAL-9dfa06
      title: '[Critics] Systemic performance remediation'
      status: blocked
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Multiple critics are underperforming and require coordinated intervention.


        2 critics require director-level intervention after repeated failures.


        Affected critics: build, tests


        Critics evaluated in run: 3


        Reports captured: 2


        Assigned to: Director Dana


        Expectations:

        - Review individual remediation tasks and look for systemic issues.

        - Adjust critic configurations, training loops, or staffing mixes.

        - Provide a coordination brief in state/context.md.

        - Close this systemic task once individual critics are back on track.'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-GLOBAL-9dfa06
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-GLOBAL-9dfa06.1
      title: Research and design for [Critics] Systemic performance remediation
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Research phase: Understand requirements and design approach for
        [Critics] Systemic performance remediation'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-GLOBAL-9dfa06.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-GLOBAL-9dfa06.2
      title: Implement [Critics] Systemic performance remediation
      status: in_progress
      dependencies:
        depends_on:
        - CRIT-PERF-GLOBAL-9dfa06.1
      exit_criteria: []
      domain: product
      description: 'Implementation phase: Execute the plan for [Critics] Systemic
        performance remediation'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-GLOBAL-9dfa06.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-GLOBAL-9dfa06.3
      title: Validate and test [Critics] Systemic performance remediation
      status: pending
      dependencies:
        depends_on:
        - CRIT-PERF-GLOBAL-9dfa06.2
      exit_criteria: []
      domain: product
      description: 'Validation phase: Test and verify [Critics] Systemic performance
        remediation'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-GLOBAL-9dfa06.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-TESTS-fcee61
      title: '[Critic:tests] Restore performance'
      status: needs_review
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Critic tests is underperforming and needs immediate remediation.


        Identity: Regression Hunter (quality, authority blocking)

        Mission: Keep the test suites healthy and ensure deterministic results across
        flows.

        Signature powers: Surfaces flaky suites and failing assertions with reproduction
        notes.; Synthesizes minimal repro commands for Autopilot triage.

        Autonomy guidance: Rerun targeted suites automatically; lean on Autopilot
        only when new failures persist.


        No successful runs recorded in the last 6 observations; 6 consecutive failures
        detected.


        Observation window: 6 runs


        Consecutive failures: 6


        Failures: 6 | Successes: 0


        Assigned to: Director Dana


        Expectations:

        - Diagnose root causes for the critic''s repeated failures.

        - Patch critic configuration, training data, or underlying automation as needed.

        - Document findings in state/context.md and roadmap notes.

        - Close this task once the critic passes reliably.


        Latest output snippet:

        == Python test suite ==

        ============================= test session starts ==============================

        platform darwin -- Python 3.10.12, pytest-8.4.2, pluggy-1.6.0

        rootdir: /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane

        configfile: pyproject.toml

        plugins: anyio-4.11.0, asyncio-1.2.0

        asyncio: mode=auto, debug=False, asyncio_default_fixture_loop_scope=None,
        asyncio_default_test_loop_scope=function

        collected 1134 items / 19 errors


        ==================================== ERRORS ====================================

        ____________ ERROR collecting tests/api/onboarding/test_progress.p...'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-TESTS-fcee61
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-TESTS-fcee61.1
      title: Research and design for [Critic:tests] Restore performance
      status: pending
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Research phase: Understand requirements and design approach for
        [Critic:tests] Restore performance'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-TESTS-fcee61.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-TESTS-fcee61.2
      title: Implement [Critic:tests] Restore performance
      status: pending
      dependencies:
        depends_on:
        - CRIT-PERF-TESTS-fcee61.1
      exit_criteria: []
      domain: product
      description: 'Implementation phase: Execute the plan for [Critic:tests] Restore
        performance'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-TESTS-fcee61.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-TESTS-fcee61.3
      title: Validate and test [Critic:tests] Restore performance
      status: pending
      dependencies:
        depends_on:
        - CRIT-PERF-TESTS-fcee61.2
      exit_criteria: []
      domain: product
      description: 'Validation phase: Test and verify [Critic:tests] Restore performance'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-TESTS-fcee61.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: E-GENERAL.1
      title: Research and design for E-GENERAL
      status: pending
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Research phase: Understand requirements and design approach for
        E-GENERAL'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/E-GENERAL.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: E-GENERAL.2
      title: Implement E-GENERAL
      status: pending
      dependencies:
        depends_on:
        - E-GENERAL.1
      exit_criteria: []
      domain: product
      description: 'Implementation phase: Execute the plan for E-GENERAL'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/E-GENERAL.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: E-GENERAL.3
      title: Validate and test E-GENERAL
      status: pending
      dependencies:
        depends_on:
        - E-GENERAL.2
      exit_criteria: []
      domain: product
      description: 'Validation phase: Test and verify E-GENERAL'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/E-GENERAL.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: E-ML-REMEDIATION.1
      title: Research and design for ML Model Remediation - From Prototype to Production
      status: pending
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Research phase: Understand requirements and design approach for
        ML Model Remediation - From Prototype to Production'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/E-ML-REMEDIATION.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: E-ML-REMEDIATION.2
      title: Implement ML Model Remediation - From Prototype to Production
      status: pending
      dependencies:
        depends_on:
        - E-ML-REMEDIATION.1
      exit_criteria: []
      domain: product
      description: 'Implementation phase: Execute the plan for ML Model Remediation
        - From Prototype to Production'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/E-ML-REMEDIATION.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: E-ML-REMEDIATION.3
      title: Validate and test ML Model Remediation - From Prototype to Production
      status: pending
      dependencies:
        depends_on:
        - E-ML-REMEDIATION.2
      exit_criteria: []
      domain: product
      description: 'Validation phase: Test and verify ML Model Remediation - From
        Prototype to Production'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/E-ML-REMEDIATION.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-0.1.1
      title: Implementation verified
      status: blocked
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Create ModelingReality critic with quantitative
        thresholds: Implementation verified'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-0.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-0.1.2
      title: Tests verified
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-0.1.1
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Create ModelingReality critic with quantitative
        thresholds: Tests verified'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-0.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-0.1.3
      title: Quality gate APPROVED
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-0.1.2
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Create ModelingReality critic with quantitative
        thresholds: Quality gate APPROVED'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-0.1.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-0.1.4
      title: Runtime verification PASSED
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-0.1.3
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Create ModelingReality critic with quantitative
        thresholds: Runtime verification PASSED'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-0.1.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-0.1.5
      title: Critical issues fixed
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-0.1.4
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Create ModelingReality critic with quantitative
        thresholds: Critical issues fixed'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-0.1.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-0.2.1
      title: Implementation verified
      status: needs_review
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Update all ML task exit criteria with objective
        metrics: Implementation verified'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-0.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-0.2.2
      title: Tests verified
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-0.2.1
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Update all ML task exit criteria with objective
        metrics: Tests verified'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-0.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-0.2.3
      title: Quality gate APPROVED
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-0.2.2
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Update all ML task exit criteria with objective
        metrics: Quality gate APPROVED'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-0.2.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-0.2.4
      title: Runtime verification PASSED
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-0.2.3
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Update all ML task exit criteria with objective
        metrics: Runtime verification PASSED'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-0.2.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-0.2.5
      title: Critical issues fixed
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-0.2.4
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Update all ML task exit criteria with objective
        metrics: Critical issues fixed'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-0.2.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-1.1.1
      title: Implementation verified
      status: needs_review
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Debug and fix weather multiplier logic in
        data generator: Implementation verified'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-1.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-1.1.2
      title: Tests verified
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-1.1.1
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Debug and fix weather multiplier logic in
        data generator: Tests verified'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-1.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-1.1.3
      title: Quality gate APPROVED
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-1.1.2
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Debug and fix weather multiplier logic in
        data generator: Quality gate APPROVED'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-1.1.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-1.1.4
      title: Runtime verification PASSED
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-1.1.3
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Debug and fix weather multiplier logic in
        data generator: Runtime verification PASSED'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-1.1.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-1.1.5
      title: Critical issues fixed
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-1.1.4
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Debug and fix weather multiplier logic in
        data generator: Critical issues fixed'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-1.1.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-1.3.1
      title: Implementation verified
      status: pending
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Create validation tests for synthetic data
        quality: Implementation verified'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-1.3.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-1.3.2
      title: Tests verified
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-1.3.1
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Create validation tests for synthetic data
        quality: Tests verified'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-1.3.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-1.3.3
      title: Quality gate APPROVED
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-1.3.2
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Create validation tests for synthetic data
        quality: Quality gate APPROVED'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-1.3.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-1.3.4
      title: Runtime verification PASSED
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-1.3.3
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Create validation tests for synthetic data
        quality: Runtime verification PASSED'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-1.3.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-1.3.5
      title: Critical issues fixed
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-1.3.4
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Create validation tests for synthetic data
        quality: Critical issues fixed'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-1.3.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-2.1.1
      title: Implementation verified
      status: pending
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Implement proper train/val/test splitting
        with no leakage: Implementation verified'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-2.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-2.1.2
      title: Tests verified
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-2.1.1
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Implement proper train/val/test splitting
        with no leakage: Tests verified'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-2.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-2.1.3
      title: Quality gate APPROVED
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-2.1.2
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Implement proper train/val/test splitting
        with no leakage: Quality gate APPROVED'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-2.1.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-2.1.4
      title: Runtime verification PASSED
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-2.1.3
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Implement proper train/val/test splitting
        with no leakage: Runtime verification PASSED'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-2.1.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-2.1.5
      title: Critical issues fixed
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-2.1.4
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Implement proper train/val/test splitting
        with no leakage: Critical issues fixed'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-2.1.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-2.2.1
      title: Implementation verified
      status: pending
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Implement LightweightMMM with weather features:
        Implementation verified'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-2.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-2.2.2
      title: Tests verified
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-2.2.1
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Implement LightweightMMM with weather features:
        Tests verified'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-2.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-2.2.3
      title: Quality gate APPROVED
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-2.2.2
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Implement LightweightMMM with weather features:
        Quality gate APPROVED'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-2.2.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-2.2.4
      title: Runtime verification PASSED
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-2.2.3
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Implement LightweightMMM with weather features:
        Runtime verification PASSED'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-2.2.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-2.2.5
      title: Critical issues fixed
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-2.2.4
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Implement LightweightMMM with weather features:
        Critical issues fixed'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-2.2.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-2.5.1
      title: Implementation verified
      status: pending
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Compare models to baseline (naive/seasonal/linear):
        Implementation verified'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-2.5.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-2.5.2
      title: Tests verified
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-2.5.1
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Compare models to baseline (naive/seasonal/linear):
        Tests verified'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-2.5.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-2.5.3
      title: Quality gate APPROVED
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-2.5.2
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Compare models to baseline (naive/seasonal/linear):
        Quality gate APPROVED'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-2.5.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-2.5.4
      title: Runtime verification PASSED
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-2.5.3
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Compare models to baseline (naive/seasonal/linear):
        Runtime verification PASSED'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-2.5.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-2.5.5
      title: Critical issues fixed
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-2.5.4
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Compare models to baseline (naive/seasonal/linear):
        Critical issues fixed'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-2.5.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-3.2.1
      title: Implementation verified
      status: pending
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Write comprehensive ML validation documentation:
        Implementation verified'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-3.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-3.2.2
      title: Tests verified
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-3.2.1
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Write comprehensive ML validation documentation:
        Tests verified'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-3.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-3.2.3
      title: Quality gate APPROVED
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-3.2.2
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Write comprehensive ML validation documentation:
        Quality gate APPROVED'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-3.2.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-3.2.4
      title: Runtime verification PASSED
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-3.2.3
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Write comprehensive ML validation documentation:
        Runtime verification PASSED'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-3.2.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-3.2.5
      title: Critical issues fixed
      status: pending
      dependencies:
        depends_on:
        - REM-T-MLR-3.2.4
      exit_criteria: []
      domain: product
      description: 'Part of [REM] Verify: Write comprehensive ML validation documentation:
        Critical issues fixed'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-3.2.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-0.3.1
      title: artifact:docs/ML_QUALITY_STANDARDS.md
      status: pending
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Part of Document world-class quality standards for ML: artifact:docs/ML_QUALITY_STANDARDS.md'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-0.3.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-0.3.2
      title: verification:Numeric thresholds for all metrics
      status: pending
      dependencies:
        depends_on:
        - T-MLR-0.3.1
      exit_criteria: []
      domain: product
      description: 'Part of Document world-class quality standards for ML: verification:Numeric
        thresholds for all metrics'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-0.3.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-0.3.3
      title: verification:Baseline comparison requirements
      status: pending
      dependencies:
        depends_on:
        - T-MLR-0.3.2
      exit_criteria: []
      domain: product
      description: 'Part of Document world-class quality standards for ML: verification:Baseline
        comparison requirements'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-0.3.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-0.3.4
      title: review:External ML practitioner peer review
      status: pending
      dependencies:
        depends_on:
        - T-MLR-0.3.3
      exit_criteria: []
      domain: product
      description: 'Part of Document world-class quality standards for ML: review:External
        ML practitioner peer review'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-0.3.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-1.2.1
      title: artifact:storage/seeds/synthetic_v2/*.parquet (20 files)
      status: pending
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Part of Generate 3 years of synthetic data for 20 tenants: artifact:storage/seeds/synthetic_v2/*.parquet
        (20 files)'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-1.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-1.2.2
      title: metric:total_rows = 219000
      status: pending
      dependencies:
        depends_on:
        - T-MLR-1.2.1
      exit_criteria: []
      domain: product
      description: 'Part of Generate 3 years of synthetic data for 20 tenants: metric:total_rows
        = 219000'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-1.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-1.2.3
      title: metric:date_range = 2022-01-01 to 2024-12-31
      status: pending
      dependencies:
        depends_on:
        - T-MLR-1.2.2
      exit_criteria: []
      domain: product
      description: 'Part of Generate 3 years of synthetic data for 20 tenants: metric:date_range
        = 2022-01-01 to 2024-12-31'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-1.2.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-1.2.4
      title: metric:weather_correlations_within_target >= 0.90
      status: pending
      dependencies:
        depends_on:
        - T-MLR-1.2.3
      exit_criteria: []
      domain: product
      description: 'Part of Generate 3 years of synthetic data for 20 tenants: metric:weather_correlations_within_target
        >= 0.90'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-1.2.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-1.2.5
      title: test:pytest tests/data_gen/test_synthetic_v2_quality.py
      status: pending
      dependencies:
        depends_on:
        - T-MLR-1.2.4
      exit_criteria: []
      domain: product
      description: 'Part of Generate 3 years of synthetic data for 20 tenants: test:pytest
        tests/data_gen/test_synthetic_v2_quality.py'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-1.2.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-1.2.6
      title: critic:data_quality
      status: pending
      dependencies:
        depends_on:
        - T-MLR-1.2.5
      exit_criteria: []
      domain: product
      description: 'Part of Generate 3 years of synthetic data for 20 tenants: critic:data_quality'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-1.2.6
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-2.4.1
      title: artifact:experiments/mmm_v2/validation_report.json
      status: pending
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Part of Validate model performance against objective thresholds:
        artifact:experiments/mmm_v2/validation_report.json'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-2.4.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-2.4.2
      title: metric:weather_sensitive_r2_pass_rate >= 0.80
      status: pending
      dependencies:
        depends_on:
        - T-MLR-2.4.1
      exit_criteria: []
      domain: product
      description: 'Part of Validate model performance against objective thresholds:
        metric:weather_sensitive_r2_pass_rate >= 0.80'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-2.4.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-2.4.3
      title: metric:weather_elasticity_sign_correct = 1.0
      status: pending
      dependencies:
        depends_on:
        - T-MLR-2.4.2
      exit_criteria: []
      domain: product
      description: 'Part of Validate model performance against objective thresholds:
        metric:weather_elasticity_sign_correct = 1.0'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-2.4.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-2.4.4
      title: metric:no_overfitting_detected = true
      status: pending
      dependencies:
        depends_on:
        - T-MLR-2.4.3
      exit_criteria: []
      domain: product
      description: 'Part of Validate model performance against objective thresholds:
        metric:no_overfitting_detected = true'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-2.4.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-2.4.5
      title: critic:modeling_reality_v2
      status: pending
      dependencies:
        depends_on:
        - T-MLR-2.4.4
      exit_criteria: []
      domain: product
      description: 'Part of Validate model performance against objective thresholds:
        critic:modeling_reality_v2'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-2.4.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-2.4.6
      title: critic:academic_rigor
      status: pending
      dependencies:
        depends_on:
        - T-MLR-2.4.5
      exit_criteria: []
      domain: product
      description: 'Part of Validate model performance against objective thresholds:
        critic:academic_rigor'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-2.4.6
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-2.6.1
      title: artifact:experiments/mmm_v2/robustness_report.json
      status: pending
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Part of Run robustness tests (outliers, missing data, edge cases):
        artifact:experiments/mmm_v2/robustness_report.json'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-2.6.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-2.6.2
      title: test:Model handles extreme weather without crash
      status: pending
      dependencies:
        depends_on:
        - T-MLR-2.6.1
      exit_criteria: []
      domain: product
      description: 'Part of Run robustness tests (outliers, missing data, edge cases):
        test:Model handles extreme weather without crash'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-2.6.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-2.6.3
      title: test:Model handles missing data gracefully
      status: pending
      dependencies:
        depends_on:
        - T-MLR-2.6.2
      exit_criteria: []
      domain: product
      description: 'Part of Run robustness tests (outliers, missing data, edge cases):
        test:Model handles missing data gracefully'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-2.6.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-2.6.4
      title: test:Zero ad spend predicts organic baseline
      status: pending
      dependencies:
        depends_on:
        - T-MLR-2.6.3
      exit_criteria: []
      domain: product
      description: 'Part of Run robustness tests (outliers, missing data, edge cases):
        test:Zero ad spend predicts organic baseline'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-2.6.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-2.6.5
      title: test:pytest tests/model/test_mmm_robustness.py (15/15 pass)
      status: pending
      dependencies:
        depends_on:
        - T-MLR-2.6.4
      exit_criteria: []
      domain: product
      description: 'Part of Run robustness tests (outliers, missing data, edge cases):
        test:pytest tests/model/test_mmm_robustness.py (15/15 pass)'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-2.6.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-2.6.6
      title: critic:modeling_reality_v2
      status: pending
      dependencies:
        depends_on:
        - T-MLR-2.6.5
      exit_criteria: []
      domain: product
      description: 'Part of Run robustness tests (outliers, missing data, edge cases):
        critic:modeling_reality_v2'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-2.6.6
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-3.1.1
      title: artifact:experiments/mmm_v2/validation_notebook.ipynb
      status: pending
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Part of Create reproducible validation notebook: artifact:experiments/mmm_v2/validation_notebook.ipynb'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-3.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-3.1.2
      title: test:Notebook runs end-to-end without errors
      status: pending
      dependencies:
        depends_on:
        - T-MLR-3.1.1
      exit_criteria: []
      domain: product
      description: 'Part of Create reproducible validation notebook: test:Notebook
        runs end-to-end without errors'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-3.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-3.1.3
      title: test:Output matches claimed metrics
      status: pending
      dependencies:
        depends_on:
        - T-MLR-3.1.2
      exit_criteria: []
      domain: product
      description: 'Part of Create reproducible validation notebook: test:Output matches
        claimed metrics'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-3.1.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-3.1.4
      title: artifact:experiments/mmm_v2/validation_notebook.html
      status: pending
      dependencies:
        depends_on:
        - T-MLR-3.1.3
      exit_criteria: []
      domain: product
      description: 'Part of Create reproducible validation notebook: artifact:experiments/mmm_v2/validation_notebook.html'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-3.1.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-3.1.5
      title: critic:academic_rigor
      status: pending
      dependencies:
        depends_on:
        - T-MLR-3.1.4
      exit_criteria: []
      domain: product
      description: 'Part of Create reproducible validation notebook: critic:academic_rigor'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-3.1.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
- id: E-ML-REMEDIATION
  title: ML Model Remediation - From Prototype to Production
  status: pending
  domain: product
  milestones:
  - id: M-MLR-0
    title: 'Foundation: Truth & Accountability'
    status: pending
    tasks:
    - id: T-MLR-0.1
      title: Create ModelingReality critic with quantitative thresholds
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:tools/wvo_mcp/src/critics/modeling_reality_v2.ts
      - prose: "test:Critic FAILS when R\xB2 < 0.50"
      - prose: test:Critic FAILS when no baseline comparison
      - prose: test:Critic FAILS when weather elasticity signs wrong
      - prose: metric:critic_strictness = 1.0
      - prose: critic:tests
      domain: product
      description: "Create critic that enforces quantitative thresholds: R\xB2 > 0.50,\
        \ correct\nelasticity signs, baseline comparison required, no subjective judgment.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-0.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-0.2
      title: Update all ML task exit criteria with objective metrics
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:state/roadmap.yaml (T12.*, T13.* updated)
      - prose: verification:All ML tasks have "metric:r2 > 0.50"
      - prose: verification:All ML tasks have "metric:beats_baseline > 1.10"
      - prose: verification:All ML tasks have "critic:modeling_reality_v2"
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-0.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-0.3
      title: Document world-class quality standards for ML
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:docs/ML_QUALITY_STANDARDS.md
      - prose: verification:Numeric thresholds for all metrics
      - prose: verification:Baseline comparison requirements
      - prose: review:External ML practitioner peer review
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-0.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M-MLR-1
    title: 'Phase 1: Fix Synthetic Data (2 weeks)'
    status: pending
    tasks:
    - id: T-MLR-1.1
      title: Debug and fix weather multiplier logic in data generator
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:scripts/weather/generate_synthetic_tenants_v2.py
      - prose: "test:Extreme correlation = 0.85 \xB1 0.05"
      - prose: "test:High correlation = 0.70 \xB1 0.05"
      - prose: "test:Medium correlation = 0.40 \xB1 0.05"
      - prose: test:None correlation < 0.10
      - prose: critic:data_quality
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-1.2
      title: Generate 3 years of synthetic data for 20 tenants
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:storage/seeds/synthetic_v2/*.parquet (20 files)
      - prose: metric:total_rows = 219000
      - prose: metric:date_range = 2022-01-01 to 2024-12-31
      - prose: metric:weather_correlations_within_target >= 0.90
      - prose: test:pytest tests/data_gen/test_synthetic_v2_quality.py
      - prose: critic:data_quality
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-1.3
      title: Create validation tests for synthetic data quality
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:tests/data_gen/test_synthetic_v2_quality.py
      - prose: test:20/20 tenant tests pass
      - prose: artifact:experiments/data_validation/correlation_plots.pdf
      - prose: critic:tests
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-1.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M-MLR-2
    title: 'Phase 2: Rigorous MMM Training (3 weeks)'
    status: pending
    tasks:
    - id: T-MLR-2.1
      title: Implement proper train/val/test splitting with no leakage
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:shared/libs/modeling/time_series_split.py
      - prose: test:Validation after training (no date overlap)
      - prose: test:Test after validation (no date overlap)
      - prose: test:Split percentages 70/15/15
      - prose: critic:leakage
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-2.2
      title: Implement LightweightMMM with weather features
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:apps/model/mmm_lightweight_weather.py
      - prose: test:Adstock transformation applied
      - prose: test:Hill saturation curves applied
      - prose: test:Weather interaction terms included
      - prose: test:pytest tests/model/test_mmm_lightweight.py (12/12 pass)
      - prose: critic:academic_rigor
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-2.4
      title: Validate model performance against objective thresholds
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/mmm_v2/validation_report.json
      - prose: metric:weather_sensitive_r2_pass_rate >= 0.80
      - prose: metric:weather_elasticity_sign_correct = 1.0
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-2.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-2.5
      title: Compare models to baseline (naive/seasonal/linear)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/mmm_v2/baseline_comparison.json
      - prose: metric:beats_naive_by >= 1.10
      - prose: metric:beats_seasonal_by >= 1.05
      - prose: metric:beats_linear_by >= 1.05
      - prose: artifact:experiments/mmm_v2/baseline_comparison_plots.pdf
      - prose: critic:modeling_reality_v2
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-2.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-2.6
      title: Run robustness tests (outliers, missing data, edge cases)
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/mmm_v2/robustness_report.json
      - prose: test:Model handles extreme weather without crash
      - prose: test:Model handles missing data gracefully
      - prose: test:Zero ad spend predicts organic baseline
      - prose: test:pytest tests/model/test_mmm_robustness.py (15/15 pass)
      - prose: critic:modeling_reality_v2
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-2.6
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M-MLR-3
    title: 'Phase 3: Reproducibility & Documentation (1 week)'
    status: pending
    tasks:
    - id: T-MLR-3.1
      title: Create reproducible validation notebook
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/mmm_v2/validation_notebook.ipynb
      - prose: test:Notebook runs end-to-end without errors
      - prose: test:Output matches claimed metrics
      - prose: artifact:experiments/mmm_v2/validation_notebook.html
      - prose: critic:academic_rigor
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-3.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-3.2
      title: Write comprehensive ML validation documentation
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:docs/ML_VALIDATION_COMPLETE.md
      - prose: verification:Links to reproducible notebook
      - prose: verification:Includes limitations section
      - prose: verification:Includes baseline comparisons
      - prose: review:External ML practitioner peer review
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-3.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
- id: E-PHASE0
  title: 'Phase 0: Measurement & Confidence'
  status: done
  domain: product
  milestones:
  - id: M0.1
    title: Measurement & Confidence Foundations
    status: done
    tasks:
    - id: T0.1.1
      title: Implement geo holdout plumbing
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:state/analytics/experiments/geo_holdouts/*.json
      - prose: artifact:state/telemetry/experiments/geo_holdout_runs.jsonl
      - prose: critic:data_quality
      domain: product
      description: Wire apps/validation/incrementality.py into ingestion runs with
        nightly job execution
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T0.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T0.1.2
      title: Build lift & confidence UI surfaces
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:apps/api/schemas/plan.py
      - prose: artifact:apps/web/src/pages/plan.tsx
      - prose: critic:tests
      - prose: critic:design_system
      domain: product
      description: Plan API surfaces experiment payloads; Plan UI renders lift/confidence
        cards with download
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T0.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T0.1.3
      title: Generate forecast calibration report
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:docs/modeling/forecast_calibration_report.md
      - prose: artifact:state/telemetry/calibration/*.json
      - prose: critic:forecast_stitch
      domain: product
      description: Quantile calibration metrics with summary published to docs
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T0.1.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
- id: E-PHASE1
  title: 'Phase 1: Experience Delivery'
  status: done
  domain: product
  milestones:
  - id: M1.1
    title: Experience Delivery MVP
    status: done
    tasks:
    - id: T1.1.3
      title: Wire onboarding progress API
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:apps/api/routes/onboarding.py
      - prose: artifact:apps/web/src/hooks/useOnboardingProgress.ts
      - prose: critic:tests
      domain: product
      description: Implement GET/POST /onboarding/progress routes with telemetry instrumentation
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T1.1.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
- id: E-REMEDIATION
  title: '[CRITICAL] Quality Remediation - Audit All Completed Work'
  status: blocked
  domain: product
  milestones:
  - id: M-MLR-2
    title: M-MLR-2
    status: pending
    tasks:
    - id: T-MLR-2.3
      title: Train models on all 20 synthetic tenants with cross-validation
      status: blocked
      dependencies:
        depends_on:
        - T-MLR-2.2
      exit_criteria: []
      domain: product
      description: Train models on all 20 synthetic tenants with cross-validation
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-2.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M-MLR-3
    title: M-MLR-3
    status: blocked
    tasks:
    - id: T-MLR-3.3
      title: Package all evidence artifacts for review
      status: blocked
      dependencies:
        depends_on:
        - T-MLR-3.2
      exit_criteria: []
      domain: product
      description: Package all evidence artifacts for review
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-3.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M-MLR-4
    title: M-MLR-4
    status: blocked
    tasks:
    - id: T-MLR-4.1
      title: Deploy ModelingReality_v2 critic to production
      status: pending
      dependencies:
        depends_on:
        - T-MLR-0.1
        - T-MLR-3.3
      exit_criteria: []
      domain: product
      description: Deploy ModelingReality_v2 critic to production
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-4.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-4.2
      title: Update autopilot policy to require critic approval
      status: done
      dependencies:
        depends_on:
        - T-MLR-4.1
      exit_criteria: []
      domain: product
      description: Update autopilot policy to require critic approval
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-4.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-4.3
      title: Create meta-critic to review past completed ML tasks
      status: blocked
      dependencies:
        depends_on:
        - T-MLR-4.2
      exit_criteria: []
      domain: product
      description: Create meta-critic to review past completed ML tasks
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-4.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T-MLR-4.4
      title: Document lessons learned and update contributor guide
      status: pending
      dependencies:
        depends_on:
        - T-MLR-4.3
      exit_criteria: []
      domain: product
      description: Document lessons learned and update contributor guide
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T-MLR-4.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M-REM-1
    title: '[CRITICAL] Core Infrastructure Audit'
    status: blocked
    tasks:
    - id: CRIT-PERF-ACADEMICRIGOR-b0301a
      title: '[Critic:academicrigor] Restore performance'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: "Critic academicrigor is underperforming and needs immediate remediation.\n\
        \nIdentity: Academic Rigor (academic_rigor, authority advisory)\nMission:\
        \ Safeguard academic_rigor discipline.\nSignature powers: Reports on findings\
        \ when configuration is missing.\n\nCritic academicrigor failed 10 of the\
        \ last 11 runs with 0 consecutive failures.\n\nObservation window: 11 runs\n\
        \nConsecutive failures: 0\n\nFailures: 10 | Successes: 1\n\nAssigned to: Autopilot\n\
        \nExpectations:\n- Diagnose root causes for the critic's repeated failures.\n\
        - Patch critic configuration, training data, or underlying automation as needed.\n\
        - Document findings in state/context.md and roadmap notes.\n- Close this task\
        \ once the critic passes reliably.\n\nLatest output snippet:\n{\n  \"epic\"\
        : \"E12\",\n  \"summary\": {\n    \"done\": 15,\n    \"in_progress\": 0,\n\
        \    \"pending\": 0,\n    \"blocked\": 0\n  },\n  \"critical_issues\": [],\n\
        \  \"high_issues\": [],\n  \"warnings\": [],\n  \"doc_path\": \"/Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/docs/CRITIQUE_E12_20251022.md\"\
        \n}"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-ACADEMICRIGOR-b0301a
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-ACADEMICRIGOR-f89932
      title: '[Critic:academicrigor] Restore performance'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: "Critic academicrigor is underperforming and needs immediate remediation.\n\
        \nIdentity: Academic Rigor (academic_rigor, authority advisory)\nMission:\
        \ Safeguard academic_rigor discipline.\nSignature powers: Reports on findings\
        \ when configuration is missing.\n\nCritic academicrigor failed 8 of the last\
        \ 10 runs with 0 consecutive failures.\n\nObservation window: 10 runs\n\n\
        Consecutive failures: 0\n\nFailures: 8 | Successes: 2\n\nAssigned to: Autopilot\n\
        \nExpectations:\n- Diagnose root causes for the critic's repeated failures.\n\
        - Patch critic configuration, training data, or underlying automation as needed.\n\
        - Document findings in state/context.md and roadmap notes.\n- Close this task\
        \ once the critic passes reliably.\n\nLatest output snippet:\n{\n  \"epic\"\
        : \"E12\",\n  \"summary\": {\n    \"done\": 15,\n    \"in_progress\": 0,\n\
        \    \"pending\": 0,\n    \"blocked\": 0\n  },\n  \"critical_issues\": [],\n\
        \  \"high_issues\": [],\n  \"warnings\": [],\n  \"doc_path\": \"/Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/docs/CRITIQUE_E12_20251022.md\"\
        \n}"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-ACADEMICRIGOR-f89932
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-ALLOCATOR-bc8604
      title: '[Critic:allocator] Restore performance'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Critic allocator is underperforming and needs immediate remediation.


        Identity: Allocator Sentinel (operations, authority advisory)

        Mission: Ensure planner allocation and task routing stay optimal.

        Signature powers: Diagnoses misrouted tasks and capacity imbalances.; Suggests
        rebalancing across agents and squads.

        Autonomy guidance: Auto-adjust planner weights when safe; escalate persistent
        misallocations to Autopilot.


        Critic allocator failed 8 of the last 10 runs with 0 consecutive failures.


        Observation window: 10 runs


        Consecutive failures: 0


        Failures: 8 | Successes: 2


        Assigned to: Autopilot


        Expectations:

        - Diagnose root causes for the critic''s repeated failures.

        - Patch critic configuration, training data, or underlying automation as needed.

        - Document findings in state/context.md and roadmap notes.

        - Close this task once the critic passes reliably.


        Latest output snippet:

        ============================= test session starts ==============================

        platform darwin -- Python 3.10.12, pytest-8.4.2, pluggy-1.6.0

        rootdir: /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane

        configfile: pyproject.toml

        plugins: anyio-3.7.1, asyncio-1.2.0

        asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None,
        asyncio_default_test_loop_scope=function

        collected 5 items


        tests/test_allocator_routes.py ..                                        [
        40%]

        tests/test_creative_route.py .                                           [
        60%]

        tests/apps/model/test_cre...'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-ALLOCATOR-bc8604
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-BUILD-958e1f
      title: '[Critic:build] Restore performance'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: "Critic build is underperforming and needs immediate remediation.\n\
        \nIdentity: Build Sentinel (engineering, authority blocking)\nMission: Guarantee\
        \ that core build processes remain reproducible and optimized across environments.\n\
        Signature powers: Diagnoses build pipeline regressions and unstable toolchains.;\
        \ Flags missing build artifacts or misconfigured dependencies before release.\n\
        Autonomy guidance: Attempt automated patching of build scripts when safe;\
        \ escalate infrastructure escalations beyond local fixes.\n\nCritic build\
        \ failed 5 of the last 6 runs with 0 consecutive failures.\n\nObservation\
        \ window: 6 runs\n\nConsecutive failures: 0\n\nFailures: 5 | Successes: 1\n\
        \nAssigned to: Autopilot\n\nExpectations:\n- Diagnose root causes for the\
        \ critic's repeated failures.\n- Patch critic configuration, training data,\
        \ or underlying automation as needed.\n- Document findings in state/context.md\
        \ and roadmap notes.\n- Close this task once the critic passes reliably.\n\
        \nLatest output snippet:\nwarning: The top-level linter settings are deprecated\
        \ in favour of their counterparts in the `lint` section. Please update the\
        \ following options in `pyproject.toml`:\n  - 'select' -> 'lint.select'"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-BUILD-958e1f
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-CAUSAL-070d3d
      title: '[Critic:causal] Restore performance'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Critic causal is underperforming and needs immediate remediation.


        Identity: Causal Strategist (ml, authority critical)

        Mission: Guarantee counterfactual validity and causal modeling rigor.

        Signature powers: Checks identifying assumptions, invariances, and instrumentation.;
        Constructs mitigation plans for confounding risks.

        Autonomy guidance: Partner with Research Orchestrator on complex interventions;
        log learnings for future experiments.


        No successful runs recorded in the last 12 observations; 12 consecutive failures
        detected.


        Observation window: 12 runs


        Consecutive failures: 12


        Failures: 12 | Successes: 0


        Assigned to: Director Dana


        Expectations:

        - Diagnose root causes for the critic''s repeated failures.

        - Patch critic configuration, training data, or underlying automation as needed.

        - Document findings in state/context.md and roadmap notes.

        - Close this task once the critic passes reliably.


        Latest output snippet:

        skipped due to capability profile'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-CAUSAL-070d3d
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-CAUSAL-e7682e
      title: '[Critic:causal] Restore performance'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: "Critic causal is underperforming and needs immediate remediation.\n\
        \nIdentity: Causal Strategist (ml, authority critical)\nMission: Guarantee\
        \ counterfactual validity and causal modeling rigor.\nSignature powers: Checks\
        \ identifying assumptions, invariances, and instrumentation.; Constructs mitigation\
        \ plans for confounding risks.\nAutonomy guidance: Partner with Research Orchestrator\
        \ on complex interventions; log learnings for future experiments.\n\nCritic\
        \ causal failed 8 of the last 10 runs with 0 consecutive failures.\n\nObservation\
        \ window: 10 runs\n\nConsecutive failures: 0\n\nFailures: 8 | Successes: 2\n\
        \nAssigned to: Autopilot\n\nExpectations:\n- Diagnose root causes for the\
        \ critic's repeated failures.\n- Patch critic configuration, training data,\
        \ or underlying automation as needed.\n- Document findings in state/context.md\
        \ and roadmap notes.\n- Close this task once the critic passes reliably.\n\
        \nLatest output snippet:\n{\n  \"status\": \"passed\",\n  \"level\": \"medium\"\
        ,\n  \"findings\": [\n    {\n      \"severity\": \"INFO\",\n      \"message\"\
        : \"Weather shock estimator present.\",\n      \"details\": null\n    },\n\
        \    {\n      \"severity\": \"INFO\",\n      \"message\": \"Weather shock\
        \ tests passed.\",\n      \"details\": \"  /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/shared/libs/causal/weather_shock.py:194:\
        \ DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\\\
        n  (Deprecated in version 0.20.5)\\n    pl.count().alias(\\\"pre_count\\\"\
        ),\\n\\ntests/shared/libs/causal/test_weather_shock.py::test_weather_shock..."
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-CAUSAL-e7682e
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-DESIGNSYSTEM-1a886a
      title: '[Critic:designsystem] Restore performance'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Critic designsystem is underperforming and needs immediate remediation.


        Identity: Design System (design_system, authority advisory)

        Mission: Safeguard design_system discipline.

        Signature powers: Reports on findings when configuration is missing.


        No successful runs recorded in the last 9 observations; 5 consecutive failures
        detected.


        Observation window: 9 runs


        Consecutive failures: 5


        Failures: 6 | Successes: 3


        Assigned to: Director Dana


        Expectations:

        - Diagnose root causes for the critic''s repeated failures.

        - Patch critic configuration, training data, or underlying automation as needed.

        - Document findings in state/context.md and roadmap notes.

        - Close this task once the critic passes reliably.


        Latest output snippet:

        ./src/pages/dashboard.tsx

        968:14  Error: Parsing error: '')'' expected.


        info  - Need to disable some ESLint rules? Learn more here: https://nextjs.org/docs/basic-features/eslint#disabling-rules'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-DESIGNSYSTEM-1a886a
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-EXECREVIEW-ef2384
      title: '[Critic:execreview] Restore performance'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Critic execreview is underperforming and needs immediate remediation.


        Identity: Exec Review (exec_review, authority advisory)

        Mission: Safeguard exec_review discipline.

        Signature powers: Reports on findings when configuration is missing.


        No successful runs recorded in the last 12 observations; 12 consecutive failures
        detected.


        Observation window: 12 runs


        Consecutive failures: 12


        Failures: 12 | Successes: 0


        Assigned to: Director Dana


        Expectations:

        - Diagnose root causes for the critic''s repeated failures.

        - Patch critic configuration, training data, or underlying automation as needed.

        - Document findings in state/context.md and roadmap notes.

        - Close this task once the critic passes reliably.


        Latest output snippet:

        skipped due to capability profile'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-EXECREVIEW-ef2384
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-GLOBAL-9882b7
      title: '[Critics] Systemic performance remediation'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Multiple critics are underperforming and require coordinated intervention.


        3 critics require director-level intervention after repeated failures.


        Affected critics: execreview, integrationfury, managerselfcheck


        Critics evaluated in run: 5


        Reports captured: 3


        Assigned to: Director Dana


        Expectations:

        - Review individual remediation tasks and look for systemic issues.

        - Adjust critic configurations, training loops, or staffing mixes.

        - Provide a coordination brief in state/context.md.

        - Close this systemic task once individual critics are back on track.'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-GLOBAL-9882b7
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-HEALTHCHECK-0e6b67
      title: '[Critic:healthcheck] Restore performance'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: "Critic healthcheck is underperforming and needs immediate remediation.\n\
        \nIdentity: Health Check (health_check, authority advisory)\nMission: Safeguard\
        \ health_check discipline.\nSignature powers: Reports on findings when configuration\
        \ is missing.\n\nCritic healthcheck failed 4 of the last 5 runs with 0 consecutive\
        \ failures.\n\nObservation window: 5 runs\n\nConsecutive failures: 0\n\nFailures:\
        \ 4 | Successes: 1\n\nAssigned to: Autopilot\n\nExpectations:\n- Diagnose\
        \ root causes for the critic's repeated failures.\n- Patch critic configuration,\
        \ training data, or underlying automation as needed.\n- Document findings\
        \ in state/context.md and roadmap notes.\n- Close this task once the critic\
        \ passes reliably.\n\nLatest output snippet:\nwarning: The top-level linter\
        \ settings are deprecated in favour of their counterparts in the `lint` section.\
        \ Please update the following options in `pyproject.toml`:\n  - 'select' ->\
        \ 'lint.select'"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-HEALTHCHECK-0e6b67
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-INTEGRATIONFURY-9401af
      title: '[Critic:integrationfury] Restore performance'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Critic integrationfury is underperforming and needs immediate
        remediation.


        Identity: Integration Fury (integration_fury, authority advisory)

        Mission: Safeguard integration_fury discipline.

        Signature powers: Reports on findings when configuration is missing.


        No successful runs recorded in the last 6 observations; 6 consecutive failures
        detected.


        Observation window: 6 runs


        Consecutive failures: 6


        Failures: 6 | Successes: 0


        Assigned to: Director Dana


        Expectations:

        - Diagnose root causes for the critic''s repeated failures.

        - Patch critic configuration, training data, or underlying automation as needed.

        - Document findings in state/context.md and roadmap notes.

        - Close this task once the critic passes reliably.


        Latest output snippet:

        ============================= test session starts ==============================

        platform darwin -- Python 3.10.12, pytest-8.4.2, pluggy-1.6.0

        rootdir: /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane

        configfile: pyproject.toml

        plugins: anyio-4.11.0, asyncio-1.2.0

        asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None,
        asyncio_default_test_loop_scope=function

        collected 242 items


        tests/api/onboarding/test_progress.py ....                               [  1%]

        tests/api/test_ad_push_routes.py ....                                    [  3%]

        tests/api/test_dashboa...'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-INTEGRATIONFURY-9401af
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-MANAGERSELFCHECK-61ab48
      title: '[Critic:managerselfcheck] Restore performance'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Critic managerselfcheck is underperforming and needs immediate
        remediation.


        Identity: Manager Self Check (manager_self_check, authority advisory)

        Mission: Safeguard manager_self_check discipline.

        Signature powers: Reports on findings when configuration is missing.


        No successful runs recorded in the last 6 observations; 6 consecutive failures
        detected.


        Observation window: 6 runs


        Consecutive failures: 6


        Failures: 6 | Successes: 0


        Assigned to: Director Dana


        Expectations:

        - Diagnose root causes for the critic''s repeated failures.

        - Patch critic configuration, training data, or underlying automation as needed.

        - Document findings in state/context.md and roadmap notes.

        - Close this task once the critic passes reliably.


        Latest output snippet:

        Rollback simulation stale (simulated_at=2025-10-15T21:05:00+00:00); rerun
        executor to refresh promotion gate.'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-MANAGERSELFCHECK-61ab48
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-ORGPM-be2140
      title: '[Critic:orgpm] Restore performance'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Critic orgpm is underperforming and needs immediate remediation.


        Identity: Org Pm (org_pm, authority advisory)

        Mission: Safeguard org_pm discipline.

        Signature powers: Reports on findings when configuration is missing.


        Critic orgpm failed 5 of the last 6 runs with 0 consecutive failures.


        Observation window: 6 runs


        Consecutive failures: 0


        Failures: 5 | Successes: 1


        Assigned to: Autopilot


        Expectations:

        - Diagnose root causes for the critic''s repeated failures.

        - Patch critic configuration, training data, or underlying automation as needed.

        - Document findings in state/context.md and roadmap notes.

        - Close this task once the critic passes reliably.


        Latest output snippet:

        Org PM charter/state checks passed.'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-ORGPM-be2140
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-PROMPTBUDGET-2c30f3
      title: '[Critic:promptbudget] Restore performance'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Critic promptbudget is underperforming and needs immediate remediation.


        Identity: Prompt Budget (prompt_budget, authority advisory)

        Mission: Safeguard prompt_budget discipline.

        Signature powers: Reports on findings when configuration is missing.


        Critic promptbudget failed 5 of the last 6 runs with 0 consecutive failures.


        Observation window: 6 runs


        Consecutive failures: 0


        Failures: 5 | Successes: 1


        Assigned to: Autopilot


        Expectations:

        - Diagnose root causes for the critic''s repeated failures.

        - Patch critic configuration, training data, or underlying automation as needed.

        - Document findings in state/context.md and roadmap notes.

        - Close this task once the critic passes reliably.


        Latest output snippet:

        {"level":"warning","message":"Code search index rebuild failed","timestamp":"2025-10-16T20:39:47.650Z","error":"The
        database connection is not open"}'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-PROMPTBUDGET-2c30f3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-SECURITY-645edd
      title: '[Critic:security] Restore performance'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: 'Critic security is underperforming and needs immediate remediation.


        Identity: Security Sentinel (security, authority critical)

        Mission: Guard secrets, policies, and attack surfaces throughout the stack.

        Signature powers: Identifies credential leaks, insecure defaults, and policy
        gaps.; Cross-references security playbooks to recommend mitigations.

        Autonomy guidance: Demand sign-off for high-risk findings; coordinate with
        Director Dana and Security Stewards.


        No successful runs recorded in the last 9 observations; 6 consecutive failures
        detected.


        Observation window: 9 runs


        Consecutive failures: 6


        Failures: 6 | Successes: 3


        Assigned to: Director Dana


        Expectations:

        - Diagnose root causes for the critic''s repeated failures.

        - Patch critic configuration, training data, or underlying automation as needed.

        - Document findings in state/context.md and roadmap notes.

        - Close this task once the critic passes reliably.


        Latest output snippet:

        skipped due to capability profile'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-SECURITY-645edd
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-TESTS-426598
      title: '[Critic:tests] Restore performance'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: "Critic tests is underperforming and needs immediate remediation.\n\
        \nIdentity: Regression Hunter (quality, authority blocking)\nMission: Keep\
        \ the test suites healthy and ensure deterministic results across flows.\n\
        Signature powers: Surfaces flaky suites and failing assertions with reproduction\
        \ notes.; Synthesizes minimal repro commands for Autopilot triage.\nAutonomy\
        \ guidance: Rerun targeted suites automatically; lean on Autopilot only when\
        \ new failures persist.\n\nCritic tests failed 5 of the last 6 runs with 0\
        \ consecutive failures.\n\nObservation window: 6 runs\n\nConsecutive failures:\
        \ 0\n\nFailures: 5 | Successes: 1\n\nAssigned to: Autopilot\n\nExpectations:\n\
        - Diagnose root causes for the critic's repeated failures.\n- Patch critic\
        \ configuration, training data, or underlying automation as needed.\n- Document\
        \ findings in state/context.md and roadmap notes.\n- Close this task once\
        \ the critic passes reliably.\n\nLatest output snippet:\n\e[33mThe CJS build\
        \ of Vite's Node API is deprecated. See https://vite.dev/guide/troubleshooting.html#vite-cjs-node-api-deprecated\
        \ for more details.\e[39m\n{\"level\":\"info\",\"message\":\"Subscription\
        \ limit tracker initialized\",\"timestamp\":\"2025-10-16T21:37:34.835Z\",\"\
        providers\":[]}\n{\"level\":\"info\",\"message\":\"Provider registered for\
        \ usage tracking\",\"timestamp\":\"2025-10-16T21:37:34.837Z\",\"provider\"\
        :\"claude\",\"account\":\"test-account\",\"tier\":\"pro\"}\n{\"level\":\"\
        info\",\"message\":\"Subscription limit tracker stopped\",\"timestamp\":\"\
        2025-10-16T21:37:34.840Z\"}\n{\"level\":\"info\",\"message\":\"Subscription\
        \ limit tracker ..."
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-TESTS-426598
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: CRIT-PERF-TESTS-c3fce7
      title: '[Critic:tests] Restore performance'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: "Critic tests is underperforming and needs immediate remediation.\n\
        \nIdentity: Regression Hunter (quality, authority blocking)\nMission: Keep\
        \ the test suites healthy and ensure deterministic results across flows.\n\
        Signature powers: Surfaces flaky suites and failing assertions with reproduction\
        \ notes.; Synthesizes minimal repro commands for Autopilot triage.\nAutonomy\
        \ guidance: Rerun targeted suites automatically; lean on Autopilot only when\
        \ new failures persist.\n\nCritic tests failed 5 of the last 6 runs with 4\
        \ consecutive failures.\n\nObservation window: 6 runs\n\nConsecutive failures:\
        \ 4\n\nFailures: 5 | Successes: 1\n\nAssigned to: Autopilot\n\nExpectations:\n\
        - Diagnose root causes for the critic's repeated failures.\n- Patch critic\
        \ configuration, training data, or underlying automation as needed.\n- Document\
        \ findings in state/context.md and roadmap notes.\n- Close this task once\
        \ the critic passes reliably.\n\nLatest output snippet:\n\e[33mThe CJS build\
        \ of Vite's Node API is deprecated. See https://vite.dev/guide/troubleshooting.html#vite-cjs-node-api-deprecated\
        \ for more details.\e[39m\n\u23AF\u23AF\u23AF\u23AF\u23AF\u23AF\u23AF Failed\
        \ Tests 6 \u23AF\u23AF\u23AF\u23AF\u23AF\u23AF\u23AF\n\n FAIL  ../../tests/web/design_system_acceptance.spec.ts\
        \ > Design system acceptance \u2013 Stories page > renders skip navigation,\
        \ main landmark, and story metadata tokens\nAssertionError: expected \"error\"\
        \ to not be called at all, but actually been called 5 times\n\nReceived: \n\
        \n  1st error call:\n\n    Array [\n      \"Warning: An update to %s inside\
        \ a test was not wrapped in act(...).\n    \n    When testing, code that cau..."
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/CRIT-PERF-TESTS-c3fce7
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: PHASE-1-HARDENING
      title: 'Phase 1: MCP Hardening'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: "BLOCKING \u2013 must complete before other work (disable YAML\
        \ writes, real usage/cost, correlation IDs, coordinator failover)"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/PHASE-1-HARDENING
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: PHASE-2-COMPACT
      title: 'Phase 2: Compact Prompts + Selfchecks'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: "BLOCKING \u2013 must complete before other work (compact context\
        \ assembler, snapshot selfcheck)"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/PHASE-2-COMPACT
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: PHASE-3-BATCH
      title: 'Phase 3: Batch Queue & Prompt Headers'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: "BLOCKING \u2013 must complete before other work (batch queue,\
        \ stable headers, token heuristics)"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/PHASE-3-BATCH
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-0.1
      title: '[REM] Verify: Create ModelingReality critic with quantitative thresholds'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T-MLR-0.1 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Create ModelingReality critic with quantitative\
        \ thresholds\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T-MLR-0.1\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T-MLR-0.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T-MLR-0.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T-MLR-0.1\n   - Check for superficial completion (empty metrics,\
        \ unused infrastructure)\n   - Verify no test manipulation (tests changed\
        \ to pass without fixing bugs)\n   - Check for documentation-code mismatches\n\
        \n7. **Integration**:\n   - Is task T-MLR-0.1 actually integrated into the\
        \ system?\n   - Can you demonstrate it working in context?\n   - Are there\
        \ any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n\
        - Missing code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR\
        \ fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation\
        \ issues\n- Doc mismatches \u2192 Update docs to match code OR implement missing\
        \ features\n- No runtime evidence \u2192 Actually run it and capture evidence\n\
        - Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n-\
        \ \u2705 Code exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705\
        \ Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial\
        \ detector: APPROVED\n- \u2705 Integration verified (feature works in context)\n\
        - \u2705 No critical issues found\n\n**SEVERITY**: Task T-MLR-0.1 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming\
        \ remediation complete\n\n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\
        \n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-0.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-0.2
      title: '[REM] Verify: Update all ML task exit criteria with objective metrics'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T-MLR-0.2 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Update all ML task exit criteria with objective\
        \ metrics\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T-MLR-0.2\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T-MLR-0.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T-MLR-0.2\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T-MLR-0.2\n   - Check for superficial completion (empty metrics,\
        \ unused infrastructure)\n   - Verify no test manipulation (tests changed\
        \ to pass without fixing bugs)\n   - Check for documentation-code mismatches\n\
        \n7. **Integration**:\n   - Is task T-MLR-0.2 actually integrated into the\
        \ system?\n   - Can you demonstrate it working in context?\n   - Are there\
        \ any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n\
        - Missing code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR\
        \ fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation\
        \ issues\n- Doc mismatches \u2192 Update docs to match code OR implement missing\
        \ features\n- No runtime evidence \u2192 Actually run it and capture evidence\n\
        - Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n-\
        \ \u2705 Code exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705\
        \ Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial\
        \ detector: APPROVED\n- \u2705 Integration verified (feature works in context)\n\
        - \u2705 No critical issues found\n\n**SEVERITY**: Task T-MLR-0.2 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming\
        \ remediation complete\n\n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\
        \n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-0.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-1.1
      title: '[REM] Verify: Debug and fix weather multiplier logic in data generator'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T-MLR-1.1 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Debug and fix weather multiplier logic in\
        \ data generator\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n \
        \  - Locate all files modified/created for task T-MLR-1.1\n   - Verify code\
        \ is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for\
        \ task T-MLR-1.1\n   - Run tests: npm test or pytest (must ALL pass)\n   -\
        \ Verify tests are meaningful (check behavior, not just run code)\n   - Check\
        \ test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   -\
        \ Run build: npm run build or python build.py (0 errors)\n   - Fix any TypeScript\
        \ errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T-MLR-1.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T-MLR-1.1\n   - Check for superficial completion (empty metrics,\
        \ unused infrastructure)\n   - Verify no test manipulation (tests changed\
        \ to pass without fixing bugs)\n   - Check for documentation-code mismatches\n\
        \n7. **Integration**:\n   - Is task T-MLR-1.1 actually integrated into the\
        \ system?\n   - Can you demonstrate it working in context?\n   - Are there\
        \ any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n\
        - Missing code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR\
        \ fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation\
        \ issues\n- Doc mismatches \u2192 Update docs to match code OR implement missing\
        \ features\n- No runtime evidence \u2192 Actually run it and capture evidence\n\
        - Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n-\
        \ \u2705 Code exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705\
        \ Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial\
        \ detector: APPROVED\n- \u2705 Integration verified (feature works in context)\n\
        - \u2705 No critical issues found\n\n**SEVERITY**: Task T-MLR-1.1 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming\
        \ remediation complete\n\n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\
        \n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-1.3
      title: '[REM] Verify: Create validation tests for synthetic data quality'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T-MLR-1.3 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Create validation tests for synthetic data\
        \ quality\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T-MLR-1.3\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T-MLR-1.3\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T-MLR-1.3\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T-MLR-1.3\n   - Check for superficial completion (empty metrics,\
        \ unused infrastructure)\n   - Verify no test manipulation (tests changed\
        \ to pass without fixing bugs)\n   - Check for documentation-code mismatches\n\
        \n7. **Integration**:\n   - Is task T-MLR-1.3 actually integrated into the\
        \ system?\n   - Can you demonstrate it working in context?\n   - Are there\
        \ any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n\
        - Missing code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR\
        \ fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation\
        \ issues\n- Doc mismatches \u2192 Update docs to match code OR implement missing\
        \ features\n- No runtime evidence \u2192 Actually run it and capture evidence\n\
        - Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n-\
        \ \u2705 Code exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705\
        \ Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial\
        \ detector: APPROVED\n- \u2705 Integration verified (feature works in context)\n\
        - \u2705 No critical issues found\n\n**SEVERITY**: Task T-MLR-1.3 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming\
        \ remediation complete\n\n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\
        \n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-1.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-2.1
      title: '[REM] Verify: Implement proper train/val/test splitting with no leakage'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T-MLR-2.1 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Implement proper train/val/test splitting\
        \ with no leakage\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n\
        \   - Locate all files modified/created for task T-MLR-2.1\n   - Verify code\
        \ is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for\
        \ task T-MLR-2.1\n   - Run tests: npm test or pytest (must ALL pass)\n   -\
        \ Verify tests are meaningful (check behavior, not just run code)\n   - Check\
        \ test coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   -\
        \ Run build: npm run build or python build.py (0 errors)\n   - Fix any TypeScript\
        \ errors, import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T-MLR-2.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T-MLR-2.1\n   - Check for superficial completion (empty metrics,\
        \ unused infrastructure)\n   - Verify no test manipulation (tests changed\
        \ to pass without fixing bugs)\n   - Check for documentation-code mismatches\n\
        \n7. **Integration**:\n   - Is task T-MLR-2.1 actually integrated into the\
        \ system?\n   - Can you demonstrate it working in context?\n   - Are there\
        \ any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n\
        - Missing code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR\
        \ fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation\
        \ issues\n- Doc mismatches \u2192 Update docs to match code OR implement missing\
        \ features\n- No runtime evidence \u2192 Actually run it and capture evidence\n\
        - Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n-\
        \ \u2705 Code exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705\
        \ Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial\
        \ detector: APPROVED\n- \u2705 Integration verified (feature works in context)\n\
        - \u2705 No critical issues found\n\n**SEVERITY**: Task T-MLR-2.1 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming\
        \ remediation complete\n\n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\
        \n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-2.2
      title: '[REM] Verify: Implement LightweightMMM with weather features'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T-MLR-2.2 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Implement LightweightMMM with weather features\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T-MLR-2.2\n   - Verify code is not empty, stub,\
        \ or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T-MLR-2.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T-MLR-2.2\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T-MLR-2.2\n   - Check for superficial completion (empty metrics,\
        \ unused infrastructure)\n   - Verify no test manipulation (tests changed\
        \ to pass without fixing bugs)\n   - Check for documentation-code mismatches\n\
        \n7. **Integration**:\n   - Is task T-MLR-2.2 actually integrated into the\
        \ system?\n   - Can you demonstrate it working in context?\n   - Are there\
        \ any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n\
        - Missing code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR\
        \ fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation\
        \ issues\n- Doc mismatches \u2192 Update docs to match code OR implement missing\
        \ features\n- No runtime evidence \u2192 Actually run it and capture evidence\n\
        - Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n-\
        \ \u2705 Code exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705\
        \ Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial\
        \ detector: APPROVED\n- \u2705 Integration verified (feature works in context)\n\
        - \u2705 No critical issues found\n\n**SEVERITY**: Task T-MLR-2.2 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming\
        \ remediation complete\n\n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\
        \n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-2.5
      title: '[REM] Verify: Compare models to baseline (naive/seasonal/linear)'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T-MLR-2.5 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Compare models to baseline (naive/seasonal/linear)\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T-MLR-2.5\n   - Verify code is not empty, stub,\
        \ or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T-MLR-2.5\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T-MLR-2.5\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T-MLR-2.5\n   - Check for superficial completion (empty metrics,\
        \ unused infrastructure)\n   - Verify no test manipulation (tests changed\
        \ to pass without fixing bugs)\n   - Check for documentation-code mismatches\n\
        \n7. **Integration**:\n   - Is task T-MLR-2.5 actually integrated into the\
        \ system?\n   - Can you demonstrate it working in context?\n   - Are there\
        \ any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n\
        - Missing code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR\
        \ fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation\
        \ issues\n- Doc mismatches \u2192 Update docs to match code OR implement missing\
        \ features\n- No runtime evidence \u2192 Actually run it and capture evidence\n\
        - Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n-\
        \ \u2705 Code exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705\
        \ Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial\
        \ detector: APPROVED\n- \u2705 Integration verified (feature works in context)\n\
        - \u2705 No critical issues found\n\n**SEVERITY**: Task T-MLR-2.5 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming\
        \ remediation complete\n\n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\
        \n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-2.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T-MLR-3.2
      title: '[REM] Verify: Write comprehensive ML validation documentation'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T-MLR-3.2 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Write comprehensive ML validation documentation\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T-MLR-3.2\n   - Verify code is not empty, stub,\
        \ or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T-MLR-3.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T-MLR-3.2\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T-MLR-3.2\n   - Check for superficial completion (empty metrics,\
        \ unused infrastructure)\n   - Verify no test manipulation (tests changed\
        \ to pass without fixing bugs)\n   - Check for documentation-code mismatches\n\
        \n7. **Integration**:\n   - Is task T-MLR-3.2 actually integrated into the\
        \ system?\n   - Can you demonstrate it working in context?\n   - Are there\
        \ any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n\
        - Missing code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR\
        \ fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation\
        \ issues\n- Doc mismatches \u2192 Update docs to match code OR implement missing\
        \ features\n- No runtime evidence \u2192 Actually run it and capture evidence\n\
        - Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n-\
        \ \u2705 Code exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705\
        \ Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial\
        \ detector: APPROVED\n- \u2705 Integration verified (feature works in context)\n\
        - \u2705 No critical issues found\n\n**SEVERITY**: Task T-MLR-3.2 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming\
        \ remediation complete\n\n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\
        \n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T-MLR-3.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T0.1.1
      title: '[REM] Verify: Implement geo holdout plumbing'
      status: blocked
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T0.1.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Implement geo holdout plumbing\n\n**VERIFICATION CHECKLIST**:\n\
        \n1. **Code Exists**:\n   - Locate all files modified/created for task T0.1.1\n\
        \   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   -\
        \ Find test files for task T0.1.1\n   - Run tests: npm test or pytest (must\
        \ ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build\
        \ Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n\
        \   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T0.1.1\n   - Verify every\
        \ claimed feature has actual implementation\n   - Check for lies: Features\
        \ documented but not implemented\n   - Update docs if implementation differs\
        \ from claims\n\n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end\
        \ with real data\n   - Provide runtime evidence: screenshot/logs showing it\
        \ working\n   - Test error cases: Does it fail gracefully?\n   - Check resource\
        \ usage: Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run\
        \ adversarial_bullshit_detector on task T0.1.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation\
        \ (tests changed to pass without fixing bugs)\n   - Check for documentation-code\
        \ mismatches\n\n7. **Integration**:\n   - Is task T0.1.1 actually integrated\
        \ into the system?\n   - Can you demonstrate it working in context?\n   -\
        \ Are there any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES\
        \ FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192 Fix\
        \ the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix\
        \ compilation issues\n- Doc mismatches \u2192 Update docs to match code OR\
        \ implement missing features\n- No runtime evidence \u2192 Actually run it\
        \ and capture evidence\n- Superficial completion \u2192 Do the real work\n\
        \n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n- \u2705 All tests\
        \ pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches\
        \ implementation\n- \u2705 Runtime evidence provided (screenshot/logs)\n-\
        \ \u2705 Adversarial detector: APPROVED\n- \u2705 Integration verified (feature\
        \ works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task\
        \ T0.1.1 was marked \"done\" but needs verification\n**PRIORITY**: Must verify\
        \ before claiming remediation complete\n\n**MANDATORY EVIDENCE COLLECTION**\
        \ (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T0.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T0.1.2
      title: '[REM] Verify: Build lift & confidence UI surfaces'
      status: blocked
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T0.1.2 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Build lift & confidence UI surfaces\n\n**VERIFICATION\
        \ CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T0.1.2\n   - Verify code is not empty, stub, or placeholder\n \
        \  - Check for TODO/FIXME comments indicating incomplete work\n\n2. **Tests\
        \ Exist & Pass**:\n   - Find test files for task T0.1.2\n   - Run tests: npm\
        \ test or pytest (must ALL pass)\n   - Verify tests are meaningful (check\
        \ behavior, not just run code)\n   - Check test coverage for new code (target:\
        \ 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py\
        \ (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\
        \n4. **Documentation Matches Code**:\n   - Read documentation for task T0.1.2\n\
        \   - Verify every claimed feature has actual implementation\n   - Check for\
        \ lies: Features documented but not implemented\n   - Update docs if implementation\
        \ differs from claims\n\n5. **Runtime Works**:\n   - Actually RUN the feature\
        \ end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n  \
        \ - Check resource usage: Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n\
        \   - Run adversarial_bullshit_detector on task T0.1.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation\
        \ (tests changed to pass without fixing bugs)\n   - Check for documentation-code\
        \ mismatches\n\n7. **Integration**:\n   - Is task T0.1.2 actually integrated\
        \ into the system?\n   - Can you demonstrate it working in context?\n   -\
        \ Are there any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES\
        \ FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192 Fix\
        \ the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix\
        \ compilation issues\n- Doc mismatches \u2192 Update docs to match code OR\
        \ implement missing features\n- No runtime evidence \u2192 Actually run it\
        \ and capture evidence\n- Superficial completion \u2192 Do the real work\n\
        \n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n- \u2705 All tests\
        \ pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches\
        \ implementation\n- \u2705 Runtime evidence provided (screenshot/logs)\n-\
        \ \u2705 Adversarial detector: APPROVED\n- \u2705 Integration verified (feature\
        \ works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task\
        \ T0.1.2 was marked \"done\" but needs verification\n**PRIORITY**: Must verify\
        \ before claiming remediation complete\n\n**MANDATORY EVIDENCE COLLECTION**\
        \ (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T0.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T0.1.3
      title: '[REM] Verify: Generate forecast calibration report'
      status: blocked
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T0.1.3 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Generate forecast calibration report\n\n**VERIFICATION\
        \ CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T0.1.3\n   - Verify code is not empty, stub, or placeholder\n \
        \  - Check for TODO/FIXME comments indicating incomplete work\n\n2. **Tests\
        \ Exist & Pass**:\n   - Find test files for task T0.1.3\n   - Run tests: npm\
        \ test or pytest (must ALL pass)\n   - Verify tests are meaningful (check\
        \ behavior, not just run code)\n   - Check test coverage for new code (target:\
        \ 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py\
        \ (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\
        \n4. **Documentation Matches Code**:\n   - Read documentation for task T0.1.3\n\
        \   - Verify every claimed feature has actual implementation\n   - Check for\
        \ lies: Features documented but not implemented\n   - Update docs if implementation\
        \ differs from claims\n\n5. **Runtime Works**:\n   - Actually RUN the feature\
        \ end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n  \
        \ - Check resource usage: Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n\
        \   - Run adversarial_bullshit_detector on task T0.1.3\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation\
        \ (tests changed to pass without fixing bugs)\n   - Check for documentation-code\
        \ mismatches\n\n7. **Integration**:\n   - Is task T0.1.3 actually integrated\
        \ into the system?\n   - Can you demonstrate it working in context?\n   -\
        \ Are there any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES\
        \ FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192 Fix\
        \ the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix\
        \ compilation issues\n- Doc mismatches \u2192 Update docs to match code OR\
        \ implement missing features\n- No runtime evidence \u2192 Actually run it\
        \ and capture evidence\n- Superficial completion \u2192 Do the real work\n\
        \n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n- \u2705 All tests\
        \ pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches\
        \ implementation\n- \u2705 Runtime evidence provided (screenshot/logs)\n-\
        \ \u2705 Adversarial detector: APPROVED\n- \u2705 Integration verified (feature\
        \ works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task\
        \ T0.1.3 was marked \"done\" but needs verification\n**PRIORITY**: Must verify\
        \ before claiming remediation complete\n\n**MANDATORY EVIDENCE COLLECTION**\
        \ (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T0.1.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T1.1.1
      title: '[REM] Verify: Design Open-Meteo + Shopify connectors and data contracts'
      status: blocked
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T1.1.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Design Open-Meteo + Shopify connectors and data contracts\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T1.1.1\n   - Verify code is not empty, stub, or\
        \ placeholder\n   - Check for TODO/FIXME comments indicating incomplete work\n\
        \n2. **Tests Exist & Pass**:\n   - Find test files for task T1.1.1\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful\
        \ (check behavior, not just run code)\n   - Check test coverage for new code\
        \ (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or\
        \ python build.py (0 errors)\n   - Fix any TypeScript errors, import errors,\
        \ or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read documentation\
        \ for task T1.1.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update\
        \ docs if implementation differs from claims\n\n5. **Runtime Works**:\n  \
        \ - Actually RUN the feature end-to-end with real data\n   - Provide runtime\
        \ evidence: screenshot/logs showing it working\n   - Test error cases: Does\
        \ it fail gracefully?\n   - Check resource usage: Memory leaks? CPU spikes?\n\
        \n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task\
        \ T1.1.1\n   - Check for superficial completion (empty metrics, unused infrastructure)\n\
        \   - Verify no test manipulation (tests changed to pass without fixing bugs)\n\
        \   - Check for documentation-code mismatches\n\n7. **Integration**:\n   -\
        \ Is task T1.1.1 actually integrated into the system?\n   - Can you demonstrate\
        \ it working in context?\n   - Are there any dead code paths that never execute?\n\
        \n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n-\
        \ Failing tests \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n\
        - Build errors \u2192 Fix compilation issues\n- Doc mismatches \u2192 Update\
        \ docs to match code OR implement missing features\n- No runtime evidence\
        \ \u2192 Actually run it and capture evidence\n- Superficial completion \u2192\
        \ Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n\
        - \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705\
        \ Documentation matches implementation\n- \u2705 Runtime evidence provided\
        \ (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\
        \n**SEVERITY**: Task T1.1.1 was marked \"done\" but needs verification\n**PRIORITY**:\
        \ Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n\
        \   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T1.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T1.1.2
      title: '[REM] Verify: Implement ingestion Prefect flow with checkpointing'
      status: blocked
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T1.1.2 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Implement ingestion Prefect flow with checkpointing\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T1.1.2\n   - Verify code is not empty, stub, or\
        \ placeholder\n   - Check for TODO/FIXME comments indicating incomplete work\n\
        \n2. **Tests Exist & Pass**:\n   - Find test files for task T1.1.2\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful\
        \ (check behavior, not just run code)\n   - Check test coverage for new code\
        \ (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or\
        \ python build.py (0 errors)\n   - Fix any TypeScript errors, import errors,\
        \ or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read documentation\
        \ for task T1.1.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update\
        \ docs if implementation differs from claims\n\n5. **Runtime Works**:\n  \
        \ - Actually RUN the feature end-to-end with real data\n   - Provide runtime\
        \ evidence: screenshot/logs showing it working\n   - Test error cases: Does\
        \ it fail gracefully?\n   - Check resource usage: Memory leaks? CPU spikes?\n\
        \n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task\
        \ T1.1.2\n   - Check for superficial completion (empty metrics, unused infrastructure)\n\
        \   - Verify no test manipulation (tests changed to pass without fixing bugs)\n\
        \   - Check for documentation-code mismatches\n\n7. **Integration**:\n   -\
        \ Is task T1.1.2 actually integrated into the system?\n   - Can you demonstrate\
        \ it working in context?\n   - Are there any dead code paths that never execute?\n\
        \n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n-\
        \ Failing tests \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n\
        - Build errors \u2192 Fix compilation issues\n- Doc mismatches \u2192 Update\
        \ docs to match code OR implement missing features\n- No runtime evidence\
        \ \u2192 Actually run it and capture evidence\n- Superficial completion \u2192\
        \ Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n\
        - \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705\
        \ Documentation matches implementation\n- \u2705 Runtime evidence provided\
        \ (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\
        \n**SEVERITY**: Task T1.1.2 was marked \"done\" but needs verification\n**PRIORITY**:\
        \ Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n\
        \   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T1.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T1.1.3
      title: '[REM] Verify: Wire onboarding progress API'
      status: needs_review
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T1.1.3 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Wire onboarding progress API\n\n**VERIFICATION CHECKLIST**:\n\
        \n1. **Code Exists**:\n   - Locate all files modified/created for task T1.1.3\n\
        \   - Verify code is not empty, stub, or placeholder\n   - Check for TODO/FIXME\
        \ comments indicating incomplete work\n\n2. **Tests Exist & Pass**:\n   -\
        \ Find test files for task T1.1.3\n   - Run tests: npm test or pytest (must\
        \ ALL pass)\n   - Verify tests are meaningful (check behavior, not just run\
        \ code)\n   - Check test coverage for new code (target: 80%+)\n\n3. **Build\
        \ Passes**:\n   - Run build: npm run build or python build.py (0 errors)\n\
        \   - Fix any TypeScript errors, import errors, or syntax issues\n\n4. **Documentation\
        \ Matches Code**:\n   - Read documentation for task T1.1.3\n   - Verify every\
        \ claimed feature has actual implementation\n   - Check for lies: Features\
        \ documented but not implemented\n   - Update docs if implementation differs\
        \ from claims\n\n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end\
        \ with real data\n   - Provide runtime evidence: screenshot/logs showing it\
        \ working\n   - Test error cases: Does it fail gracefully?\n   - Check resource\
        \ usage: Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run\
        \ adversarial_bullshit_detector on task T1.1.3\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation\
        \ (tests changed to pass without fixing bugs)\n   - Check for documentation-code\
        \ mismatches\n\n7. **Integration**:\n   - Is task T1.1.3 actually integrated\
        \ into the system?\n   - Can you demonstrate it working in context?\n   -\
        \ Are there any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES\
        \ FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192 Fix\
        \ the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix\
        \ compilation issues\n- Doc mismatches \u2192 Update docs to match code OR\
        \ implement missing features\n- No runtime evidence \u2192 Actually run it\
        \ and capture evidence\n- Superficial completion \u2192 Do the real work\n\
        \n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n- \u2705 All tests\
        \ pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches\
        \ implementation\n- \u2705 Runtime evidence provided (screenshot/logs)\n-\
        \ \u2705 Adversarial detector: APPROVED\n- \u2705 Integration verified (feature\
        \ works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task\
        \ T1.1.3 was marked \"done\" but needs verification\n**PRIORITY**: Must verify\
        \ before claiming remediation complete\n\n**MANDATORY EVIDENCE COLLECTION**\
        \ (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T1.1.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T1.2.1
      title: '[REM] Verify: Blend historical + forecast weather, enforce timezone
        alignm'
      status: blocked
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T1.2.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Blend historical + forecast weather, enforce timezone\
        \ alignm\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T1.2.1\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T1.2.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T1.2.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T1.2.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T1.2.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T1.2.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T1.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T1.2.2
      title: '[REM] Verify: Add leakage guardrails to feature builder'
      status: blocked
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T1.2.2 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Add leakage guardrails to feature builder\n\n**VERIFICATION\
        \ CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T1.2.2\n   - Verify code is not empty, stub, or placeholder\n \
        \  - Check for TODO/FIXME comments indicating incomplete work\n\n2. **Tests\
        \ Exist & Pass**:\n   - Find test files for task T1.2.2\n   - Run tests: npm\
        \ test or pytest (must ALL pass)\n   - Verify tests are meaningful (check\
        \ behavior, not just run code)\n   - Check test coverage for new code (target:\
        \ 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py\
        \ (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\
        \n4. **Documentation Matches Code**:\n   - Read documentation for task T1.2.2\n\
        \   - Verify every claimed feature has actual implementation\n   - Check for\
        \ lies: Features documented but not implemented\n   - Update docs if implementation\
        \ differs from claims\n\n5. **Runtime Works**:\n   - Actually RUN the feature\
        \ end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n  \
        \ - Check resource usage: Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n\
        \   - Run adversarial_bullshit_detector on task T1.2.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation\
        \ (tests changed to pass without fixing bugs)\n   - Check for documentation-code\
        \ mismatches\n\n7. **Integration**:\n   - Is task T1.2.2 actually integrated\
        \ into the system?\n   - Can you demonstrate it working in context?\n   -\
        \ Are there any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES\
        \ FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192 Fix\
        \ the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix\
        \ compilation issues\n- Doc mismatches \u2192 Update docs to match code OR\
        \ implement missing features\n- No runtime evidence \u2192 Actually run it\
        \ and capture evidence\n- Superficial completion \u2192 Do the real work\n\
        \n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n- \u2705 All tests\
        \ pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches\
        \ implementation\n- \u2705 Runtime evidence provided (screenshot/logs)\n-\
        \ \u2705 Adversarial detector: APPROVED\n- \u2705 Integration verified (feature\
        \ works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task\
        \ T1.2.2 was marked \"done\" but needs verification\n**PRIORITY**: Must verify\
        \ before claiming remediation complete\n\n**MANDATORY EVIDENCE COLLECTION**\
        \ (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T1.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T10.1.1
      title: '[REM] Verify: Cost telemetry and budget alerts'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T10.1.1 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Cost telemetry and budget alerts\n\n**VERIFICATION\
        \ CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T10.1.1\n   - Verify code is not empty, stub, or placeholder\n\
        \   - Check for TODO/FIXME comments indicating incomplete work\n\n2. **Tests\
        \ Exist & Pass**:\n   - Find test files for task T10.1.1\n   - Run tests:\
        \ npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check\
        \ behavior, not just run code)\n   - Check test coverage for new code (target:\
        \ 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py\
        \ (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\
        \n4. **Documentation Matches Code**:\n   - Read documentation for task T10.1.1\n\
        \   - Verify every claimed feature has actual implementation\n   - Check for\
        \ lies: Features documented but not implemented\n   - Update docs if implementation\
        \ differs from claims\n\n5. **Runtime Works**:\n   - Actually RUN the feature\
        \ end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n  \
        \ - Check resource usage: Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n\
        \   - Run adversarial_bullshit_detector on task T10.1.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation\
        \ (tests changed to pass without fixing bugs)\n   - Check for documentation-code\
        \ mismatches\n\n7. **Integration**:\n   - Is task T10.1.1 actually integrated\
        \ into the system?\n   - Can you demonstrate it working in context?\n   -\
        \ Are there any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES\
        \ FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192 Fix\
        \ the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix\
        \ compilation issues\n- Doc mismatches \u2192 Update docs to match code OR\
        \ implement missing features\n- No runtime evidence \u2192 Actually run it\
        \ and capture evidence\n- Superficial completion \u2192 Do the real work\n\
        \n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n- \u2705 All tests\
        \ pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches\
        \ implementation\n- \u2705 Runtime evidence provided (screenshot/logs)\n-\
        \ \u2705 Adversarial detector: APPROVED\n- \u2705 Integration verified (feature\
        \ works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task\
        \ T10.1.1 was marked \"done\" but needs verification\n**PRIORITY**: Must verify\
        \ before claiming remediation complete\n\n**MANDATORY EVIDENCE COLLECTION**\
        \ (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T10.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T11.1.1
      title: '[REM] Verify: Implement hardware probe & profile persistence'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T11.1.1 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Implement hardware probe & profile persistence\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T11.1.1\n   - Verify code is not empty, stub,\
        \ or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T11.1.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T11.1.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T11.1.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T11.1.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T11.1.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T11.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T11.1.2
      title: '[REM] Verify: Adaptive scheduling for heavy tasks'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T11.1.2 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Adaptive scheduling for heavy tasks\n\n**VERIFICATION\
        \ CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T11.1.2\n   - Verify code is not empty, stub, or placeholder\n\
        \   - Check for TODO/FIXME comments indicating incomplete work\n\n2. **Tests\
        \ Exist & Pass**:\n   - Find test files for task T11.1.2\n   - Run tests:\
        \ npm test or pytest (must ALL pass)\n   - Verify tests are meaningful (check\
        \ behavior, not just run code)\n   - Check test coverage for new code (target:\
        \ 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py\
        \ (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\
        \n4. **Documentation Matches Code**:\n   - Read documentation for task T11.1.2\n\
        \   - Verify every claimed feature has actual implementation\n   - Check for\
        \ lies: Features documented but not implemented\n   - Update docs if implementation\
        \ differs from claims\n\n5. **Runtime Works**:\n   - Actually RUN the feature\
        \ end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n  \
        \ - Check resource usage: Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n\
        \   - Run adversarial_bullshit_detector on task T11.1.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation\
        \ (tests changed to pass without fixing bugs)\n   - Check for documentation-code\
        \ mismatches\n\n7. **Integration**:\n   - Is task T11.1.2 actually integrated\
        \ into the system?\n   - Can you demonstrate it working in context?\n   -\
        \ Are there any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES\
        \ FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192 Fix\
        \ the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix\
        \ compilation issues\n- Doc mismatches \u2192 Update docs to match code OR\
        \ implement missing features\n- No runtime evidence \u2192 Actually run it\
        \ and capture evidence\n- Superficial completion \u2192 Do the real work\n\
        \n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n- \u2705 All tests\
        \ pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches\
        \ implementation\n- \u2705 Runtime evidence provided (screenshot/logs)\n-\
        \ \u2705 Adversarial detector: APPROVED\n- \u2705 Integration verified (feature\
        \ works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task\
        \ T11.1.2 was marked \"done\" but needs verification\n**PRIORITY**: Must verify\
        \ before claiming remediation complete\n\n**MANDATORY EVIDENCE COLLECTION**\
        \ (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T11.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T11.2.1
      title: '[REM] Verify: Design system elevation (motion, typography, theming)'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T11.2.1 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Design system elevation (motion, typography,\
        \ theming)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T11.2.1\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T11.2.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T11.2.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T11.2.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T11.2.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T11.2.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T11.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T11.2.2
      title: '[REM] Verify: Award-level experience audit & remediation'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T11.2.2 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Award-level experience audit & remediation\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T11.2.2\n   - Verify code is not empty, stub,\
        \ or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T11.2.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T11.2.2\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T11.2.2\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T11.2.2 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T11.2.2 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T11.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T11.2.3
      title: '[REM] Verify: Extend calm/aero theme tokens to Automations and Experiments'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T11.2.3 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Extend calm/aero theme tokens to Automations\
        \ and Experiments\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n\
        \   - Locate all files modified/created for task T11.2.3\n   - Verify code\
        \ is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for\
        \ task T11.2.3\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify\
        \ tests are meaningful (check behavior, not just run code)\n   - Check test\
        \ coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build:\
        \ npm run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T11.2.3\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T11.2.3\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T11.2.3 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T11.2.3 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T11.2.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T11.2.4
      title: '[REM] Verify: Refactor landing/marketing gradients into reusable tokens'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T11.2.4 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Refactor landing/marketing gradients into\
        \ reusable tokens\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n\
        \   - Locate all files modified/created for task T11.2.4\n   - Verify code\
        \ is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for\
        \ task T11.2.4\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify\
        \ tests are meaningful (check behavior, not just run code)\n   - Check test\
        \ coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build:\
        \ npm run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T11.2.4\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T11.2.4\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T11.2.4 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T11.2.4 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T11.2.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T11.2.5
      title: '[REM] Verify: Centralize retry button styles in shared component once
        App'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T11.2.5 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Centralize retry button styles in shared\
        \ component once App \n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n\
        \   - Locate all files modified/created for task T11.2.5\n   - Verify code\
        \ is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for\
        \ task T11.2.5\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify\
        \ tests are meaningful (check behavior, not just run code)\n   - Check test\
        \ coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build:\
        \ npm run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T11.2.5\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T11.2.5\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T11.2.5 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T11.2.5 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T11.2.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T11.2.6
      title: '[REM] Verify: Formalize shared panel mixin (border + shadow) to reduce
        ove'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T11.2.6 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Formalize shared panel mixin (border + shadow)\
        \ to reduce ove\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n  \
        \ - Locate all files modified/created for task T11.2.6\n   - Verify code is\
        \ not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for\
        \ task T11.2.6\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify\
        \ tests are meaningful (check behavior, not just run code)\n   - Check test\
        \ coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build:\
        \ npm run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T11.2.6\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T11.2.6\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T11.2.6 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T11.2.6 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T11.2.6
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T12.0.3
      title: '[REM] Verify: Document synthetic tenant characteristics'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T12.0.3 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Document synthetic tenant characteristics\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T12.0.3\n   - Verify code is not empty, stub,\
        \ or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T12.0.3\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T12.0.3\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T12.0.3\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T12.0.3 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T12.0.3 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T12.0.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T12.1.2
      title: '[REM] Verify: Validate feature store joins against historical weather
        base'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T12.1.2 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Validate feature store joins against historical\
        \ weather base\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   -\
        \ Locate all files modified/created for task T12.1.2\n   - Verify code is\
        \ not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for\
        \ task T12.1.2\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify\
        \ tests are meaningful (check behavior, not just run code)\n   - Check test\
        \ coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build:\
        \ npm run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T12.1.2\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T12.1.2\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T12.1.2 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T12.1.2 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T12.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T12.2.1
      title: '[REM] Verify: Backtest weather-aware model vs control across top tenants'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T12.2.1 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Backtest weather-aware model vs control across\
        \ top tenants\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   -\
        \ Locate all files modified/created for task T12.2.1\n   - Verify code is\
        \ not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for\
        \ task T12.2.1\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify\
        \ tests are meaningful (check behavior, not just run code)\n   - Check test\
        \ coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build:\
        \ npm run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T12.2.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T12.2.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T12.2.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T12.2.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T12.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T12.PoC.3
      title: '[REM] Verify: Create PoC demo results and proof brief'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T12.PoC.3 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Create PoC demo results and proof brief\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T12.PoC.3\n   - Verify code is not empty, stub,\
        \ or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T12.PoC.3\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T12.PoC.3\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T12.PoC.3\n   - Check for superficial completion (empty metrics,\
        \ unused infrastructure)\n   - Verify no test manipulation (tests changed\
        \ to pass without fixing bugs)\n   - Check for documentation-code mismatches\n\
        \n7. **Integration**:\n   - Is task T12.PoC.3 actually integrated into the\
        \ system?\n   - Can you demonstrate it working in context?\n   - Are there\
        \ any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n\
        - Missing code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR\
        \ fix the tests (if tests are wrong)\n- Build errors \u2192 Fix compilation\
        \ issues\n- Doc mismatches \u2192 Update docs to match code OR implement missing\
        \ features\n- No runtime evidence \u2192 Actually run it and capture evidence\n\
        - Superficial completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n-\
        \ \u2705 Code exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705\
        \ Build passes (0 errors)\n- \u2705 Documentation matches implementation\n\
        - \u2705 Runtime evidence provided (screenshot/logs)\n- \u2705 Adversarial\
        \ detector: APPROVED\n- \u2705 Integration verified (feature works in context)\n\
        - \u2705 No critical issues found\n\n**SEVERITY**: Task T12.PoC.3 was marked\
        \ \"done\" but needs verification\n**PRIORITY**: Must verify before claiming\
        \ remediation complete\n\n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\
        \n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T12.PoC.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T13.1.1
      title: '[REM] Verify: Validate 90-day tenant data coverage across sales, spend,
        an'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T13.1.1 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Validate 90-day tenant data coverage across\
        \ sales, spend, an\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n\
        \   - Locate all files modified/created for task T13.1.1\n   - Verify code\
        \ is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for\
        \ task T13.1.1\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify\
        \ tests are meaningful (check behavior, not just run code)\n   - Check test\
        \ coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build:\
        \ npm run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T13.1.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T13.1.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T13.1.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T13.1.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T13.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T13.1.3
      title: '[REM] Verify: Implement product taxonomy auto-classification with weather'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T13.1.3 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Implement product taxonomy auto-classification\
        \ with weather \n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n  \
        \ - Locate all files modified/created for task T13.1.3\n   - Verify code is\
        \ not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for\
        \ task T13.1.3\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify\
        \ tests are meaningful (check behavior, not just run code)\n   - Check test\
        \ coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build:\
        \ npm run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T13.1.3\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T13.1.3\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T13.1.3 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T13.1.3 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T13.1.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T13.1.4
      title: '[REM] Verify: Data quality validation framework (verify data fitness
        for M'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T13.1.4 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Data quality validation framework (verify\
        \ data fitness for M\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n\
        \   - Locate all files modified/created for task T13.1.4\n   - Verify code\
        \ is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for\
        \ task T13.1.4\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify\
        \ tests are meaningful (check behavior, not just run code)\n   - Check test\
        \ coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build:\
        \ npm run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T13.1.4\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T13.1.4\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T13.1.4 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T13.1.4 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T13.1.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T13.2.1
      title: '[REM] Verify: Replace heuristic MMM with LightweightMMM adstock+saturation'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T13.2.1 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Replace heuristic MMM with LightweightMMM\
        \ adstock+saturation\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n\
        \   - Locate all files modified/created for task T13.2.1\n   - Verify code\
        \ is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for\
        \ task T13.2.1\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify\
        \ tests are meaningful (check behavior, not just run code)\n   - Check test\
        \ coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build:\
        \ npm run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T13.2.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T13.2.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T13.2.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T13.2.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T13.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T13.2.3
      title: '[REM] Verify: Replace heuristic allocator with constraint-aware optimizer'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T13.2.3 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Replace heuristic allocator with constraint-aware\
        \ optimizer\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T13.2.3\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T13.2.3\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T13.2.3\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T13.2.3\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T13.2.3 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T13.2.3 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T13.2.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T13.3.2
      title: '[REM] Verify: Implement DMA-first geographic aggregation with hierarchical'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T13.3.2 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Implement DMA-first geographic aggregation\
        \ with hierarchical\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n\
        \   - Locate all files modified/created for task T13.3.2\n   - Verify code\
        \ is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for\
        \ task T13.3.2\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify\
        \ tests are meaningful (check behavior, not just run code)\n   - Check test\
        \ coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build:\
        \ npm run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T13.3.2\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T13.3.2\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T13.3.2 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T13.3.2 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T13.3.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T13.4.1
      title: '[REM] Verify: Add modeling reality critic to Autopilot'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T13.4.1 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Add modeling reality critic to Autopilot\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T13.4.1\n   - Verify code is not empty, stub,\
        \ or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T13.4.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T13.4.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T13.4.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T13.4.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T13.4.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T13.4.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T13.5.1
      title: '[REM] Verify: Train weather-aware allocation model on top of MMM baseline'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T13.5.1 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Train weather-aware allocation model on top\
        \ of MMM baseline\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n\
        \   - Locate all files modified/created for task T13.5.1\n   - Verify code\
        \ is not empty, stub, or placeholder\n   - Check for TODO/FIXME comments indicating\
        \ incomplete work\n\n2. **Tests Exist & Pass**:\n   - Find test files for\
        \ task T13.5.1\n   - Run tests: npm test or pytest (must ALL pass)\n   - Verify\
        \ tests are meaningful (check behavior, not just run code)\n   - Check test\
        \ coverage for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build:\
        \ npm run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T13.5.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T13.5.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T13.5.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T13.5.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T13.5.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T2.1.1
      title: '[REM] Verify: Build lag/rolling feature generators with deterministic
        seed'
      status: blocked
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T2.1.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Build lag/rolling feature generators with deterministic\
        \ seed\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T2.1.1\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T2.1.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T2.1.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T2.1.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T2.1.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T2.1.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T2.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T2.2.1
      title: '[REM] Verify: Train weather-aware GAM baseline and document methodology'
      status: needs_review
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T2.2.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Train weather-aware GAM baseline and document methodology\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T2.2.1\n   - Verify code is not empty, stub, or\
        \ placeholder\n   - Check for TODO/FIXME comments indicating incomplete work\n\
        \n2. **Tests Exist & Pass**:\n   - Find test files for task T2.2.1\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful\
        \ (check behavior, not just run code)\n   - Check test coverage for new code\
        \ (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or\
        \ python build.py (0 errors)\n   - Fix any TypeScript errors, import errors,\
        \ or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read documentation\
        \ for task T2.2.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update\
        \ docs if implementation differs from claims\n\n5. **Runtime Works**:\n  \
        \ - Actually RUN the feature end-to-end with real data\n   - Provide runtime\
        \ evidence: screenshot/logs showing it working\n   - Test error cases: Does\
        \ it fail gracefully?\n   - Check resource usage: Memory leaks? CPU spikes?\n\
        \n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task\
        \ T2.2.1\n   - Check for superficial completion (empty metrics, unused infrastructure)\n\
        \   - Verify no test manipulation (tests changed to pass without fixing bugs)\n\
        \   - Check for documentation-code mismatches\n\n7. **Integration**:\n   -\
        \ Is task T2.2.1 actually integrated into the system?\n   - Can you demonstrate\
        \ it working in context?\n   - Are there any dead code paths that never execute?\n\
        \n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n-\
        \ Failing tests \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n\
        - Build errors \u2192 Fix compilation issues\n- Doc mismatches \u2192 Update\
        \ docs to match code OR implement missing features\n- No runtime evidence\
        \ \u2192 Actually run it and capture evidence\n- Superficial completion \u2192\
        \ Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n\
        - \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705\
        \ Documentation matches implementation\n- \u2705 Runtime evidence provided\
        \ (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\
        \n**SEVERITY**: Task T2.2.1 was marked \"done\" but needs verification\n**PRIORITY**:\
        \ Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n\
        \   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T2.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T3.1.1
      title: '[REM] Verify: Implement budget allocator stress tests and regret bounds'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T3.1.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Implement budget allocator stress tests and regret\
        \ bounds\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T3.1.1\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.1.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T3.1.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T3.1.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T3.1.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T3.1.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T3.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T3.2.1
      title: '[REM] Verify: Run design system critic and ensure accessibility coverage'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T3.2.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Run design system critic and ensure accessibility\
        \ coverage\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T3.2.1\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.2.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T3.2.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T3.2.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T3.2.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T3.2.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T3.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T3.2.2
      title: '[REM] Verify: Elevate dashboard storytelling & UX'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T3.2.2 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Elevate dashboard storytelling & UX\n\n**VERIFICATION\
        \ CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T3.2.2\n   - Verify code is not empty, stub, or placeholder\n \
        \  - Check for TODO/FIXME comments indicating incomplete work\n\n2. **Tests\
        \ Exist & Pass**:\n   - Find test files for task T3.2.2\n   - Run tests: npm\
        \ test or pytest (must ALL pass)\n   - Verify tests are meaningful (check\
        \ behavior, not just run code)\n   - Check test coverage for new code (target:\
        \ 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py\
        \ (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\
        \n4. **Documentation Matches Code**:\n   - Read documentation for task T3.2.2\n\
        \   - Verify every claimed feature has actual implementation\n   - Check for\
        \ lies: Features documented but not implemented\n   - Update docs if implementation\
        \ differs from claims\n\n5. **Runtime Works**:\n   - Actually RUN the feature\
        \ end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n  \
        \ - Check resource usage: Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n\
        \   - Run adversarial_bullshit_detector on task T3.2.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation\
        \ (tests changed to pass without fixing bugs)\n   - Check for documentation-code\
        \ mismatches\n\n7. **Integration**:\n   - Is task T3.2.2 actually integrated\
        \ into the system?\n   - Can you demonstrate it working in context?\n   -\
        \ Are there any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES\
        \ FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192 Fix\
        \ the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix\
        \ compilation issues\n- Doc mismatches \u2192 Update docs to match code OR\
        \ implement missing features\n- No runtime evidence \u2192 Actually run it\
        \ and capture evidence\n- Superficial completion \u2192 Do the real work\n\
        \n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n- \u2705 All tests\
        \ pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches\
        \ implementation\n- \u2705 Runtime evidence provided (screenshot/logs)\n-\
        \ \u2705 Adversarial detector: APPROVED\n- \u2705 Integration verified (feature\
        \ works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task\
        \ T3.2.2 was marked \"done\" but needs verification\n**PRIORITY**: Must verify\
        \ before claiming remediation complete\n\n**MANDATORY EVIDENCE COLLECTION**\
        \ (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T3.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T3.3.1
      title: '[REM] Verify: Draft multi-agent charter & delegation mesh (AutoGen/Swarm
        p'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T3.3.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Draft multi-agent charter & delegation mesh (AutoGen/Swarm\
        \ p\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T3.3.1\n   - Verify code is not empty, stub,\
        \ or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.3.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T3.3.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T3.3.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T3.3.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T3.3.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T3.3.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T3.3.2
      title: '[REM] Verify: Implement hierarchical consensus & escalation engine'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T3.3.2 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Implement hierarchical consensus & escalation engine\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T3.3.2\n   - Verify code is not empty, stub, or\
        \ placeholder\n   - Check for TODO/FIXME comments indicating incomplete work\n\
        \n2. **Tests Exist & Pass**:\n   - Find test files for task T3.3.2\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful\
        \ (check behavior, not just run code)\n   - Check test coverage for new code\
        \ (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or\
        \ python build.py (0 errors)\n   - Fix any TypeScript errors, import errors,\
        \ or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read documentation\
        \ for task T3.3.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update\
        \ docs if implementation differs from claims\n\n5. **Runtime Works**:\n  \
        \ - Actually RUN the feature end-to-end with real data\n   - Provide runtime\
        \ evidence: screenshot/logs showing it working\n   - Test error cases: Does\
        \ it fail gracefully?\n   - Check resource usage: Memory leaks? CPU spikes?\n\
        \n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task\
        \ T3.3.2\n   - Check for superficial completion (empty metrics, unused infrastructure)\n\
        \   - Verify no test manipulation (tests changed to pass without fixing bugs)\n\
        \   - Check for documentation-code mismatches\n\n7. **Integration**:\n   -\
        \ Is task T3.3.2 actually integrated into the system?\n   - Can you demonstrate\
        \ it working in context?\n   - Are there any dead code paths that never execute?\n\
        \n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n-\
        \ Failing tests \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n\
        - Build errors \u2192 Fix compilation issues\n- Doc mismatches \u2192 Update\
        \ docs to match code OR implement missing features\n- No runtime evidence\
        \ \u2192 Actually run it and capture evidence\n- Superficial completion \u2192\
        \ Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n\
        - \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705\
        \ Documentation matches implementation\n- \u2705 Runtime evidence provided\
        \ (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\
        \n**SEVERITY**: Task T3.3.2 was marked \"done\" but needs verification\n**PRIORITY**:\
        \ Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n\
        \   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T3.3.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T3.3.3
      title: '[REM] Verify: Build closed-loop simulation harness for autonomous teams'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T3.3.3 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Build closed-loop simulation harness for autonomous\
        \ teams\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T3.3.3\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.3.3\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T3.3.3\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T3.3.3\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T3.3.3 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T3.3.3 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T3.3.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T3.3.4
      title: '[REM] Verify: Instrument dynamic staffing telemetry & learning pipeline'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T3.3.4 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Instrument dynamic staffing telemetry & learning pipeline\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T3.3.4\n   - Verify code is not empty, stub, or\
        \ placeholder\n   - Check for TODO/FIXME comments indicating incomplete work\n\
        \n2. **Tests Exist & Pass**:\n   - Find test files for task T3.3.4\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful\
        \ (check behavior, not just run code)\n   - Check test coverage for new code\
        \ (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or\
        \ python build.py (0 errors)\n   - Fix any TypeScript errors, import errors,\
        \ or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read documentation\
        \ for task T3.3.4\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update\
        \ docs if implementation differs from claims\n\n5. **Runtime Works**:\n  \
        \ - Actually RUN the feature end-to-end with real data\n   - Provide runtime\
        \ evidence: screenshot/logs showing it working\n   - Test error cases: Does\
        \ it fail gracefully?\n   - Check resource usage: Memory leaks? CPU spikes?\n\
        \n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task\
        \ T3.3.4\n   - Check for superficial completion (empty metrics, unused infrastructure)\n\
        \   - Verify no test manipulation (tests changed to pass without fixing bugs)\n\
        \   - Check for documentation-code mismatches\n\n7. **Integration**:\n   -\
        \ Is task T3.3.4 actually integrated into the system?\n   - Can you demonstrate\
        \ it working in context?\n   - Are there any dead code paths that never execute?\n\
        \n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n-\
        \ Failing tests \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n\
        - Build errors \u2192 Fix compilation issues\n- Doc mismatches \u2192 Update\
        \ docs to match code OR implement missing features\n- No runtime evidence\
        \ \u2192 Actually run it and capture evidence\n- Superficial completion \u2192\
        \ Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n\
        - \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705\
        \ Documentation matches implementation\n- \u2705 Runtime evidence provided\
        \ (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\
        \n**SEVERITY**: Task T3.3.4 was marked \"done\" but needs verification\n**PRIORITY**:\
        \ Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n\
        \   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T3.3.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T3.4.1
      title: '[REM] Verify: Implement Plan overview page with weather-driven insights'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T3.4.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Implement Plan overview page with weather-driven insights\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T3.4.1\n   - Verify code is not empty, stub, or\
        \ placeholder\n   - Check for TODO/FIXME comments indicating incomplete work\n\
        \n2. **Tests Exist & Pass**:\n   - Find test files for task T3.4.1\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful\
        \ (check behavior, not just run code)\n   - Check test coverage for new code\
        \ (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or\
        \ python build.py (0 errors)\n   - Fix any TypeScript errors, import errors,\
        \ or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read documentation\
        \ for task T3.4.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update\
        \ docs if implementation differs from claims\n\n5. **Runtime Works**:\n  \
        \ - Actually RUN the feature end-to-end with real data\n   - Provide runtime\
        \ evidence: screenshot/logs showing it working\n   - Test error cases: Does\
        \ it fail gracefully?\n   - Check resource usage: Memory leaks? CPU spikes?\n\
        \n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task\
        \ T3.4.1\n   - Check for superficial completion (empty metrics, unused infrastructure)\n\
        \   - Verify no test manipulation (tests changed to pass without fixing bugs)\n\
        \   - Check for documentation-code mismatches\n\n7. **Integration**:\n   -\
        \ Is task T3.4.1 actually integrated into the system?\n   - Can you demonstrate\
        \ it working in context?\n   - Are there any dead code paths that never execute?\n\
        \n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n-\
        \ Failing tests \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n\
        - Build errors \u2192 Fix compilation issues\n- Doc mismatches \u2192 Update\
        \ docs to match code OR implement missing features\n- No runtime evidence\
        \ \u2192 Actually run it and capture evidence\n- Superficial completion \u2192\
        \ Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n\
        - \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705\
        \ Documentation matches implementation\n- \u2705 Runtime evidence provided\
        \ (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\
        \n**SEVERITY**: Task T3.4.1 was marked \"done\" but needs verification\n**PRIORITY**:\
        \ Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n\
        \   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T3.4.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T3.4.2
      title: '[REM] Verify: Build WeatherOps dashboard with allocator + weather KPIs'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T3.4.2 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Build WeatherOps dashboard with allocator + weather\
        \ KPIs\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T3.4.2\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.4.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T3.4.2\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T3.4.2\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T3.4.2 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T3.4.2 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T3.4.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T3.4.3
      title: '[REM] Verify: Ship Experiments hub UI for uplift & incrementality reviews'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T3.4.3 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Ship Experiments hub UI for uplift & incrementality\
        \ reviews\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T3.4.3\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.4.3\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T3.4.3\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T3.4.3\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T3.4.3 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T3.4.3 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T3.4.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T3.4.4
      title: '[REM] Verify: Deliver storytelling Reports view with weather + spend
        narra'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T3.4.4 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Deliver storytelling Reports view with weather + spend\
        \ narra\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T3.4.4\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.4.4\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T3.4.4\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T3.4.4\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T3.4.4 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T3.4.4 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T3.4.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T3.4.5
      title: '[REM] Verify: Conduct design_system + UX acceptance review across implemen'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T3.4.5 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Conduct design_system + UX acceptance review across\
        \ implemen\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T3.4.5\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.4.5\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T3.4.5\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T3.4.5\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T3.4.5 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T3.4.5 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T3.4.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T3.4.6
      title: '[REM] Verify: Rewrite WeatherOps dashboard around plain-language decisions'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T3.4.6 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Rewrite WeatherOps dashboard around plain-language\
        \ decisions\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T3.4.6\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.4.6\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T3.4.6\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T3.4.6\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T3.4.6 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T3.4.6 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T3.4.6
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T3.4.7
      title: '[REM] Verify: Reimagine Automations change log as a trust-first narrative'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T3.4.7 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Reimagine Automations change log as a trust-first\
        \ narrative\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T3.4.7\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T3.4.7\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T3.4.7\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T3.4.7\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T3.4.7 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T3.4.7 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T3.4.7
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T4.1.10
      title: '[REM] Verify: Cross-market saturation optimization (fairness-aware)'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T4.1.10 was completed correctly with quality.\n\n\
        **ORIGINAL TASK**: [REM] Verify: Cross-market saturation optimization (fairness-aware)\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T4.1.10\n   - Verify code is not empty, stub,\
        \ or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T4.1.10\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T4.1.10\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T4.1.10\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T4.1.10 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T4.1.10 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T4.1.10
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T4.1.3
      title: '[REM] Verify: Causal uplift modeling & incremental lift validation'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T4.1.3 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Causal uplift modeling & incremental lift validation\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T4.1.3\n   - Verify code is not empty, stub, or\
        \ placeholder\n   - Check for TODO/FIXME comments indicating incomplete work\n\
        \n2. **Tests Exist & Pass**:\n   - Find test files for task T4.1.3\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful\
        \ (check behavior, not just run code)\n   - Check test coverage for new code\
        \ (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or\
        \ python build.py (0 errors)\n   - Fix any TypeScript errors, import errors,\
        \ or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read documentation\
        \ for task T4.1.3\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update\
        \ docs if implementation differs from claims\n\n5. **Runtime Works**:\n  \
        \ - Actually RUN the feature end-to-end with real data\n   - Provide runtime\
        \ evidence: screenshot/logs showing it working\n   - Test error cases: Does\
        \ it fail gracefully?\n   - Check resource usage: Memory leaks? CPU spikes?\n\
        \n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task\
        \ T4.1.3\n   - Check for superficial completion (empty metrics, unused infrastructure)\n\
        \   - Verify no test manipulation (tests changed to pass without fixing bugs)\n\
        \   - Check for documentation-code mismatches\n\n7. **Integration**:\n   -\
        \ Is task T4.1.3 actually integrated into the system?\n   - Can you demonstrate\
        \ it working in context?\n   - Are there any dead code paths that never execute?\n\
        \n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n-\
        \ Failing tests \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n\
        - Build errors \u2192 Fix compilation issues\n- Doc mismatches \u2192 Update\
        \ docs to match code OR implement missing features\n- No runtime evidence\
        \ \u2192 Actually run it and capture evidence\n- Superficial completion \u2192\
        \ Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n\
        - \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705\
        \ Documentation matches implementation\n- \u2705 Runtime evidence provided\
        \ (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\
        \n**SEVERITY**: Task T4.1.3 was marked \"done\" but needs verification\n**PRIORITY**:\
        \ Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n\
        \   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T4.1.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T4.1.4
      title: '[REM] Verify: Multi-horizon ensemble forecasting'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T4.1.4 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Multi-horizon ensemble forecasting\n\n**VERIFICATION\
        \ CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T4.1.4\n   - Verify code is not empty, stub, or placeholder\n \
        \  - Check for TODO/FIXME comments indicating incomplete work\n\n2. **Tests\
        \ Exist & Pass**:\n   - Find test files for task T4.1.4\n   - Run tests: npm\
        \ test or pytest (must ALL pass)\n   - Verify tests are meaningful (check\
        \ behavior, not just run code)\n   - Check test coverage for new code (target:\
        \ 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py\
        \ (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\
        \n4. **Documentation Matches Code**:\n   - Read documentation for task T4.1.4\n\
        \   - Verify every claimed feature has actual implementation\n   - Check for\
        \ lies: Features documented but not implemented\n   - Update docs if implementation\
        \ differs from claims\n\n5. **Runtime Works**:\n   - Actually RUN the feature\
        \ end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n  \
        \ - Check resource usage: Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n\
        \   - Run adversarial_bullshit_detector on task T4.1.4\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation\
        \ (tests changed to pass without fixing bugs)\n   - Check for documentation-code\
        \ mismatches\n\n7. **Integration**:\n   - Is task T4.1.4 actually integrated\
        \ into the system?\n   - Can you demonstrate it working in context?\n   -\
        \ Are there any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES\
        \ FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192 Fix\
        \ the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix\
        \ compilation issues\n- Doc mismatches \u2192 Update docs to match code OR\
        \ implement missing features\n- No runtime evidence \u2192 Actually run it\
        \ and capture evidence\n- Superficial completion \u2192 Do the real work\n\
        \n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n- \u2705 All tests\
        \ pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches\
        \ implementation\n- \u2705 Runtime evidence provided (screenshot/logs)\n-\
        \ \u2705 Adversarial detector: APPROVED\n- \u2705 Integration verified (feature\
        \ works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task\
        \ T4.1.4 was marked \"done\" but needs verification\n**PRIORITY**: Must verify\
        \ before claiming remediation complete\n\n**MANDATORY EVIDENCE COLLECTION**\
        \ (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T4.1.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T4.1.5
      title: '[REM] Verify: Non-linear allocation optimizer with constraints (ROAS,
        spen'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T4.1.5 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Non-linear allocation optimizer with constraints (ROAS,\
        \ spen\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T4.1.5\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T4.1.5\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T4.1.5\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T4.1.5\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T4.1.5 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T4.1.5 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T4.1.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T4.1.6
      title: '[REM] Verify: High-frequency spend response modeling (intraday adjustments'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T4.1.6 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: High-frequency spend response modeling (intraday adjustments\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T4.1.6\n   - Verify code is not empty, stub, or\
        \ placeholder\n   - Check for TODO/FIXME comments indicating incomplete work\n\
        \n2. **Tests Exist & Pass**:\n   - Find test files for task T4.1.6\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful\
        \ (check behavior, not just run code)\n   - Check test coverage for new code\
        \ (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or\
        \ python build.py (0 errors)\n   - Fix any TypeScript errors, import errors,\
        \ or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read documentation\
        \ for task T4.1.6\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update\
        \ docs if implementation differs from claims\n\n5. **Runtime Works**:\n  \
        \ - Actually RUN the feature end-to-end with real data\n   - Provide runtime\
        \ evidence: screenshot/logs showing it working\n   - Test error cases: Does\
        \ it fail gracefully?\n   - Check resource usage: Memory leaks? CPU spikes?\n\
        \n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task\
        \ T4.1.6\n   - Check for superficial completion (empty metrics, unused infrastructure)\n\
        \   - Verify no test manipulation (tests changed to pass without fixing bugs)\n\
        \   - Check for documentation-code mismatches\n\n7. **Integration**:\n   -\
        \ Is task T4.1.6 actually integrated into the system?\n   - Can you demonstrate\
        \ it working in context?\n   - Are there any dead code paths that never execute?\n\
        \n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n-\
        \ Failing tests \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n\
        - Build errors \u2192 Fix compilation issues\n- Doc mismatches \u2192 Update\
        \ docs to match code OR implement missing features\n- No runtime evidence\
        \ \u2192 Actually run it and capture evidence\n- Superficial completion \u2192\
        \ Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n\
        - \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705\
        \ Documentation matches implementation\n- \u2705 Runtime evidence provided\
        \ (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\
        \n**SEVERITY**: Task T4.1.6 was marked \"done\" but needs verification\n**PRIORITY**:\
        \ Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n\
        \   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T4.1.6
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T4.1.7
      title: '[REM] Verify: Marketing mix budget solver (multi-channel, weather-aware)'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T4.1.7 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Marketing mix budget solver (multi-channel, weather-aware)\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T4.1.7\n   - Verify code is not empty, stub, or\
        \ placeholder\n   - Check for TODO/FIXME comments indicating incomplete work\n\
        \n2. **Tests Exist & Pass**:\n   - Find test files for task T4.1.7\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful\
        \ (check behavior, not just run code)\n   - Check test coverage for new code\
        \ (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or\
        \ python build.py (0 errors)\n   - Fix any TypeScript errors, import errors,\
        \ or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read documentation\
        \ for task T4.1.7\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update\
        \ docs if implementation differs from claims\n\n5. **Runtime Works**:\n  \
        \ - Actually RUN the feature end-to-end with real data\n   - Provide runtime\
        \ evidence: screenshot/logs showing it working\n   - Test error cases: Does\
        \ it fail gracefully?\n   - Check resource usage: Memory leaks? CPU spikes?\n\
        \n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task\
        \ T4.1.7\n   - Check for superficial completion (empty metrics, unused infrastructure)\n\
        \   - Verify no test manipulation (tests changed to pass without fixing bugs)\n\
        \   - Check for documentation-code mismatches\n\n7. **Integration**:\n   -\
        \ Is task T4.1.7 actually integrated into the system?\n   - Can you demonstrate\
        \ it working in context?\n   - Are there any dead code paths that never execute?\n\
        \n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n-\
        \ Failing tests \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n\
        - Build errors \u2192 Fix compilation issues\n- Doc mismatches \u2192 Update\
        \ docs to match code OR implement missing features\n- No runtime evidence\
        \ \u2192 Actually run it and capture evidence\n- Superficial completion \u2192\
        \ Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n\
        - \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705\
        \ Documentation matches implementation\n- \u2705 Runtime evidence provided\
        \ (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\
        \n**SEVERITY**: Task T4.1.7 was marked \"done\" but needs verification\n**PRIORITY**:\
        \ Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n\
        \   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T4.1.7
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T4.1.8
      title: '[REM] Verify: Reinforcement-learning shadow mode (safe exploration)'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T4.1.8 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Reinforcement-learning shadow mode (safe exploration)\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T4.1.8\n   - Verify code is not empty, stub, or\
        \ placeholder\n   - Check for TODO/FIXME comments indicating incomplete work\n\
        \n2. **Tests Exist & Pass**:\n   - Find test files for task T4.1.8\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful\
        \ (check behavior, not just run code)\n   - Check test coverage for new code\
        \ (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or\
        \ python build.py (0 errors)\n   - Fix any TypeScript errors, import errors,\
        \ or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read documentation\
        \ for task T4.1.8\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update\
        \ docs if implementation differs from claims\n\n5. **Runtime Works**:\n  \
        \ - Actually RUN the feature end-to-end with real data\n   - Provide runtime\
        \ evidence: screenshot/logs showing it working\n   - Test error cases: Does\
        \ it fail gracefully?\n   - Check resource usage: Memory leaks? CPU spikes?\n\
        \n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task\
        \ T4.1.8\n   - Check for superficial completion (empty metrics, unused infrastructure)\n\
        \   - Verify no test manipulation (tests changed to pass without fixing bugs)\n\
        \   - Check for documentation-code mismatches\n\n7. **Integration**:\n   -\
        \ Is task T4.1.8 actually integrated into the system?\n   - Can you demonstrate\
        \ it working in context?\n   - Are there any dead code paths that never execute?\n\
        \n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n-\
        \ Failing tests \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n\
        - Build errors \u2192 Fix compilation issues\n- Doc mismatches \u2192 Update\
        \ docs to match code OR implement missing features\n- No runtime evidence\
        \ \u2192 Actually run it and capture evidence\n- Superficial completion \u2192\
        \ Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n\
        - \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705\
        \ Documentation matches implementation\n- \u2705 Runtime evidence provided\
        \ (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\
        \n**SEVERITY**: Task T4.1.8 was marked \"done\" but needs verification\n**PRIORITY**:\
        \ Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n\
        \   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T4.1.8
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T4.1.9
      title: '[REM] Verify: Creative-level response modeling with brand safety guardrail'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T4.1.9 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Creative-level response modeling with brand safety\
        \ guardrail\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T4.1.9\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T4.1.9\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T4.1.9\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T4.1.9\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T4.1.9 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T4.1.9 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T4.1.9
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T5.1.1
      title: '[REM] Verify: Implement Meta Marketing API client (creative + campaign
        man'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T5.1.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Implement Meta Marketing API client (creative + campaign\
        \ man\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T5.1.1\n   - Verify code is not empty, stub,\
        \ or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T5.1.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T5.1.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T5.1.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T5.1.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T5.1.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T5.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T5.1.2
      title: '[REM] Verify: Meta sandbox and dry-run executor with credential vaulting'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T5.1.2 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Meta sandbox and dry-run executor with credential\
        \ vaulting\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T5.1.2\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T5.1.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T5.1.2\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T5.1.2\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T5.1.2 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T5.1.2 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T5.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T5.2.1
      title: '[REM] Verify: Google Ads API integration (campaign create/update, shared
        b'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T5.2.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Google Ads API integration (campaign create/update,\
        \ shared b\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T5.2.1\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T5.2.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T5.2.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T5.2.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T5.2.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T5.2.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T5.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T5.2.2
      title: '[REM] Verify: Budget reconciliation & spend guardrails across platforms'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T5.2.2 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Budget reconciliation & spend guardrails across platforms\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T5.2.2\n   - Verify code is not empty, stub, or\
        \ placeholder\n   - Check for TODO/FIXME comments indicating incomplete work\n\
        \n2. **Tests Exist & Pass**:\n   - Find test files for task T5.2.2\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful\
        \ (check behavior, not just run code)\n   - Check test coverage for new code\
        \ (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or\
        \ python build.py (0 errors)\n   - Fix any TypeScript errors, import errors,\
        \ or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read documentation\
        \ for task T5.2.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update\
        \ docs if implementation differs from claims\n\n5. **Runtime Works**:\n  \
        \ - Actually RUN the feature end-to-end with real data\n   - Provide runtime\
        \ evidence: screenshot/logs showing it working\n   - Test error cases: Does\
        \ it fail gracefully?\n   - Check resource usage: Memory leaks? CPU spikes?\n\
        \n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task\
        \ T5.2.2\n   - Check for superficial completion (empty metrics, unused infrastructure)\n\
        \   - Verify no test manipulation (tests changed to pass without fixing bugs)\n\
        \   - Check for documentation-code mismatches\n\n7. **Integration**:\n   -\
        \ Is task T5.2.2 actually integrated into the system?\n   - Can you demonstrate\
        \ it working in context?\n   - Are there any dead code paths that never execute?\n\
        \n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n-\
        \ Failing tests \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n\
        - Build errors \u2192 Fix compilation issues\n- Doc mismatches \u2192 Update\
        \ docs to match code OR implement missing features\n- No runtime evidence\
        \ \u2192 Actually run it and capture evidence\n- Superficial completion \u2192\
        \ Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n\
        - \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705\
        \ Documentation matches implementation\n- \u2705 Runtime evidence provided\
        \ (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\
        \n**SEVERITY**: Task T5.2.2 was marked \"done\" but needs verification\n**PRIORITY**:\
        \ Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n\
        \   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T5.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T5.3.1
      title: '[REM] Verify: Dry-run & diff visualizer for ad pushes (pre-flight checks)'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T5.3.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Dry-run & diff visualizer for ad pushes (pre-flight\
        \ checks)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T5.3.1\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T5.3.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T5.3.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T5.3.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T5.3.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T5.3.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T5.3.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T5.3.2
      title: '[REM] Verify: Automated rollback + alerting when performance/regression
        de'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T5.3.2 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Automated rollback + alerting when performance/regression\
        \ de\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all\
        \ files modified/created for task T5.3.2\n   - Verify code is not empty, stub,\
        \ or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T5.3.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T5.3.2\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T5.3.2\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T5.3.2 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T5.3.2 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T5.3.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T6.1.1
      title: '[REM] Verify: MCP server integration tests (all 25 tools across both
        provi'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T6.1.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: MCP server integration tests (all 25 tools across\
        \ both provi\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T6.1.1\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.1.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T6.1.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T6.1.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T6.1.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T6.1.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T6.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T6.1.2
      title: '[REM] Verify: Provider failover testing (token limit simulation & automati'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T6.1.2 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Provider failover testing (token limit simulation\
        \ & automati\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T6.1.2\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.1.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T6.1.2\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T6.1.2\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T6.1.2 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T6.1.2 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T6.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T6.1.3
      title: '[REM] Verify: State persistence testing (checkpoint recovery across
        sessio'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T6.1.3 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: State persistence testing (checkpoint recovery across\
        \ sessio\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T6.1.3\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.1.3\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T6.1.3\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T6.1.3\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T6.1.3 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T6.1.3 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T6.1.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T6.1.4
      title: '[REM] Verify: Quality framework validation (10 dimensions operational)'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T6.1.4 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Quality framework validation (10 dimensions operational)\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T6.1.4\n   - Verify code is not empty, stub, or\
        \ placeholder\n   - Check for TODO/FIXME comments indicating incomplete work\n\
        \n2. **Tests Exist & Pass**:\n   - Find test files for task T6.1.4\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful\
        \ (check behavior, not just run code)\n   - Check test coverage for new code\
        \ (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or\
        \ python build.py (0 errors)\n   - Fix any TypeScript errors, import errors,\
        \ or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read documentation\
        \ for task T6.1.4\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update\
        \ docs if implementation differs from claims\n\n5. **Runtime Works**:\n  \
        \ - Actually RUN the feature end-to-end with real data\n   - Provide runtime\
        \ evidence: screenshot/logs showing it working\n   - Test error cases: Does\
        \ it fail gracefully?\n   - Check resource usage: Memory leaks? CPU spikes?\n\
        \n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task\
        \ T6.1.4\n   - Check for superficial completion (empty metrics, unused infrastructure)\n\
        \   - Verify no test manipulation (tests changed to pass without fixing bugs)\n\
        \   - Check for documentation-code mismatches\n\n7. **Integration**:\n   -\
        \ Is task T6.1.4 actually integrated into the system?\n   - Can you demonstrate\
        \ it working in context?\n   - Are there any dead code paths that never execute?\n\
        \n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n-\
        \ Failing tests \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n\
        - Build errors \u2192 Fix compilation issues\n- Doc mismatches \u2192 Update\
        \ docs to match code OR implement missing features\n- No runtime evidence\
        \ \u2192 Actually run it and capture evidence\n- Superficial completion \u2192\
        \ Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n\
        - \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705\
        \ Documentation matches implementation\n- \u2705 Runtime evidence provided\
        \ (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\
        \n**SEVERITY**: Task T6.1.4 was marked \"done\" but needs verification\n**PRIORITY**:\
        \ Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n\
        \   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T6.1.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T6.2.1
      title: '[REM] Verify: Credentials security audit (auth.json, API keys, token
        rotat'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T6.2.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Credentials security audit (auth.json, API keys, token\
        \ rotat\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T6.2.1\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.2.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T6.2.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T6.2.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T6.2.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T6.2.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T6.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T6.2.2
      title: '[REM] Verify: Error recovery testing (graceful degradation, retry logic)'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T6.2.2 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Error recovery testing (graceful degradation, retry\
        \ logic)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T6.2.2\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.2.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T6.2.2\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T6.2.2\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T6.2.2 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T6.2.2 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T6.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T6.2.3
      title: '[REM] Verify: Schema validation enforcement (all data contracts validated)'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T6.2.3 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Schema validation enforcement (all data contracts\
        \ validated)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T6.2.3\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.2.3\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T6.2.3\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T6.2.3\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T6.2.3 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T6.2.3 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T6.2.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T6.2.4
      title: '[REM] Verify: API rate limiting & exponential backoff (Open-Meteo, Shopify'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T6.2.4 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: API rate limiting & exponential backoff (Open-Meteo,\
        \ Shopify\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T6.2.4\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.2.4\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T6.2.4\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T6.2.4\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T6.2.4 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T6.2.4 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T6.2.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T6.3.1
      title: '[REM] Verify: Performance benchmarking (MCP overhead, checkpoint size,
        tok'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T6.3.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Performance benchmarking (MCP overhead, checkpoint\
        \ size, tok\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T6.3.1\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.3.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T6.3.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T6.3.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T6.3.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T6.3.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T6.3.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T6.3.2
      title: '[REM] Verify: Enhanced observability export (structured logs, metrics
        dash'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T6.3.2 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Enhanced observability export (structured logs, metrics\
        \ dash\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T6.3.2\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.3.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T6.3.2\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T6.3.2\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T6.3.2 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T6.3.2 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T6.3.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T6.3.3
      title: '[REM] Verify: Autopilot loop end-to-end testing (full autonomous cycle
        val'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T6.3.3 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Autopilot loop end-to-end testing (full autonomous\
        \ cycle val\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T6.3.3\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T6.3.3\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T6.3.3\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T6.3.3\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T6.3.3 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T6.3.3 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T6.3.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T6.4.0
      title: '[REM] Verify: Upgrade invariants & preflight guardrails'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T6.4.0 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Upgrade invariants & preflight guardrails\n\n**VERIFICATION\
        \ CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T6.4.0\n   - Verify code is not empty, stub, or placeholder\n \
        \  - Check for TODO/FIXME comments indicating incomplete work\n\n2. **Tests\
        \ Exist & Pass**:\n   - Find test files for task T6.4.0\n   - Run tests: npm\
        \ test or pytest (must ALL pass)\n   - Verify tests are meaningful (check\
        \ behavior, not just run code)\n   - Check test coverage for new code (target:\
        \ 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py\
        \ (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\
        \n4. **Documentation Matches Code**:\n   - Read documentation for task T6.4.0\n\
        \   - Verify every claimed feature has actual implementation\n   - Check for\
        \ lies: Features documented but not implemented\n   - Update docs if implementation\
        \ differs from claims\n\n5. **Runtime Works**:\n   - Actually RUN the feature\
        \ end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n  \
        \ - Check resource usage: Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n\
        \   - Run adversarial_bullshit_detector on task T6.4.0\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation\
        \ (tests changed to pass without fixing bugs)\n   - Check for documentation-code\
        \ mismatches\n\n7. **Integration**:\n   - Is task T6.4.0 actually integrated\
        \ into the system?\n   - Can you demonstrate it working in context?\n   -\
        \ Are there any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES\
        \ FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192 Fix\
        \ the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix\
        \ compilation issues\n- Doc mismatches \u2192 Update docs to match code OR\
        \ implement missing features\n- No runtime evidence \u2192 Actually run it\
        \ and capture evidence\n- Superficial completion \u2192 Do the real work\n\
        \n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n- \u2705 All tests\
        \ pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches\
        \ implementation\n- \u2705 Runtime evidence provided (screenshot/logs)\n-\
        \ \u2705 Adversarial detector: APPROVED\n- \u2705 Integration verified (feature\
        \ works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task\
        \ T6.4.0 was marked \"done\" but needs verification\n**PRIORITY**: Must verify\
        \ before claiming remediation complete\n\n**MANDATORY EVIDENCE COLLECTION**\
        \ (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T6.4.0
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T6.4.1
      title: '[REM] Verify: Live feature flag store with kill switch'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T6.4.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Live feature flag store with kill switch\n\n**VERIFICATION\
        \ CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T6.4.1\n   - Verify code is not empty, stub, or placeholder\n \
        \  - Check for TODO/FIXME comments indicating incomplete work\n\n2. **Tests\
        \ Exist & Pass**:\n   - Find test files for task T6.4.1\n   - Run tests: npm\
        \ test or pytest (must ALL pass)\n   - Verify tests are meaningful (check\
        \ behavior, not just run code)\n   - Check test coverage for new code (target:\
        \ 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py\
        \ (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\
        \n4. **Documentation Matches Code**:\n   - Read documentation for task T6.4.1\n\
        \   - Verify every claimed feature has actual implementation\n   - Check for\
        \ lies: Features documented but not implemented\n   - Update docs if implementation\
        \ differs from claims\n\n5. **Runtime Works**:\n   - Actually RUN the feature\
        \ end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n  \
        \ - Check resource usage: Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n\
        \   - Run adversarial_bullshit_detector on task T6.4.1\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation\
        \ (tests changed to pass without fixing bugs)\n   - Check for documentation-code\
        \ mismatches\n\n7. **Integration**:\n   - Is task T6.4.1 actually integrated\
        \ into the system?\n   - Can you demonstrate it working in context?\n   -\
        \ Are there any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES\
        \ FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192 Fix\
        \ the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix\
        \ compilation issues\n- Doc mismatches \u2192 Update docs to match code OR\
        \ implement missing features\n- No runtime evidence \u2192 Actually run it\
        \ and capture evidence\n- Superficial completion \u2192 Do the real work\n\
        \n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n- \u2705 All tests\
        \ pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches\
        \ implementation\n- \u2705 Runtime evidence provided (screenshot/logs)\n-\
        \ \u2705 Adversarial detector: APPROVED\n- \u2705 Integration verified (feature\
        \ works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task\
        \ T6.4.1 was marked \"done\" but needs verification\n**PRIORITY**: Must verify\
        \ before claiming remediation complete\n\n**MANDATORY EVIDENCE COLLECTION**\
        \ (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T6.4.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T6.4.2
      title: '[REM] Verify: Blue/green worker manager & front-end proxy'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T6.4.2 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Blue/green worker manager & front-end proxy\n\n**VERIFICATION\
        \ CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T6.4.2\n   - Verify code is not empty, stub, or placeholder\n \
        \  - Check for TODO/FIXME comments indicating incomplete work\n\n2. **Tests\
        \ Exist & Pass**:\n   - Find test files for task T6.4.2\n   - Run tests: npm\
        \ test or pytest (must ALL pass)\n   - Verify tests are meaningful (check\
        \ behavior, not just run code)\n   - Check test coverage for new code (target:\
        \ 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py\
        \ (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\
        \n4. **Documentation Matches Code**:\n   - Read documentation for task T6.4.2\n\
        \   - Verify every claimed feature has actual implementation\n   - Check for\
        \ lies: Features documented but not implemented\n   - Update docs if implementation\
        \ differs from claims\n\n5. **Runtime Works**:\n   - Actually RUN the feature\
        \ end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n  \
        \ - Check resource usage: Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n\
        \   - Run adversarial_bullshit_detector on task T6.4.2\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation\
        \ (tests changed to pass without fixing bugs)\n   - Check for documentation-code\
        \ mismatches\n\n7. **Integration**:\n   - Is task T6.4.2 actually integrated\
        \ into the system?\n   - Can you demonstrate it working in context?\n   -\
        \ Are there any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES\
        \ FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192 Fix\
        \ the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix\
        \ compilation issues\n- Doc mismatches \u2192 Update docs to match code OR\
        \ implement missing features\n- No runtime evidence \u2192 Actually run it\
        \ and capture evidence\n- Superficial completion \u2192 Do the real work\n\
        \n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n- \u2705 All tests\
        \ pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches\
        \ implementation\n- \u2705 Runtime evidence provided (screenshot/logs)\n-\
        \ \u2705 Adversarial detector: APPROVED\n- \u2705 Integration verified (feature\
        \ works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task\
        \ T6.4.2 was marked \"done\" but needs verification\n**PRIORITY**: Must verify\
        \ before claiming remediation complete\n\n**MANDATORY EVIDENCE COLLECTION**\
        \ (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T6.4.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T6.4.3
      title: '[REM] Verify: Worker entrypoint with DRY_RUN safeguards'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T6.4.3 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Worker entrypoint with DRY_RUN safeguards\n\n**VERIFICATION\
        \ CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T6.4.3\n   - Verify code is not empty, stub, or placeholder\n \
        \  - Check for TODO/FIXME comments indicating incomplete work\n\n2. **Tests\
        \ Exist & Pass**:\n   - Find test files for task T6.4.3\n   - Run tests: npm\
        \ test or pytest (must ALL pass)\n   - Verify tests are meaningful (check\
        \ behavior, not just run code)\n   - Check test coverage for new code (target:\
        \ 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py\
        \ (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\
        \n4. **Documentation Matches Code**:\n   - Read documentation for task T6.4.3\n\
        \   - Verify every claimed feature has actual implementation\n   - Check for\
        \ lies: Features documented but not implemented\n   - Update docs if implementation\
        \ differs from claims\n\n5. **Runtime Works**:\n   - Actually RUN the feature\
        \ end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n  \
        \ - Check resource usage: Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n\
        \   - Run adversarial_bullshit_detector on task T6.4.3\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation\
        \ (tests changed to pass without fixing bugs)\n   - Check for documentation-code\
        \ mismatches\n\n7. **Integration**:\n   - Is task T6.4.3 actually integrated\
        \ into the system?\n   - Can you demonstrate it working in context?\n   -\
        \ Are there any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES\
        \ FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192 Fix\
        \ the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix\
        \ compilation issues\n- Doc mismatches \u2192 Update docs to match code OR\
        \ implement missing features\n- No runtime evidence \u2192 Actually run it\
        \ and capture evidence\n- Superficial completion \u2192 Do the real work\n\
        \n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n- \u2705 All tests\
        \ pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches\
        \ implementation\n- \u2705 Runtime evidence provided (screenshot/logs)\n-\
        \ \u2705 Adversarial detector: APPROVED\n- \u2705 Integration verified (feature\
        \ works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task\
        \ T6.4.3 was marked \"done\" but needs verification\n**PRIORITY**: Must verify\
        \ before claiming remediation complete\n\n**MANDATORY EVIDENCE COLLECTION**\
        \ (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T6.4.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T6.4.4
      title: '[REM] Verify: Canary upgrade harness & shadow validation'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T6.4.4 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Canary upgrade harness & shadow validation\n\n**VERIFICATION\
        \ CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files modified/created\
        \ for task T6.4.4\n   - Verify code is not empty, stub, or placeholder\n \
        \  - Check for TODO/FIXME comments indicating incomplete work\n\n2. **Tests\
        \ Exist & Pass**:\n   - Find test files for task T6.4.4\n   - Run tests: npm\
        \ test or pytest (must ALL pass)\n   - Verify tests are meaningful (check\
        \ behavior, not just run code)\n   - Check test coverage for new code (target:\
        \ 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or python build.py\
        \ (0 errors)\n   - Fix any TypeScript errors, import errors, or syntax issues\n\
        \n4. **Documentation Matches Code**:\n   - Read documentation for task T6.4.4\n\
        \   - Verify every claimed feature has actual implementation\n   - Check for\
        \ lies: Features documented but not implemented\n   - Update docs if implementation\
        \ differs from claims\n\n5. **Runtime Works**:\n   - Actually RUN the feature\
        \ end-to-end with real data\n   - Provide runtime evidence: screenshot/logs\
        \ showing it working\n   - Test error cases: Does it fail gracefully?\n  \
        \ - Check resource usage: Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n\
        \   - Run adversarial_bullshit_detector on task T6.4.4\n   - Check for superficial\
        \ completion (empty metrics, unused infrastructure)\n   - Verify no test manipulation\
        \ (tests changed to pass without fixing bugs)\n   - Check for documentation-code\
        \ mismatches\n\n7. **Integration**:\n   - Is task T6.4.4 actually integrated\
        \ into the system?\n   - Can you demonstrate it working in context?\n   -\
        \ Are there any dead code paths that never execute?\n\n**WHAT TO FIX IF ISSUES\
        \ FOUND**:\n- Missing code \u2192 Implement it\n- Failing tests \u2192 Fix\
        \ the bugs OR fix the tests (if tests are wrong)\n- Build errors \u2192 Fix\
        \ compilation issues\n- Doc mismatches \u2192 Update docs to match code OR\
        \ implement missing features\n- No runtime evidence \u2192 Actually run it\
        \ and capture evidence\n- Superficial completion \u2192 Do the real work\n\
        \n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n- \u2705 All tests\
        \ pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705 Documentation matches\
        \ implementation\n- \u2705 Runtime evidence provided (screenshot/logs)\n-\
        \ \u2705 Adversarial detector: APPROVED\n- \u2705 Integration verified (feature\
        \ works in context)\n- \u2705 No critical issues found\n\n**SEVERITY**: Task\
        \ T6.4.4 was marked \"done\" but needs verification\n**PRIORITY**: Must verify\
        \ before claiming remediation complete\n\n**MANDATORY EVIDENCE COLLECTION**\
        \ (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T6.4.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T6.4.7
      title: '[REM] Verify: Automatic rollback monitors & kill-switch reset'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T6.4.7 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Automatic rollback monitors & kill-switch reset\n\n\
        **VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T6.4.7\n   - Verify code is not empty, stub, or\
        \ placeholder\n   - Check for TODO/FIXME comments indicating incomplete work\n\
        \n2. **Tests Exist & Pass**:\n   - Find test files for task T6.4.7\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful\
        \ (check behavior, not just run code)\n   - Check test coverage for new code\
        \ (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or\
        \ python build.py (0 errors)\n   - Fix any TypeScript errors, import errors,\
        \ or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read documentation\
        \ for task T6.4.7\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update\
        \ docs if implementation differs from claims\n\n5. **Runtime Works**:\n  \
        \ - Actually RUN the feature end-to-end with real data\n   - Provide runtime\
        \ evidence: screenshot/logs showing it working\n   - Test error cases: Does\
        \ it fail gracefully?\n   - Check resource usage: Memory leaks? CPU spikes?\n\
        \n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task\
        \ T6.4.7\n   - Check for superficial completion (empty metrics, unused infrastructure)\n\
        \   - Verify no test manipulation (tests changed to pass without fixing bugs)\n\
        \   - Check for documentation-code mismatches\n\n7. **Integration**:\n   -\
        \ Is task T6.4.7 actually integrated into the system?\n   - Can you demonstrate\
        \ it working in context?\n   - Are there any dead code paths that never execute?\n\
        \n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n-\
        \ Failing tests \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n\
        - Build errors \u2192 Fix compilation issues\n- Doc mismatches \u2192 Update\
        \ docs to match code OR implement missing features\n- No runtime evidence\
        \ \u2192 Actually run it and capture evidence\n- Superficial completion \u2192\
        \ Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n\
        - \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705\
        \ Documentation matches implementation\n- \u2705 Runtime evidence provided\
        \ (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\
        \n**SEVERITY**: Task T6.4.7 was marked \"done\" but needs verification\n**PRIORITY**:\
        \ Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n\
        \   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T6.4.7
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T6.4.8
      title: '[REM] Verify: Observability & resource budgets during upgrade'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T6.4.8 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Observability & resource budgets during upgrade\n\n\
        **VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T6.4.8\n   - Verify code is not empty, stub, or\
        \ placeholder\n   - Check for TODO/FIXME comments indicating incomplete work\n\
        \n2. **Tests Exist & Pass**:\n   - Find test files for task T6.4.8\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful\
        \ (check behavior, not just run code)\n   - Check test coverage for new code\
        \ (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or\
        \ python build.py (0 errors)\n   - Fix any TypeScript errors, import errors,\
        \ or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read documentation\
        \ for task T6.4.8\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update\
        \ docs if implementation differs from claims\n\n5. **Runtime Works**:\n  \
        \ - Actually RUN the feature end-to-end with real data\n   - Provide runtime\
        \ evidence: screenshot/logs showing it working\n   - Test error cases: Does\
        \ it fail gracefully?\n   - Check resource usage: Memory leaks? CPU spikes?\n\
        \n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task\
        \ T6.4.8\n   - Check for superficial completion (empty metrics, unused infrastructure)\n\
        \   - Verify no test manipulation (tests changed to pass without fixing bugs)\n\
        \   - Check for documentation-code mismatches\n\n7. **Integration**:\n   -\
        \ Is task T6.4.8 actually integrated into the system?\n   - Can you demonstrate\
        \ it working in context?\n   - Are there any dead code paths that never execute?\n\
        \n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n-\
        \ Failing tests \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n\
        - Build errors \u2192 Fix compilation issues\n- Doc mismatches \u2192 Update\
        \ docs to match code OR implement missing features\n- No runtime evidence\
        \ \u2192 Actually run it and capture evidence\n- Superficial completion \u2192\
        \ Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n\
        - \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705\
        \ Documentation matches implementation\n- \u2705 Runtime evidence provided\
        \ (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\
        \n**SEVERITY**: Task T6.4.8 was marked \"done\" but needs verification\n**PRIORITY**:\
        \ Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n\
        \   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T6.4.8
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T7.1.1
      title: '[REM] Verify: Complete geocoding integration (city->lat/lon, cache strateg'
      status: needs_review
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T7.1.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Complete geocoding integration (city->lat/lon, cache\
        \ strateg\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T7.1.1\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T7.1.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T7.1.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T7.1.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T7.1.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T7.1.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T7.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T7.1.2
      title: '[REM] Verify: Weather feature join to model matrix (prevent future leakage'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T7.1.2 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Weather feature join to model matrix (prevent future\
        \ leakage\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T7.1.2\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T7.1.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T7.1.2\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T7.1.2\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T7.1.2 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T7.1.2 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T7.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T7.1.3
      title: '[REM] Verify: Data contract schema validation (Shopify, weather, ads)'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T7.1.3 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Data contract schema validation (Shopify, weather,\
        \ ads)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T7.1.3\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T7.1.3\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T7.1.3\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T7.1.3\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T7.1.3 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T7.1.3 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T7.1.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T7.2.1
      title: '[REM] Verify: Incremental ingestion with deduplication & checkpointing'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T7.2.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Incremental ingestion with deduplication & checkpointing\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T7.2.1\n   - Verify code is not empty, stub, or\
        \ placeholder\n   - Check for TODO/FIXME comments indicating incomplete work\n\
        \n2. **Tests Exist & Pass**:\n   - Find test files for task T7.2.1\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful\
        \ (check behavior, not just run code)\n   - Check test coverage for new code\
        \ (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or\
        \ python build.py (0 errors)\n   - Fix any TypeScript errors, import errors,\
        \ or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read documentation\
        \ for task T7.2.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update\
        \ docs if implementation differs from claims\n\n5. **Runtime Works**:\n  \
        \ - Actually RUN the feature end-to-end with real data\n   - Provide runtime\
        \ evidence: screenshot/logs showing it working\n   - Test error cases: Does\
        \ it fail gracefully?\n   - Check resource usage: Memory leaks? CPU spikes?\n\
        \n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task\
        \ T7.2.1\n   - Check for superficial completion (empty metrics, unused infrastructure)\n\
        \   - Verify no test manipulation (tests changed to pass without fixing bugs)\n\
        \   - Check for documentation-code mismatches\n\n7. **Integration**:\n   -\
        \ Is task T7.2.1 actually integrated into the system?\n   - Can you demonstrate\
        \ it working in context?\n   - Are there any dead code paths that never execute?\n\
        \n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n-\
        \ Failing tests \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n\
        - Build errors \u2192 Fix compilation issues\n- Doc mismatches \u2192 Update\
        \ docs to match code OR implement missing features\n- No runtime evidence\
        \ \u2192 Actually run it and capture evidence\n- Superficial completion \u2192\
        \ Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n\
        - \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705\
        \ Documentation matches implementation\n- \u2705 Runtime evidence provided\
        \ (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\
        \n**SEVERITY**: Task T7.2.1 was marked \"done\" but needs verification\n**PRIORITY**:\
        \ Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n\
        \   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T7.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T7.2.2
      title: '[REM] Verify: Data quality monitoring & alerting (anomaly detection)'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T7.2.2 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Data quality monitoring & alerting (anomaly detection)\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T7.2.2\n   - Verify code is not empty, stub, or\
        \ placeholder\n   - Check for TODO/FIXME comments indicating incomplete work\n\
        \n2. **Tests Exist & Pass**:\n   - Find test files for task T7.2.2\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful\
        \ (check behavior, not just run code)\n   - Check test coverage for new code\
        \ (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or\
        \ python build.py (0 errors)\n   - Fix any TypeScript errors, import errors,\
        \ or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read documentation\
        \ for task T7.2.2\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update\
        \ docs if implementation differs from claims\n\n5. **Runtime Works**:\n  \
        \ - Actually RUN the feature end-to-end with real data\n   - Provide runtime\
        \ evidence: screenshot/logs showing it working\n   - Test error cases: Does\
        \ it fail gracefully?\n   - Check resource usage: Memory leaks? CPU spikes?\n\
        \n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task\
        \ T7.2.2\n   - Check for superficial completion (empty metrics, unused infrastructure)\n\
        \   - Verify no test manipulation (tests changed to pass without fixing bugs)\n\
        \   - Check for documentation-code mismatches\n\n7. **Integration**:\n   -\
        \ Is task T7.2.2 actually integrated into the system?\n   - Can you demonstrate\
        \ it working in context?\n   - Are there any dead code paths that never execute?\n\
        \n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n-\
        \ Failing tests \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n\
        - Build errors \u2192 Fix compilation issues\n- Doc mismatches \u2192 Update\
        \ docs to match code OR implement missing features\n- No runtime evidence\
        \ \u2192 Actually run it and capture evidence\n- Superficial completion \u2192\
        \ Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n\
        - \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705\
        \ Documentation matches implementation\n- \u2705 Runtime evidence provided\
        \ (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\
        \n**SEVERITY**: Task T7.2.2 was marked \"done\" but needs verification\n**PRIORITY**:\
        \ Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n\
        \   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T7.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T8.1.1
      title: '[REM] Verify: Lock MCP schemas to Zod shapes (SAFE: guardrail)'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T8.1.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Lock MCP schemas to Zod shapes (SAFE: guardrail)\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T8.1.1\n   - Verify code is not empty, stub, or\
        \ placeholder\n   - Check for TODO/FIXME comments indicating incomplete work\n\
        \n2. **Tests Exist & Pass**:\n   - Find test files for task T8.1.1\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful\
        \ (check behavior, not just run code)\n   - Check test coverage for new code\
        \ (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or\
        \ python build.py (0 errors)\n   - Fix any TypeScript errors, import errors,\
        \ or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read documentation\
        \ for task T8.1.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update\
        \ docs if implementation differs from claims\n\n5. **Runtime Works**:\n  \
        \ - Actually RUN the feature end-to-end with real data\n   - Provide runtime\
        \ evidence: screenshot/logs showing it working\n   - Test error cases: Does\
        \ it fail gracefully?\n   - Check resource usage: Memory leaks? CPU spikes?\n\
        \n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task\
        \ T8.1.1\n   - Check for superficial completion (empty metrics, unused infrastructure)\n\
        \   - Verify no test manipulation (tests changed to pass without fixing bugs)\n\
        \   - Check for documentation-code mismatches\n\n7. **Integration**:\n   -\
        \ Is task T8.1.1 actually integrated into the system?\n   - Can you demonstrate\
        \ it working in context?\n   - Are there any dead code paths that never execute?\n\
        \n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n-\
        \ Failing tests \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n\
        - Build errors \u2192 Fix compilation issues\n- Doc mismatches \u2192 Update\
        \ docs to match code OR implement missing features\n- No runtime evidence\
        \ \u2192 Actually run it and capture evidence\n- Superficial completion \u2192\
        \ Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n\
        - \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705\
        \ Documentation matches implementation\n- \u2705 Runtime evidence provided\
        \ (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\
        \n**SEVERITY**: Task T8.1.1 was marked \"done\" but needs verification\n**PRIORITY**:\
        \ Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n\
        \   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T8.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T8.1.2
      title: '[REM] Verify: Implement command allow-list in guardrails (SAFE: additive
        s'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T8.1.2 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Implement command allow-list in guardrails (SAFE:\
        \ additive s\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T8.1.2\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T8.1.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T8.1.2\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T8.1.2\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T8.1.2 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T8.1.2 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T8.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T8.1.3
      title: '[REM] Verify: Thread correlation IDs through state transitions (SAFE:
        obse'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T8.1.3 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Thread correlation IDs through state transitions (SAFE:\
        \ obse\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T8.1.3\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T8.1.3\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T8.1.3\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T8.1.3\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T8.1.3 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T8.1.3 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T8.1.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T8.2.1
      title: '[REM] Verify: Implement compact evidence-pack prompt mode (SAFE: new
        funct'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T8.2.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Implement compact evidence-pack prompt mode (SAFE:\
        \ new funct\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T8.2.1\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T8.2.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T8.2.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T8.2.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T8.2.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T8.2.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T8.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T8.2.2
      title: "[REM] Verify: Finalize Claude\u2194Codex coordinator failover (SAFE:\
        \ expose exi"
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T8.2.2 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Finalize Claude\u2194Codex coordinator failover (SAFE:\
        \ expose exi\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T8.2.2\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T8.2.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T8.2.2\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T8.2.2\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T8.2.2 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T8.2.2 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T8.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T9.1.1
      title: '[REM] Verify: Stable prompt headers with provider caching (SAFE: additive'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T9.1.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Stable prompt headers with provider caching (SAFE:\
        \ additive \n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T9.1.1\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T9.1.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T9.1.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T9.1.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T9.1.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T9.1.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T9.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T9.1.2
      title: '[REM] Verify: Batch queue for non-urgent prompts (SAFE: new queueing
        syste'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T9.1.2 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Batch queue for non-urgent prompts (SAFE: new queueing\
        \ syste\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T9.1.2\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T9.1.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T9.1.2\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T9.1.2\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T9.1.2 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T9.1.2 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T9.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T9.2.1
      title: '[REM] Verify: Strict output DSL validation (SAFE: validation layer only)'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T9.2.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Strict output DSL validation (SAFE: validation layer\
        \ only)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T9.2.1\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T9.2.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T9.2.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T9.2.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T9.2.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T9.2.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T9.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T9.2.2
      title: '[REM] Verify: Idempotency keys for mutating tools (SAFE: caching layer)'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T9.2.2 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: Idempotency keys for mutating tools (SAFE: caching\
        \ layer)\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T9.2.2\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T9.2.2\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T9.2.2\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T9.2.2\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T9.2.2 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T9.2.2 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T9.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T9.3.1
      title: '[REM] Verify: OpenTelemetry spans for all operations (SAFE: tracing
        wrappe'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T9.3.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: OpenTelemetry spans for all operations (SAFE: tracing\
        \ wrappe\n\n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate\
        \ all files modified/created for task T9.3.1\n   - Verify code is not empty,\
        \ stub, or placeholder\n   - Check for TODO/FIXME comments indicating incomplete\
        \ work\n\n2. **Tests Exist & Pass**:\n   - Find test files for task T9.3.1\n\
        \   - Run tests: npm test or pytest (must ALL pass)\n   - Verify tests are\
        \ meaningful (check behavior, not just run code)\n   - Check test coverage\
        \ for new code (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm\
        \ run build or python build.py (0 errors)\n   - Fix any TypeScript errors,\
        \ import errors, or syntax issues\n\n4. **Documentation Matches Code**:\n\
        \   - Read documentation for task T9.3.1\n   - Verify every claimed feature\
        \ has actual implementation\n   - Check for lies: Features documented but\
        \ not implemented\n   - Update docs if implementation differs from claims\n\
        \n5. **Runtime Works**:\n   - Actually RUN the feature end-to-end with real\
        \ data\n   - Provide runtime evidence: screenshot/logs showing it working\n\
        \   - Test error cases: Does it fail gracefully?\n   - Check resource usage:\
        \ Memory leaks? CPU spikes?\n\n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector\
        \ on task T9.3.1\n   - Check for superficial completion (empty metrics, unused\
        \ infrastructure)\n   - Verify no test manipulation (tests changed to pass\
        \ without fixing bugs)\n   - Check for documentation-code mismatches\n\n7.\
        \ **Integration**:\n   - Is task T9.3.1 actually integrated into the system?\n\
        \   - Can you demonstrate it working in context?\n   - Are there any dead\
        \ code paths that never execute?\n\n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing\
        \ code \u2192 Implement it\n- Failing tests \u2192 Fix the bugs OR fix the\
        \ tests (if tests are wrong)\n- Build errors \u2192 Fix compilation issues\n\
        - Doc mismatches \u2192 Update docs to match code OR implement missing features\n\
        - No runtime evidence \u2192 Actually run it and capture evidence\n- Superficial\
        \ completion \u2192 Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code\
        \ exists and is complete\n- \u2705 All tests pass (100%)\n- \u2705 Build passes\
        \ (0 errors)\n- \u2705 Documentation matches implementation\n- \u2705 Runtime\
        \ evidence provided (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n\
        - \u2705 Integration verified (feature works in context)\n- \u2705 No critical\
        \ issues found\n\n**SEVERITY**: Task T9.3.1 was marked \"done\" but needs\
        \ verification\n**PRIORITY**: Must verify before claiming remediation complete\n\
        \n**MANDATORY EVIDENCE COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n\
        \   ```bash\n   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T9.3.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REM-T9.4.1
      title: '[REM] Verify: SQLite FTS5 index for code search (SAFE: new index)'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Implementation verified
      - prose: Tests verified
      - prose: Quality gate APPROVED
      - prose: Runtime verification PASSED
      - prose: Critical issues fixed
      domain: product
      description: "VERIFY task T9.4.1 was completed correctly with quality.\n\n**ORIGINAL\
        \ TASK**: [REM] Verify: SQLite FTS5 index for code search (SAFE: new index)\n\
        \n**VERIFICATION CHECKLIST**:\n\n1. **Code Exists**:\n   - Locate all files\
        \ modified/created for task T9.4.1\n   - Verify code is not empty, stub, or\
        \ placeholder\n   - Check for TODO/FIXME comments indicating incomplete work\n\
        \n2. **Tests Exist & Pass**:\n   - Find test files for task T9.4.1\n   - Run\
        \ tests: npm test or pytest (must ALL pass)\n   - Verify tests are meaningful\
        \ (check behavior, not just run code)\n   - Check test coverage for new code\
        \ (target: 80%+)\n\n3. **Build Passes**:\n   - Run build: npm run build or\
        \ python build.py (0 errors)\n   - Fix any TypeScript errors, import errors,\
        \ or syntax issues\n\n4. **Documentation Matches Code**:\n   - Read documentation\
        \ for task T9.4.1\n   - Verify every claimed feature has actual implementation\n\
        \   - Check for lies: Features documented but not implemented\n   - Update\
        \ docs if implementation differs from claims\n\n5. **Runtime Works**:\n  \
        \ - Actually RUN the feature end-to-end with real data\n   - Provide runtime\
        \ evidence: screenshot/logs showing it working\n   - Test error cases: Does\
        \ it fail gracefully?\n   - Check resource usage: Memory leaks? CPU spikes?\n\
        \n6. **Adversarial Checks**:\n   - Run adversarial_bullshit_detector on task\
        \ T9.4.1\n   - Check for superficial completion (empty metrics, unused infrastructure)\n\
        \   - Verify no test manipulation (tests changed to pass without fixing bugs)\n\
        \   - Check for documentation-code mismatches\n\n7. **Integration**:\n   -\
        \ Is task T9.4.1 actually integrated into the system?\n   - Can you demonstrate\
        \ it working in context?\n   - Are there any dead code paths that never execute?\n\
        \n**WHAT TO FIX IF ISSUES FOUND**:\n- Missing code \u2192 Implement it\n-\
        \ Failing tests \u2192 Fix the bugs OR fix the tests (if tests are wrong)\n\
        - Build errors \u2192 Fix compilation issues\n- Doc mismatches \u2192 Update\
        \ docs to match code OR implement missing features\n- No runtime evidence\
        \ \u2192 Actually run it and capture evidence\n- Superficial completion \u2192\
        \ Do the real work\n\n**EXIT CRITERIA**:\n- \u2705 Code exists and is complete\n\
        - \u2705 All tests pass (100%)\n- \u2705 Build passes (0 errors)\n- \u2705\
        \ Documentation matches implementation\n- \u2705 Runtime evidence provided\
        \ (screenshot/logs)\n- \u2705 Adversarial detector: APPROVED\n- \u2705 Integration\
        \ verified (feature works in context)\n- \u2705 No critical issues found\n\
        \n**SEVERITY**: Task T9.4.1 was marked \"done\" but needs verification\n**PRIORITY**:\
        \ Must verify before claiming remediation complete\n\n**MANDATORY EVIDENCE\
        \ COLLECTION** (for quality gates):\n\n1. **BUILD Evidence**:\n   ```bash\n\
        \   cd /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/tools/wvo_mcp\n\
        \   npm run build 2>&1\n   ```\n   - Capture FULL output\n   - Must show \"\
        0 errors\"\n   - Provide output in verification report\n\n2. **TEST Evidence**:\n\
        \   ```bash\n   npm test 2>&1\n   ```\n   - Capture FULL output\n   - Must\
        \ show \"X/X passing\" (all tests pass)\n   - Provide output in verification\
        \ report\n\n3. **AUDIT Evidence**:\n   ```bash\n   npm audit 2>&1\n   ```\n\
        \   - Capture FULL output\n   - Must show \"0 vulnerabilities\"\n   - Provide\
        \ output in verification report\n\n4. **RUNTIME Evidence** (at least ONE of):\n\
        \   - Screenshot of feature running in browser/CLI\n   - Log file from feature\
        \ execution\n   - Artifact created by feature (JSON file, report, etc.)\n\
        \   - Demonstration video/recording\n\n5. **DOCUMENTATION Evidence**:\n  \
        \ - List files modified/created\n   - Quote relevant documentation sections\n\
        \   - Verify docs match implementation\n\n**Quality Gate Checklist**:\n- [\
        \ ] Build output collected (0 errors required)\n- [ ] Test output collected\
        \ (all passing required)\n- [ ] Audit output collected (0 vulnerabilities\
        \ required)\n- [ ] Runtime evidence provided (artifacts/logs/screenshots)\n\
        - [ ] Documentation verified (no mismatches)\n\n**NOTE**: Without ALL evidence\
        \ above, quality gates will AUTOMATICALLY REJECT.\nDo NOT skip evidence collection.\
        \ Do NOT assume quality gates will pass without proof.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REM-T9.4.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REMEDIATION-ALL-MCP-SERVER
      title: '[CRITICAL] Audit ALL MCP server code for quality issues'
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Build passes with 0 errors
      - prose: ALL tests pass (currently 865 tests)
      - prose: npm audit shows 0 vulnerabilities
      - prose: Quality gate adversarial detector APPROVED
      - prose: Runtime evidence provided for each major system
      - prose: No superficial completion detected
      - prose: No documentation-code mismatches
      - prose: Decision log shows APPROVED status
      domain: product
      description: "AUDIT all MCP server implementation for quality issues.\n\n**SCOPE**:\
        \ tools/wvo_mcp/src/ (all TypeScript code)\n\n**WHAT TO AUDIT**:\n1. Orchestrator:\
        \ unified_orchestrator.ts, quality_gate_orchestrator.ts\n2. State Management:\
        \ state_machine.ts, roadmap_tracker.ts\n3. Model Routing: model_router.ts,\
        \ capacity tracking\n4. Telemetry: logging, metrics, analytics\n5. Critics:\
        \ All critic implementations\n6. Resource Management: agent_pool.ts, resource_lifecycle_manager.ts\n\
        \n**VERIFICATION STEPS**:\n1. Build: cd tools/wvo_mcp && npm run build (0\
        \ errors required)\n2. Tests: npm test (ALL must pass, currently 985+)\n3.\
        \ Audit: npm audit (0 vulnerabilities required)\n4. Coverage: npm run test:coverage\
        \ (80%+ on orchestrator code)\n5. Adversarial detector: Run on all modules\n\
        6. Runtime: Start orchestrator, run 1 task end-to-end, collect logs\n7. Decision\
        \ log: Verify state/analytics/quality_gate_decisions.jsonl has real decisions\n\
        \n**EXIT CRITERIA**:\n- \u2705 Build: 0 errors\n- \u2705 Tests: 985/985+ passing\n\
        - \u2705 Audit: 0 vulnerabilities\n- \u2705 Coverage: 80%+ on critical code\n\
        - \u2705 Runtime evidence: Screenshots/logs of orchestrator running\n- \u2705\
        \ Decision log: Real decisions from autopilot (not demos)\n- \u2705 No superficial\
        \ completion detected"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REMEDIATION-ALL-MCP-SERVER
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REMEDIATION-ALL-QUALITY-GATES-DOGFOOD
      title: '[CRITICAL] Integrate multi-domain genius-level reviews (GATE 5)'
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: unknown
      - prose: Decision log shows decisions from REAL tasks (not demos)
      - prose: Post-task verification confirmed in autopilot logs
      - prose: unknown
      - prose: unknown
      domain: product
      description: "INTEGRATE multi-domain genius-level reviews as GATE 5.\n\nTransform\
        \ quality gates from checkbox thinking to expert-level domain analysis.\n\n\
        **WHAT TO BUILD**:\n1. Import DomainExpertReviewer into quality_gate_orchestrator.ts\n\
        2. Add GATE 5: Multi-domain expert review (after GATE 4)\n3. Update QualityGateDecision\
        \ to include domainExpert results\n4. Extend makeConsensusDecision() to check\
        \ domain expert approval\n5. Update TaskEvidence to include title + description\n\
        \n**IMPLEMENTATION**:\n- quality_gate_orchestrator.ts:20-25: Add imports\n\
        - quality_gate_orchestrator.ts:71: Update interface\n- quality_gate_orchestrator.ts:105:\
        \ Instantiate reviewer\n- quality_gate_orchestrator.ts:255-258: Execute GATE\
        \ 5\n- quality_gate_orchestrator.ts:397-421: Update consensus\n- adversarial_bullshit_detector.ts:26-27:\
        \ Add title/description to TaskEvidence\n\n**VERIFICATION**:\n1. Build: cd\
        \ tools/wvo_mcp && npm run build (0 errors)\n2. Tests: npm test (985+ passing)\n\
        3. Audit: npm audit (0 vulnerabilities)\n4. Run orchestrator: See GATE 5 execute\
        \ with 3+ domain experts\n5. Check logs: Domain expert reviews appear in quality_gate_decisions.jsonl\n\
        6. Test rejection: Verify tasks rejected when experts find issues\n\n**EXIT\
        \ CRITERIA**:\n- \u2705 GATE 5 integrated and executing\n- \u2705 Build: 0\
        \ errors\n- \u2705 Tests: 985/985+ passing\n- \u2705 Audit: 0 vulnerabilities\n\
        - \u2705 Logs show domain expert reviews (3+ experts per task)\n- \u2705 Evidence\
        \ document created (docs/DOMAIN_EXPERT_INTEGRATION_EVIDENCE.md)"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REMEDIATION-ALL-QUALITY-GATES-DOGFOOD
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REMEDIATION-ALL-TESTING-INFRASTRUCTURE
      title: '[CRITICAL] Verify testing infrastructure quality'
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: npm test shows 865/865 passing (100%)
      - prose: Test quality validation passes on ALL test files
      - prose: No unconditional success mocks detected
      - prose: Integration tests exist and pass
      - prose: Edge case coverage verified
      domain: product
      description: "VERIFY testing infrastructure is high-quality and tests are meaningful.\n\
        \n**WHAT TO VERIFY**:\n- Test QUALITY: Do tests verify behavior or just run\
        \ code?\n- Test COVERAGE: Is critical code actually tested?\n- Test ASSERTIONS:\
        \ Are assertions checking real conditions?\n- Integration tests: Do they test\
        \ actual integration or just mocks?\n- Runtime tests: Do critical systems\
        \ have end-to-end tests?\n\n**SPECIFIC FILES TO AUDIT**:\n1. adversarial_bullshit_detector.test.ts:\
        \ 15+ tests, all 6 detection categories\n2. quality_gate_orchestrator.test.ts:\
        \ All 5 gates + consensus\n3. unified_orchestrator.test.ts: End-to-end task\
        \ execution\n4. domain_expert_reviewer.test.ts: Multi-domain reviews\n5. state_machine.test.ts:\
        \ State transitions, concurrent access\n6. model_router.test.ts: Model selection,\
        \ capacity tracking\n\n**VERIFICATION STEPS**:\n1. Run tests: npm test (must\
        \ ALL pass)\n2. Coverage: npm run test:coverage (generate report)\n3. Break\
        \ test: Intentionally break critical code, verify tests FAIL\n4. Fix test:\
        \ Fix the break, verify tests PASS\n5. Review assertions: Check every test's\
        \ assertions\n6. Add missing tests: For untested critical code\n\n**QUALITY\
        \ CHECKS**:\n- Are tests checking behavior (GOOD) or just running code (BAD)?\n\
        - Do tests use meaningful assertions (GOOD) or just expect(x).toBeDefined()\
        \ (BAD)?\n- Do integration tests actually integrate (GOOD) or mock everything\
        \ (BAD)?\n- Do tests fail when code breaks (GOOD) or always pass (BAD)?\n\n\
        **EXIT CRITERIA**:\n- \u2705 All tests passing (985+)\n- \u2705 Coverage:\
        \ 80%+ on critical code\n- \u2705 Tests demonstrate they catch bugs (tested\
        \ by breaking code)\n- \u2705 No superficial tests (all verify behavior)\n\
        - \u2705 Integration tests run orchestrator end-to-end\n- \u2705 Evidence:\
        \ Coverage report + test quality analysis"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REMEDIATION-ALL-TESTING-INFRASTRUCTURE
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REMEDIATION-T1.1.2-PREFECT-FLOW
      title: '[URGENT] Convert ingestion to actual Prefect flow'
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Code uses @flow and @task decorators from Prefect
      - prose: Flow can be registered with Prefect server
      - prose: Flow execution produces Prefect UI artifacts
      - prose: Checkpointing uses Prefect state management
      - prose: 'Runtime evidence: Prefect UI screenshot showing flow run'
      - prose: Tests validate Prefect integration
      domain: product
      description: 'REMEDIATION: Task T1.1.2 was marked "done" but WRONG framework
        used.


        **Audit Finding**: Code exists but doesn''t use Prefect.

        - Found: shared/libs/ingestion/ contains code

        - Problem: No @flow or @task decorators found

        - Problem: Not using Prefect framework despite task requirement


        **Required Work**:

        1. Convert ingestion pipeline to use Prefect decorators

        2. Define flow with @flow decorator

        3. Define tasks with @task decorator

        4. Integrate with Prefect state/checkpoint system

        5. Test flow registration and execution

        6. Document Prefect-specific features used


        **Verification Requirements**:

        - grep -r "@flow|@task" shared/libs/ingestion/ returns matches

        - Can run: prefect deployment build (or equivalent)

        - Flow appears in Prefect UI (screenshot required)

        - Tests cover Prefect integration


        **Severity**: HIGH - Wrong technology implementation

        **Priority**: HIGH - Must use correct framework'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REMEDIATION-T1.1.2-PREFECT-FLOW
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REMEDIATION-T2.2.1-GAM-BASELINE
      title: '[URGENT] Implement missing Weather-aware GAM baseline'
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Training script apps/modeling/train_weather_gam.py exists and runs
      - prose: Script produces model artifacts in expected location
      - prose: Documentation in WEATHER_PROOF_OF_CONCEPT.md references actual implementation
      - prose: 'Runtime evidence: screenshot of training run with metrics'
      - prose: Tests exist and pass for GAM training pipeline
      domain: product
      description: 'REMEDIATION: Task T2.2.1 was marked "done" but implementation
        is missing.


        **Audit Finding**: Documentation exists but NO code found.

        - Expected: apps/modeling/train_weather_gam.py or apps/modeling/weather_gam.py

        - Found: Nothing

        - Documentation references features that don''t exist


        **Required Work**:

        1. Implement weather-aware GAM baseline training script

        2. Integrate with existing feature pipeline

        3. Produce model artifacts matching documentation claims

        4. Add tests covering training, prediction, and validation

        5. Run end-to-end and provide runtime evidence


        **Verification Requirements**:

        - Build passes

        - Tests pass

        - Script actually runs: python apps/modeling/train_weather_gam.py --dry-run

        - Produces expected outputs

        - Documentation matches implementation


        **Severity**: HIGH - Claimed feature completely missing

        **Priority**: CRITICAL - Must fix before claiming task complete'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REMEDIATION-T2.2.1-GAM-BASELINE
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: REMEDIATION-T6.3.1-PERF-BENCHMARKING
      title: '[URGENT] Fix empty performance benchmarking system'
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: state/analytics/orchestration_metrics.json contains >0 decision entries
      - prose: Performance benchmarks exist in docs with REAL data
      - prose: MCP overhead measured and documented
      - prose: Checkpoint size limits validated
      - prose: Token usage tracked over time
      - prose: 'Runtime evidence: metrics collection in action'
      domain: product
      description: 'REMEDIATION: Task T6.3.1 was marked "done" but system is EMPTY.


        **Audit Finding**: Infrastructure exists but unused (0 metrics recorded).

        - Found: state/analytics/orchestration_metrics.json (empty: 0 decisions)

        - Found: docs/MODEL_PERFORMANCE_THRESHOLDS.md (generic thresholds, no real
        data)

        - Problem: System built but never actually used


        **Required Work**:

        1. Actually USE the performance monitoring system

        2. Collect real performance data from autopilot runs

        3. Measure MCP overhead vs direct calls

        4. Measure checkpoint sizes over time

        5. Track token efficiency metrics

        6. Update docs with ACTUAL measured data


        **Verification Requirements**:

        - Run autopilot for at least 10 iterations

        - Verify metrics file grows (check file size before/after)

        - Extract sample metrics: jq ''.decisions | length'' state/analytics/orchestration_metrics.json

        - Document shows real numbers from real runs


        **Severity**: MEDIUM - Feature exists but superficially completed

        **Priority**: HIGH - Must demonstrate system actually works'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/REMEDIATION-T6.3.1-PERF-BENCHMARKING
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: TASK-RESEARCH-AD-AUTOMATION
      title: Research Meta/Google ads automation constraints
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: doc:docs/api/ads_capability_matrix.md
      - prose: doc:docs/security/ads_automation_sop.md
      - prose: artifact:state/artifacts/research/ads_api_compliance.json
      domain: product
      description: Consolidate API capabilities, credential flows, and compliance
        requirements so E5 automation tasks can launch with allocator and security
        critics satisfied.
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/TASK-RESEARCH-AD-AUTOMATION
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: TASK-RESEARCH-CONSENSUS-BENCHMARKS
      title: Research staffing heuristics for consensus engine rollout
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:state/analytics/consensus_workload.json
      - prose: doc:docs/research/consensus_staffing_playbook.md
      - prose: artifact:state/analytics/orchestration_metrics.json
      domain: product
      description: Collect decision workload traces, benchmark quorum cost, and codify
        staffing heuristics so T3.3.x tasks can wire consensus + telemetry with real
        evidence.
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/TASK-RESEARCH-CONSENSUS-BENCHMARKS
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: TASK-RESEARCH-DATA-GUARDRAILS
      title: Research ingestion data-quality guardrails
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: doc:docs/research/data_quality_guardrails.md
      - prose: artifact:state/analytics/data_quality_baselines.json
      - prose: artifact:state/artifacts/research/geocoding_coverage_report.json
      domain: product
      description: Define geocoding coverage thresholds, schema validation rules,
        and incremental dedupe checks so E7 and E12/E13 work inherit trusted data.
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/TASK-RESEARCH-DATA-GUARDRAILS
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: TASK-RESEARCH-DEMO
      title: 'Research: evaluate cache warming pattern'
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: Investigate academic cache warming strategies and summarize findings
        for orchestration upgrades.
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/TASK-RESEARCH-DEMO
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: TASK-RESEARCH-EXPERIENCE-VALIDATION
      title: Research Experiments/Reports validation with core personas
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: doc:docs/research/experiments_reports_validation.md
      - prose: artifact:state/artifacts/research/experiments_sessions
      - prose: doc:docs/UX_CRITIQUE.md
      domain: product
      description: Run moderated sessions across Sarah, Leo, and Priya personas, produce
        evidence-backed acceptance metrics, and update UX briefs so T3.4.x implementation
        unblocks without rework.
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/TASK-RESEARCH-EXPERIENCE-VALIDATION
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: TASK-RESEARCH-SWEEP
      title: Research backlog sweep
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: Identify high-impact roadmap areas lacking research coverage and
        propose follow-up research tasks.
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/TASK-RESEARCH-SWEEP
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: TEST-1
      title: Run build and verify no errors
      status: blocked
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: Execute npm run build in tools/wvo_mcp and verify 0 TypeScript
        errors
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/TEST-1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: TEST-2
      title: Run npm audit and verify no vulnerabilities
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: Execute npm audit and verify 0 vulnerabilities found
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/TEST-2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: TEST-3
      title: Review CLAUDE.md for completeness
      status: done
      dependencies:
        depends_on: []
      exit_criteria: []
      domain: product
      description: Read CLAUDE.md and verify all verification loop documentation is
        present
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/TEST-3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  description: 'CRITICAL PRIORITY: Comprehensive quality audit of ALL work completed
    before quality gates.


    Assumption: ALL "done" tasks have quality issues until proven otherwise.


    This epic contains remediation tasks for every major system.

    '
- id: E1
  title: "Epic 1 \u2014 Ingest & Weather Foundations"
  status: done
  domain: product
  milestones:
  - id: M1.1
    title: Connector scaffolding
    status: done
    tasks:
    - id: T1.1.1
      title: Design Open-Meteo + Shopify connectors and data contracts
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:build
      - prose: critic:tests
      - prose: doc:docs/INGESTION.md
      domain: product
      description: Interactive scenario flows with API endpoints for scenario snapshots
        and storybook coverage
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T1.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T1.1.2
      title: Implement ingestion Prefect flow with checkpointing
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:data_quality
      - prose: critic:org_pm
      - prose: artifact:experiments/ingest/dq_report.json
      domain: product
      description: Map + chart overlays with export service (PPT/CSV)
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T1.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M1.2
    title: Weather harmonisation
    status: done
    tasks:
    - id: T1.2.1
      title: Blend historical + forecast weather, enforce timezone alignment
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:forecast_stitch
      - prose: doc:docs/weather/blending.md
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T1.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T1.2.2
      title: Add leakage guardrails to feature builder
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:leakage
      - prose: critic:tests
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T1.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  description: Stand up weather + marketing ingestion, harmonise geo/time, and validate
    data quality.
- id: E10
  title: "PHASE-6-COST \u2014 Usage-Based Optimisations"
  status: done
  domain: mcp
  milestones:
  - id: M10.1
    title: Usage telemetry & guardrails
    status: done
    tasks:
    - id: T10.1.1
      title: Cost telemetry and budget alerts
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Provider cost telemetry recorded in state/telemetry/operations.jsonl
      - prose: Budget thresholds configurable per environment
      - prose: Alert surfaced via state/context.md and orchestration logs
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T10.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
- id: E11
  title: Resource-Aware Intelligence & Personalisation
  status: done
  domain: product
  milestones:
  - id: M11.1
    title: Capability Detection
    status: done
    tasks:
    - id: T11.1.1
      title: Implement hardware probe & profile persistence
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:build
      - prose: doc:docs/ROADMAP.md
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T11.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T11.1.2
      title: Adaptive scheduling for heavy tasks
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:tests
      - prose: artifact:state/device_profiles.json
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T11.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M11.2
    title: Falcon Design System & Award-ready UX
    status: done
    tasks:
    - id: T11.2.1
      title: Design system elevation (motion, typography, theming)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:design_system
      - prose: doc:docs/WEB_DESIGN_SYSTEM.md
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T11.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T11.2.2
      title: Award-level experience audit & remediation
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:exec_review
      - prose: artifact:docs/UX_CRITIQUE.md
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T11.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T11.2.3
      title: Extend calm/aero theme tokens to Automations and Experiments surfaces
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:apps/web/styles/themes/calm.ts
      - prose: artifact:apps/web/styles/themes/aero.ts
      - prose: critic:design_system
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T11.2.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T11.2.4
      title: Refactor landing/marketing gradients into reusable tokens
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:apps/web/styles/tokens/gradients.md
      - prose: critic:design_system
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T11.2.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T11.2.5
      title: Centralize retry button styles in shared component once App Router lands
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:apps/web/components/buttons/RetryButton.tsx
      - prose: unknown
      - prose: critic:design_system
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T11.2.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T11.2.6
      title: Formalize shared panel mixin (border + shadow) to reduce overrides
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:apps/web/styles/mixins/panel.css
      - prose: critic:design_system
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T11.2.6
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  description: Auto-detect hardware, adapt workloads, and guarantee great performance
    on constrained machines.
- id: E12
  title: "Epic 12 \u2014 Weather Model Production Validation"
  status: blocked
  domain: product
  milestones:
  - id: M12.0
    title: Synthetic Multi-Tenant Dataset Generation
    status: blocked
    tasks:
    - id: T12.0.1
      title: Generate synthetic multi-tenant dataset with weather-sensitive products
      status: blocked
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:storage/seeds/synthetic/*.parquet
      - prose: artifact:state/analytics/synthetic_tenant_profiles.json
      - prose: metric:extreme_tenant_correlation >= 0.80
      - prose: metric:high_tenant_correlation >= 0.65
      - prose: metric:medium_tenant_correlation >= 0.35
      - prose: metric:none_tenant_correlation < 0.15
      - prose: metric:data_completeness = 1.0
      - prose: metric:no_missing_dates = true
      - prose: critic:data_quality
      - prose: critic:modeling_reality_v2
      domain: product
      description: Create 4 simulated tenants with Shopify products, Meta/Google ads
        spend, Klaviyo data, and weather-driven demand patterns
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T12.0.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T12.0.2
      title: Validate synthetic data quality and weather correlation
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:state/analytics/synthetic_data_validation.json
      - prose: artifact:docs/DATA_GENERATION.md
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:test_mape < 0.20
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:causal
      - prose: critic:data_quality
      domain: product
      description: Confirm data volume, coverage, completeness; measure weather elasticity
        for each tenant
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T12.0.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T12.0.3
      title: Document synthetic tenant characteristics
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:docs/SYNTHETIC_TENANTS.md
      - prose: artifact:state/analytics/tenant_weather_profiles.json
      - prose: metric:extreme_tenant_correlation >= 0.80
      - prose: metric:high_tenant_correlation >= 0.65
      - prose: metric:medium_tenant_correlation >= 0.35
      - prose: metric:none_tenant_correlation < 0.15
      - prose: metric:data_completeness = 1.0
      - prose: metric:no_missing_dates = true
      - prose: critic:data_quality
      - prose: critic:modeling_reality_v2
      domain: product
      description: Create data dictionary with tenant profiles, weather sensitivity,
        expected model behaviors
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T12.0.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M12.1
    title: Weather ingestion + feature QA
    status: pending
    tasks:
    - id: T12.1.1
      title: Run smoke-context and weather ingestion regression suite nightly
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:state/telemetry/weather_ingestion.json
      - prose: artifact:experiments/t12/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:data_quality
      - prose: critic:causal
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T12.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T12.1.2
      title: Validate feature store joins against historical weather baselines
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/weather/feature_backfill_report.md
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:test_mape < 0.20
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:causal
      - prose: critic:data_quality
      - prose: critic:forecast_stitch
      - prose: critic:tests
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T12.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M12.2
    title: Weather model capability sign-off
    status: pending
    tasks:
    - id: T12.2.1
      title: Backtest weather-aware model vs control across top tenants
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/weather/model_backtest_summary.md
      - prose: artifact:experiments/t12/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:causal
      - prose: critic:allocator
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T12.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T12.2.2
      title: Publish weather capability runbook and monitoring dashboards
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:apps/web/pages/ops/weather-capabilities.tsx
      - prose: artifact:experiments/t12/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:org_pm
      - prose: critic:causal
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T12.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M12.3
    title: Weather-Aware MMM Model Training
    status: pending
    tasks:
    - id: T12.3.1
      title: Train weather-aware MMM on validated 90-day tenant data
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/mcp/mmm_weather_model.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:test_mape < 0.20
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:causal
      - prose: critic:data_quality
      domain: product
      description: Train multi-channel MMM using validated 90-day data with weather
        features integrated
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T12.3.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T12.3.2
      title: Implement weather sensitivity elasticity estimation
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:docs/models/weather_elasticity_analysis.md
      - prose: artifact:experiments/t12/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:causal
      domain: product
      description: Quantify how demand elasticity varies by weather (temperature sensitivity,
        rain impact, seasonal patterns)
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T12.3.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T12.3.3
      title: Ship production MMM inference service with real-time weather scoring
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:apps/api/routes/models/weather_mixin.py
      - prose: artifact:apps/worker/models/mmm_weather_inference.py
      - prose: artifact:experiments/t12/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:tests
      - prose: critic:causal
      domain: product
      description: Deploy MMM with weather features as production inference service
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T12.3.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M12.Demo
    title: Executive Demo & Stakeholder Sign-Off
    status: pending
    tasks:
    - id: T12.Demo.1
      title: Build interactive demo UI showing weather impact on ROAS
      status: pending
      dependencies:
        depends_on:
        - T12.UX.1
        - T12.UX.2
        - T12.UX.3
        - T12.UX.4
      exit_criteria:
      - prose: artifact:apps/web/src/pages/demo-weather-analysis.tsx
      - prose: artifact:state/artifacts/screenshots/demo_ui_iteration/
      - prose: artifact:experiments/t12/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:design_system
      domain: product
      description: Create web UI that lets stakeholders toggle weather on/off and
        see impact on predicted revenue and ROAS for each synthetic tenant. Interactive
        proof that weather matters for demand forecasting. DESIGN EXCELLENCE REQUIRED
        - Follow M12.UXExcellence standards (Playwright iteration, world-class inspiration,
        surprise & delight, zero AI aesthetic). Use screenshot_session for visual
        QA. Validate with design_system critic.
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T12.Demo.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T12.Demo.2
      title: Record demo video and create stakeholder brief
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:docs/WEATHER_DEMO_BRIEF.md
      - prose: artifact:state/artifacts/stakeholder/weather_demo_script.md
      - prose: artifact:experiments/t12/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      domain: product
      description: Record 5-min demo video showing weather-aware model in action.
        Create 1-page brief for executives explaining business impact (revenue upside,
        forecast accuracy improvement, ROAS optimization potential).
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T12.Demo.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M12.PoC
    title: Proof of Concept & Model Testing
    status: pending
    tasks:
    - id: T12.PoC.1
      title: Train weather-aware model on synthetic tenant data
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/mcp/weather_poc_model.pkl
      - prose: artifact:experiments/mcp/weather_poc_metrics.json
      - prose: artifact:experiments/t12/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:data_quality
      domain: product
      description: Train a baseline weather-aware regression model on each synthetic
        tenant (high/extreme/medium/none sensitivity) to validate the weather correlation
        detection works across different product types and sensitivity profiles.
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T12.PoC.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T12.PoC.2
      title: Validate PoC model predictions on hold-out data
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/mcp/weather_poc_validation.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:test_mape < 0.20
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:causal
      - prose: critic:data_quality
      domain: product
      description: "Test PoC model on final 30 days of each synthetic tenant. Verify\
        \ that: (1) High/Extreme tenants show strong weather effects, (2) None tenant\
        \ shows no weather effect, (3) Model R\xB2 > 0.6 on validation set"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T12.PoC.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T12.PoC.3
      title: Create PoC demo results and proof brief
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:docs/WEATHER_PROOF_OF_CONCEPT.md
      - prose: artifact:experiments/mcp/poc_demo_charts.json
      - prose: artifact:experiments/t12/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      domain: product
      description: 'Create executive brief demonstrating that weather-aware modeling
        works: show before/after model performance, weather elasticity coefficients,
        and prediction examples. Use this to get stakeholder buy-in before full MMM
        training.'
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T12.PoC.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M12.UXExcellence
    title: Design Excellence Infrastructure (Lower Priority - After Data Gen)
    status: pending
    tasks:
    - id: T12.UX.1
      title: Setup Playwright screenshot workflow for design iteration
      status: pending
      dependencies:
        depends_on:
        - T12.0.1
      exit_criteria:
      - prose: artifact:state/screenshot_config.yaml
      - prose: artifact:docs/DESIGN_WORKFLOW.md
      - prose: artifact:experiments/t12/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:tests
      domain: product
      description: "Configure screenshot_session, screenshot_capture_multiple tools\
        \ for visual design iteration. Document workflow for Build\u2192Screenshot\u2192\
        Critique\u2192Refine loop. PRIORITY NOTE - Execute only after T12.0.1 (data\
        \ generation) completes."
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T12.UX.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T12.UX.2
      title: Create design research process and inspiration library
      status: pending
      dependencies:
        depends_on:
        - T12.0.1
      exit_criteria:
      - prose: artifact:docs/DESIGN_INSPIRATION.md
      - prose: artifact:state/design_references.json
      - prose: artifact:experiments/t12/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      domain: product
      description: "Document process for researching award-winning UIs (Awwwards,\
        \ FWA, Stripe, Linear, Observable). Create reference library of world-class\
        \ design patterns to inform WeatherVane UI decisions. Extract principles from\
        \ Dieter Rams, M\xFCller-Brockmann, Vignelli, Paul Rand."
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T12.UX.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T12.UX.3
      title: Establish surprise & delight checklist and validation criteria
      status: pending
      dependencies:
        depends_on:
        - T12.0.1
      exit_criteria:
      - prose: artifact:docs/SURPRISE_DELIGHT_CHECKLIST.md
      - prose: artifact:tools/wvo_mcp/src/critics/ux_delight_scorer.ts
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:test_mape < 0.20
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:causal
      - prose: critic:data_quality
      domain: product
      description: Create checklist for trivial delights (micro-interactions, loading
        states, empty states, witty copy) and non-trivial delights (anticipatory UX,
        intelligent recovery, time-saving magic). Validation - Would user screenshot
        and share? Would Don Norman/Kathy Sierra/Julie Zhuo approve?
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T12.UX.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T12.UX.4
      title: Configure design_system critic for zero-AI-aesthetic enforcement
      status: pending
      dependencies:
        depends_on:
        - T12.UX.1
        - T12.UX.2
        - T12.UX.3
      exit_criteria:
      - prose: artifact:tools/wvo_mcp/src/critics/design_system_enhanced.ts
      - prose: artifact:experiments/t12/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:design_system
      domain: product
      description: Enhance design_system critic to detect generic gradients, stock
        layouts, template components. Enforce heritage design principles. Integrate
        with screenshot workflow to compare against world-class references. Block
        merge if AI aesthetic detected.
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T12.UX.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  description: Prioritise end-to-end weather ingestion QA, model backtests, and operational
    readiness so WeatherVane shiproom can demo weather insights with confidence.
- id: E13
  title: "Epic 13 \u2014 Weather-Aware Modeling Reality"
  status: pending
  domain: product
  milestones:
  - id: M13.1
    title: Data Backbone Verified
    status: pending
    tasks:
    - id: T13.1.1
      title: Validate 90-day tenant data coverage across sales, spend, and weather
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/features/weather_join_validation.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:test_mape < 0.20
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:causal
      - prose: critic:data_quality
      - prose: critic:data_quality passes with weather join metrics captured
      domain: product
      description: Ensure the Shopify/ads/weather ingestion flows actually populate
        product_daily with geocoded spend and 90+ days of history so MMM inputs are
        real, not theoretical.
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T13.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T13.1.2
      title: Autopilot guardrail for ingestion + weather drift
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/t13/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:weather_coverage autop-run nightly with failure escalation to
          Atlas
      - prose: critic:data_quality
      - prose: critic:causal
      domain: product
      description: Bake data completeness checks into Autopilot so future weather-awareness
        regressions trigger automated investigations.
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T13.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T13.1.3
      title: Implement product taxonomy auto-classification with weather affinity
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/t13/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:data_quality
      - prose: critic:causal
      domain: product
      description: 'Auto-classify products from Shopify/Meta/Google using LLM (Claude/GPT-4)
        to tag products with weather affinity and category hierarchy. This enables
        product-level modeling (not just brand-level) and cold-start for new brands.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T13.1.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T13.1.4
      title: Data quality validation framework (verify data fitness for ML)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:test_mape < 0.20
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:causal
      - prose: critic:data_quality
      domain: product
      description: 'Implement data quality checks to verify data is ready for ML training.
        Prevents training models on insufficient/corrupted data.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T13.1.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M13.2
    title: MMM Upgrade & Backtests
    status: pending
    tasks:
    - id: T13.2.1
      title: Replace heuristic MMM with LightweightMMM adstock+saturation fit
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/t13/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:model_fit passes with synthetic recovery tests
      - prose: critic:causal
      domain: product
      description: Integrate the existing LightweightMMM wrapper so allocations use
        Bayesian adstock/saturation estimates instead of covariance heuristics.
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T13.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T13.2.2
      title: Build MMM backtesting + regression suite
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/t13/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:causal
      domain: product
      description: Establish out-of-sample evaluation so MMM recommendations are validated
        continuously.
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T13.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T13.2.3
      title: Replace heuristic allocator with constraint-aware optimizer
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/t13/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:causal
      domain: product
      description: "Replace heuristic allocation rules (\xB110% budget adjustments)\
        \ with proper constrained optimization. Use cvxpy or OR-Tools to maximize\
        \ ROAS subject to budget, inventory, and platform constraints.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T13.2.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M13.3
    title: Causal & Geography Alignment
    status: pending
    tasks:
    - id: T13.3.1
      title: Swap uplift propensity scoring with DID/synthetic control for weather
        shocks
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: metric:extreme_tenant_correlation >= 0.80
      - prose: metric:high_tenant_correlation >= 0.65
      - prose: metric:medium_tenant_correlation >= 0.35
      - prose: metric:none_tenant_correlation < 0.15
      - prose: metric:data_completeness = 1.0
      - prose: metric:no_missing_dates = true
      - prose: critic:data_quality
      - prose: critic:modeling_reality_v2
      domain: product
      description: Adopt causal estimators appropriate for non-manipulable treatments
        so weather impact claims are statistically defensible.
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T13.3.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T13.3.2
      title: Implement DMA-first geographic aggregation with hierarchical fallback
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/t13/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:causal
      domain: product
      description: Resolve the open question on geographic granularity by codifying
        DMA-first modeling with automatic fallback.
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T13.3.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M13.4
    title: Autopilot Meta-Critique Loop
    status: pending
    tasks:
    - id: T13.4.1
      title: Add modeling reality critic to Autopilot
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/t13/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      domain: product
      description: Teach Autopilot to generate the sort of gap analysis we just performed
        so future discrepancies surface automatically.
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T13.4.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T13.4.2
      title: Meta-evaluation playbook for modeling roadmap
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/t13/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      domain: product
      description: "Provide a repeatable process\u2014documentation, scheduling, and\
        \ telemetry\u2014for leadership to review modeling execution against strategy."
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T13.4.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M13.5
    title: Weather-Aware Allocation Model Deployment
    status: pending
    tasks:
    - id: T13.5.1
      title: Train weather-aware allocation model on top of MMM baseline
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/allocation/weather_aware_model.json
      - prose: artifact:experiments/t13/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:allocator
      - prose: critic:causal
      domain: product
      description: Build allocation optimization model that incorporates weather-driven
        demand elasticity from MMM training
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T13.5.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T13.5.2
      title: Implement weather-responsive budget allocation constraints
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/allocation/constraint_validation.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:tests
      - prose: critic:causal
      domain: product
      description: Add constraints that adjust budget allocation based on weather
        forecasts (e.g., reduce spend on low-demand weather days)
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T13.5.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T13.5.3
      title: Deploy weather-aware allocator to production
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:apps/api/routes/allocate.py updated with weather model
      - prose: artifact:apps/worker/allocation_service.py deployed
      - prose: artifact:experiments/t13/validation_report.json
      - prose: metric:out_of_sample_r2 > 0.50
      - prose: metric:weather_elasticity_sign_correct = true
      - prose: metric:beats_naive_baseline_mape > 1.10
      - prose: metric:beats_seasonal_baseline_mape > 1.10
      - prose: metric:no_overfitting_detected = true
      - prose: critic:modeling_reality_v2
      - prose: critic:academic_rigor
      - prose: critic:tests
      - prose: critic:allocator
      domain: product
      description: Ship weather-aware allocation as the primary recommendation engine
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T13.5.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  description: Close the execution gap between sophisticated modeling plans and the
    current codebase, while embedding Autopilot self-critique so these regressions
    cannot hide in the future.
- id: E13.1
  title: "Research and design for Epic 13 \u2014 Weather-Aware Modeling Reality"
  status: pending
  domain: product
  milestones: []
  description: "Research phase: Understand requirements and design approach for Epic\
    \ 13 \u2014 Weather-Aware Modeling Reality"
- id: E13.2
  title: "Implement Epic 13 \u2014 Weather-Aware Modeling Reality"
  status: pending
  domain: product
  milestones: []
  description: "Implementation phase: Execute the plan for Epic 13 \u2014 Weather-Aware\
    \ Modeling Reality"
- id: E13.3
  title: "Validate and test Epic 13 \u2014 Weather-Aware Modeling Reality"
  status: pending
  domain: product
  milestones: []
  description: "Validation phase: Test and verify Epic 13 \u2014 Weather-Aware Modeling\
    \ Reality"
- id: E2
  title: "Epic 2 \u2014 Features & Modeling Baseline"
  status: done
  domain: product
  milestones:
  - id: M2.1
    title: Feature pipeline
    status: done
    tasks:
    - id: T2.1.1
      title: Build lag/rolling feature generators with deterministic seeds
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:build
      - prose: critic:tests
      - prose: critic:data_quality
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T2.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M2.2
    title: Baseline modeling
    status: done
    tasks:
    - id: T2.2.1
      title: Train weather-aware GAM baseline and document methodology
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:causal
      - prose: critic:academic_rigor
      - prose: doc:docs/models/baseline.md
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T2.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  description: Ship lagged features, baseline models, and evaluation harness.
- id: E3
  title: "Epic 3 \u2014 Allocation & UX"
  status: done
  domain: product
  milestones:
  - id: M3.1
    title: Allocator guardrails
    status: done
    tasks:
    - id: T3.1.1
      title: Implement budget allocator stress tests and regret bounds
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:allocator
      - prose: critic:cost_perf
      - prose: artifact:experiments/policy/regret.json
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T3.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M3.2
    title: Dashboard + UX review
    status: done
    tasks:
    - id: T3.2.1
      title: Run design system critic and ensure accessibility coverage
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:design_system
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T3.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T3.2.2
      title: Elevate dashboard storytelling & UX
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:design_system
      - prose: critic:exec_review
      - prose: doc:docs/UX_CRITIQUE.md
      - prose: artifact:docs/product/UX_CRITIQUE.md
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T3.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M3.3
    title: Autonomous Orchestration Blueprints
    status: done
    tasks:
    - id: T3.3.1
      title: Draft multi-agent charter & delegation mesh (AutoGen/Swarm patterns)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:docs/orchestration/multi_agent_charter.md
      - prose: critic:manager_self_check
      - prose: critic:org_pm
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T3.3.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T3.3.2
      title: Implement hierarchical consensus & escalation engine
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:integration_fury
      - prose: critic:manager_self_check
      - prose: doc:docs/orchestration/consensus_engine.md
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T3.3.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T3.3.3
      title: Build closed-loop simulation harness for autonomous teams
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:experiments/orchestration/simulation_report.md
      - prose: critic:tests
      - prose: critic:health_check
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T3.3.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T3.3.4
      title: Instrument dynamic staffing telemetry & learning pipeline
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:prompt_budget
      - prose: critic:exec_review
      - prose: artifact:state/analytics/orchestration_metrics.json
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T3.3.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M3.4
    title: Experience Implementation
    status: done
    tasks:
    - id: T3.4.1
      title: Implement Plan overview page with weather-driven insights
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:apps/web/pages/plan.tsx
      - prose: unknown
      - prose: critic:design_system
      - prose: unknown
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T3.4.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T3.4.2
      title: Build WeatherOps dashboard with allocator + weather KPIs
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:apps/web/pages/dashboard.tsx
      - prose: unknown
      - prose: critic:design_system
      - prose: unknown
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T3.4.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T3.4.3
      title: Ship Experiments hub UI for uplift & incrementality reviews
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:apps/web/pages/experiments.tsx
      - prose: unknown
      - prose: critic:design_system
      - prose: unknown
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T3.4.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T3.4.4
      title: Deliver storytelling Reports view with weather + spend narratives
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: artifact:apps/web/pages/reports.tsx
      - prose: unknown
      - prose: critic:design_system
      - prose: unknown
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T3.4.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T3.4.5
      title: Conduct design_system + UX acceptance review across implemented pages
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:design_system
      - prose: doc:docs/product/acceptance_report.md
      - prose: artifact:state/critics/designsystem.json
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T3.4.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T3.4.6
      title: Rewrite WeatherOps dashboard around plain-language decisions
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: unknown
      - prose: unknown
      - prose: critic:design_system
      - prose: critic:exec_review
      - prose: unknown
      domain: product
      description: "Requirements:\n  - Show operators exactly what WeatherVane changed\
        \ or recommends changing, with clear \u201Cwhat/why/next\u201D messaging.\n\
        \  - Remove jargon (\u201Cguardrail\u201D, \u201Ctriage\u201D) in favour of\
        \ user-facing language (e.g., \u201COverspend alert\u201D, \u201CWeather action\u201D\
        ).\n  - Keep first view scannable: one hero recommendation, secondary cards,\
        \ optional detail drill-down.\nStandards:\n  - Copy: conversational, action-oriented,\
        \ no internal terminology.\n  - UX: minimalist layout, responsive to desktop/tablet,\
        \ accessible (WCAG AA).\n  - Engineering: Playwright smoke must pass; analytics\
        \ instrumentation preserved; Vitest coverage for helpers.\nImplementation\
        \ Plan:\n  - Draft/record design brief in docs/UX_CRITIQUE.md.\n  - Refactor\
        \ hero + summary components around new copy and layout.\n  - Update analytics\
        \ helpers/tests, run Vitest + Playwright.\n  - Capture iteration in state/context.md\
        \ with screenshots and critic notes.\nDeliverables:\n  - Updated React/CSS\
        \ modules under apps/web/src/pages/dashboard.tsx and styles.\n  - Revised\
        \ helper libraries/tests (apps/web/src/lib/**, tests/web/**).\n  - Playwright\
        \ report + screenshots stored under state/artifacts/ui/weatherops.\nIntegration\
        \ Points:\n  - API suggestion telemetry (shared/services/dashboard_analytics_ingestion.py)\
        \ ensuring copy aligns with payload fields.\n  - Analytics events (`trackDashboardEvent`)\
        \ and downstream dashboards; coordinate with data/ML owners if field names\
        \ change.\n  - Worker-generated suggestion summaries (apps/worker/flows/poc_pipeline.py)\
        \ to maintain consistency across channels.\nEvidence:\n  - Playwright run\
        \ ID + html report.\n  - design_system + exec_review critic outputs (once\
        \ available).\n  - Context entry summarising decisions, open questions, next\
        \ iteration."
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T3.4.6
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T3.4.7
      title: Reimagine Automations change log as a trust-first narrative
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: unknown
      - prose: unknown
      - prose: critic:design_system
      - prose: critic:exec_review
      - prose: unknown
      domain: product
      description: "Requirements:\n  - Explain every autonomous change in plain language\
        \ (what changed, when, why, impact).\n  - Provide explicit approval/rollback\
        \ affordances for humans and highlight pending reviews.\n  - Surface evidence\
        \ (metrics, weather context, spend forecasts) inline or a click away.\nStandards:\n\
        \  - Copy: transparent, confidence-building, avoids \u201Caudit/guardrail\u201D\
        \ jargon.\n  - UX: timeline or table must prioritise newest changes, support\
        \ filtering; accessible controls for approvals.\n  - Engineering: tests updated\
        \ (Vitest, Playwright), telemetry preserved, change log data schema documented.\n\
        \  - ML context (if applicable): explain model confidence/reason codes clearly.\n\
        Implementation Plan:\n  - Extend docs/UX_CRITIQUE.md with Automations brief,\
        \ list user questions + acceptance metrics.\n  - Redesign components/layout\
        \ in apps/web/src/pages/automations.tsx; integrate evidence panels.\n  - Update\
        \ helpers/tests, run Vitest + Playwright, capture critics.\n  - Log iterations\
        \ in state/context.md with before/after screenshots and open questions.\n\
        Deliverables:\n  - Updated Automations page/components/styles.\n  - Supporting\
        \ helper modules/tests (automationInsights, validation, etc.).\n  - Evidence\
        \ artifacts (Playwright report, screenshots, context notes).\nIntegration\
        \ Points:\n  - Automation audit APIs/events (apps/api/services/dashboard_service.py,\
        \ shared schemas) so reason codes remain synchronized.\n  - Worker automation\
        \ execution logs (`apps/worker/flows/**`) and telemetry exports consumed by\
        \ directors/Dana.\n  - Notification channels or forthcoming approval workflows\
        \ (e.g., Slack/email) to ensure new statuses map correctly.\nEvidence:\n \
        \ - Playwright run + report stored under state/artifacts/ui/automations.\n\
        \  - design_system + exec_review critic confirmation.\n  - Context log summarising\
        \ decisions, trade-offs, next steps."
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T3.4.7
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  description: Allocator robustness checks, dashboards, and UI polish.
- id: E4
  title: "Epic 4 \u2014 Operational Excellence"
  status: done
  domain: product
  milestones:
  - id: M4.1
    title: Optimization sprint
    status: done
    tasks:
    - id: T4.1.10
      title: Cross-market saturation optimization (fairness-aware)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:allocator
      - prose: artifact:experiments/allocator/saturation_report.json
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T4.1.10
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T4.1.3
      title: Causal uplift modeling & incremental lift validation
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:causal
      - prose: artifact:experiments/causal/uplift_report.json
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T4.1.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T4.1.4
      title: Multi-horizon ensemble forecasting
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:forecast_stitch
      - prose: artifact:experiments/forecast/ensemble_metrics.json
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T4.1.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T4.1.5
      title: Non-linear allocation optimizer with constraints (ROAS, spend caps)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:allocator
      - prose: unknown
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T4.1.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T4.1.6
      title: High-frequency spend response modeling (intraday adjustments)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:allocator
      - prose: artifact:experiments/allocator/hf_response.json
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T4.1.6
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T4.1.7
      title: Marketing mix budget solver (multi-channel, weather-aware)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:allocator
      - prose: unknown
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T4.1.7
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T4.1.8
      title: Reinforcement-learning shadow mode (safe exploration)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:allocator
      - prose: artifact:experiments/rl/shadow_mode.json
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T4.1.8
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T4.1.9
      title: Creative-level response modeling with brand safety guardrails
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:design_system
      - prose: artifact:experiments/creative/response_scores.json
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T4.1.9
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  description: Maintain velocity while hardening performance and delivery processes.
- id: E5
  title: Ad Platform Execution & Automation
  status: done
  domain: product
  milestones:
  - id: M5.1
    title: Meta Ads Command Pipeline
    status: done
    tasks:
    - id: T5.1.1
      title: Implement Meta Marketing API client (creative + campaign management)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:allocator
      - prose: unknown
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T5.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T5.1.2
      title: Meta sandbox and dry-run executor with credential vaulting
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:security
      - prose: artifact:experiments/meta/sandbox_run.json
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T5.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M5.2
    title: Google Ads Execution & Budget Sync
    status: done
    tasks:
    - id: T5.2.1
      title: Google Ads API integration (campaign create/update, shared budgets)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:allocator
      - prose: unknown
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T5.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T5.2.2
      title: Budget reconciliation & spend guardrails across platforms
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:allocator
      - prose: artifact:experiments/allocator/spend_guardrails.json
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T5.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M5.3
    title: QA, Rollback & Safety Harness
    status: done
    tasks:
    - id: T5.3.1
      title: Dry-run & diff visualizer for ad pushes (pre-flight checks)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:tests
      - prose: artifact:state/ad_push_diffs.json
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T5.3.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T5.3.2
      title: Automated rollback + alerting when performance/regression detected
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:manager_self_check
      - prose: artifact:experiments/allocator/rollback_sim.json
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T5.3.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  description: Enable WeatherVane to programmatically create, update, monitor, and
    rollback ads across major platforms.
- id: E6
  title: MCP Orchestrator Production Readiness
  status: done
  domain: mcp
  milestones:
  - id: M6.1
    title: Core Infrastructure Validation
    status: done
    tasks:
    - id: T6.1.1
      title: MCP server integration tests (all 25 tools across both providers)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:tests
      - prose: artifact:tests/test_mcp_tools.py
      - prose: 'Guardrail: integration suite enforces blue/green safety invariants
          (no unhandled throws, DRY_RUN parity)'
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T6.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T6.1.2
      title: Provider failover testing (token limit simulation & automatic switching)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:manager_self_check
      - prose: artifact:experiments/mcp/failover_test.json
      - prose: 'Guardrail: circuit-breaker rollback and DISABLE_NEW kill switch verified
          under simulated failures'
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T6.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T6.1.3
      title: State persistence testing (checkpoint recovery across sessions)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:tests
      - prose: artifact:tests/test_state_persistence.py
      - prose: 'Guardrail: recovery flow preserves upgrade locks and safety state
          without manual intervention'
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T6.1.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T6.1.4
      title: Quality framework validation (10 dimensions operational)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:manager_self_check
      - prose: artifact:state/quality/assessment_log.json
      - prose: 'Guardrail: quality checks confirm run-safety metrics from blue/green
          playbook remain green'
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T6.1.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M6.2
    title: Security & Reliability Hardening
    status: done
    tasks:
    - id: T6.2.1
      title: Credentials security audit (auth.json, API keys, token rotation)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:security
      - prose: doc:docs/SECURITY_AUDIT.md
      - prose: 'Guardrail: audit verifies secrets handling inside blue/green upgrade
          flow and DRY_RUN constraints'
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T6.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T6.2.2
      title: Error recovery testing (graceful degradation, retry logic)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:tests
      - prose: artifact:experiments/mcp/error_recovery.json
      - prose: 'Guardrail: automated rollback path exercised with observation window
          + DISABLE_NEW reset'
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T6.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T6.2.3
      title: Schema validation enforcement (all data contracts validated)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:data_quality
      - prose: artifact:shared/contracts/*.schema.json
      - prose: 'Guardrail: dual-write / expand-cutover-contract workflow logged for
          100% safe migrations'
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T6.2.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T6.2.4
      title: API rate limiting & exponential backoff (Open-Meteo, Shopify, Ads APIs)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:allocator
      - prose: unknown
      - prose: 'Guardrail: rate-limit handling respects worker timeouts and prevents
          cascading failures during upgrades'
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T6.2.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M6.3
    title: Observability & Performance
    status: done
    tasks:
    - id: T6.3.1
      title: Performance benchmarking (MCP overhead, checkpoint size, token usage)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:cost_perf
      - prose: artifact:experiments/mcp/performance_benchmarks.json
      - prose: 'Guardrail: benchmarks include worker swap scenarios and confirm resource
          limits (timeouts, RSS) hold'
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T6.3.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T6.3.2
      title: Enhanced observability export (structured logs, metrics dashboards)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:manager_self_check
      - prose: artifact:state/telemetry/metrics_summary.json
      - prose: "Guardrail: telemetry captures Step 0\u201315 safety signals with alerting\
          \ on breaches"
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T6.3.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T6.3.3
      title: Autopilot loop end-to-end testing (full autonomous cycle validation)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:manager_self_check
      - prose: artifact:experiments/mcp/autopilot_e2e.json
      - prose: 'Guardrail: autonomous loop validates automatic promotion + rollback
          without manual resets'
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T6.3.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M6.4
    title: Zero-downtime self-upgrade
    status: pending
    tasks:
    - id: T6.4.0
      title: Upgrade invariants & preflight guardrails
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: state/upgrade.lock created before work and removed on exit
      - prose: "Preflight script validates git status, Node/npm versions, disk \u2265\
          500MB, sandbox availability"
      - prose: Four-step gate recorded in logs; any failure returns {error:"upgrade_aborted"}
      domain: mcp
      description: "Define upgrade preflight: clean git, version sanity, \u2265500MB\
        \ disk, SQLite lock probe, and single-flight upgrade.lock. Gate promotion\
        \ through build \u2192 unit \u2192 selfchecks \u2192 canary, aborting with\
        \ {error:\"upgrade_aborted\"} on any failure."
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T6.4.0
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T6.4.1
      title: Live feature flag store with kill switch
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: settings table created with defaults + DISABLE_NEW
      - prose: LiveFlags poller refreshes in-memory cache during runtime
      - prose: Integration test flips PROMPT_MODE without restart
      domain: mcp
      description: "Replace environment toggles with a SQLite-backed `settings` table,\
        \ seed\ndefaults, and hot-refresh cached flags (\u2264500 ms poll). Include\
        \ a\n`DISABLE_NEW` global kill switch that forces legacy behaviour instantly.\n"
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T6.4.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T6.4.2
      title: Blue/green worker manager & front-end proxy
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: WorkerManager exposes startActive/startCanary/switchToCanary
      - prose: Front-end tool handlers call workers.getActive().call(...)
      - prose: RPC protocol enforces ready handshake, 30s timeouts, and structured
          {ok,error} results
      - prose: Test demonstrates zero-downtime swap between worker binaries
      domain: mcp
      description: 'Keep the MCP front-end process stable while managing active and
        canary

        worker children over IPC. Ensure requests route through a proxy that can

        atomically switch to the validated canary without disconnecting clients.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T6.4.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T6.4.3
      title: Worker entrypoint with DRY_RUN safeguards
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Route function covers health/plan/dispatch/runTool/verify/report.mo
      - prose: SQLite opened via file:state/state.db?mode=ro when DRY_RUN=1
      - prose: applyPatch/mutate operations rejected while DRY_RUN=1
      - prose: tests/test_worker_dry_run.py captures read-only guarantees
      domain: mcp
      description: 'Implement a dedicated worker entry that routes RPCs, enforces
        DRY_RUN=1

        by opening the state DB read-only, refuses mutating calls, and confirms

        legacy behaviour when DRY_RUN=0.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T6.4.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T6.4.4
      title: Canary upgrade harness & shadow validation
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: scripts/mcp_safe_upgrade.sh orchestrates worktree build + tests
      - prose: Shadow checks compare active vs canary outputs in logs
      - prose: "Promotion flow documents gate order and staged routing (DRY \u2192\
          \ live) with metrics snapshots"
      - prose: experiments/mcp/upgrade/<ts>/report.json recorded for each run
      domain: mcp
      description: 'Automate the upgrade flow: create a separate git worktree, build/test
        new

        code, spawn a DRY_RUN canary, run shadow health/plan/report checks, then

        promote only if outputs match expectations.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T6.4.4
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T6.4.5
      title: Feature flag gating for compact prompts & sandbox pool
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: PROMPT_MODE, SANDBOX_MODE, SCHEDULER_MODE, SELECTIVE_TESTS, DANGER_GATES,
          MO_ENGINE read from LiveFlags
      - prose: Regression fixtures cover legacy vs new mode per feature
      - prose: docs/MCP_ORCHESTRATOR.md updated with flag toggle order
      domain: mcp
      description: 'Gate compact prompt headers, sandbox pooling, scheduler WSJF mode,

        selective tests, danger gates, and MO engine behind live flags so they

        only activate after successful canary validation.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T6.4.5
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T6.4.6
      title: Runtime tool registration & admin flag controls
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Tool handlers return 'disabled' until corresponding flag enabled
      - prose: settings.update, upgrade.applyPatch, route.switch commands exposed
          with structured errors
      - prose: Operator guide added under docs/MCP_AUTOMATION.md#live-flags
      domain: mcp
      description: 'Ensure tool surfaces remain stable while routing to v1/v2 handlers
        based

        on flags. Provide an MCP admin tool or CLI to update settings atomically

        without restarts.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T6.4.6
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T6.4.7
      title: Automatic rollback monitors & kill-switch reset
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Heartbeat every 2s with 3-strike circuit breaker routes back to standby
      - prose: Error budget (5%/2min) and SLO monitors trigger automatic rollback
      - prose: DISABLE_NEW flag automatically flipped during rollback
      - prose: docs/MCP_ORCHESTRATOR.md includes rollback playbook
      domain: mcp
      description: 'Add health monitoring that reverts to the previous worker and
        resets

        flags when error rates spike post-promotion. Document on-call rollback

        steps and ensure DISABLE_NEW restores legacy behaviour.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T6.4.7
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T6.4.8
      title: Observability & resource budgets during upgrade
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Span/log attributes include method, lane, ok/error, duration, task.id
      - prose: runTool/plan timeouts (30s/120s) & lane concurrency limits enforced
      - prose: RSS watchdog throttles batch lane when >1.5x baseline
      domain: mcp
      description: 'Emit OTel spans (or structured JSON logs) for every worker call
        with

        timing, lane, task, and outcome metadata. Enforce concurrency, timeout,

        and RSS guards to prevent runaway resource usage.

        '
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T6.4.8
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  description: Validate and harden the dual-provider MCP orchestrator for autonomous
    operation while preserving 100% run safety. All milestones under this epic must
    enforce the blue/green upgrade guardrails defined in docs/MCP_ORCHESTRATOR.md#1113-tight-integration-playbook-steps-0-15.
- id: E7
  title: Data Pipeline Hardening
  status: done
  domain: product
  milestones:
  - id: M7.1
    title: Geocoding & Weather Integration
    status: done
    tasks:
    - id: T7.1.1
      title: Complete geocoding integration (city->lat/lon, cache strategy)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:data_quality
      - prose: unknown
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T7.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T7.1.2
      title: Weather feature join to model matrix (prevent future leakage)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:leakage
      - prose: artifact:experiments/features/weather_join_validation.json
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T7.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T7.1.3
      title: Data contract schema validation (Shopify, weather, ads)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:data_quality
      - prose: unknown
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T7.1.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M7.2
    title: Pipeline Robustness
    status: done
    tasks:
    - id: T7.2.1
      title: Incremental ingestion with deduplication & checkpointing
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:data_quality
      - prose: unknown
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T7.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T7.2.2
      title: Data quality monitoring & alerting (anomaly detection)
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: critic:data_quality
      - prose: artifact:state/dq_monitoring.json
      domain: product
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T7.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  description: Complete geocoding integration, weather feature joins, and data quality
    validation.
- id: E8
  title: "PHASE-4-POLISH \u2014 MCP Production Hardening"
  status: done
  domain: mcp
  milestones:
  - id: M8.1
    title: MCP Compliance & Security
    status: done
    tasks:
    - id: T8.1.1
      title: 'Lock MCP schemas to Zod shapes (SAFE: guardrail)'
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: utils/schema.ts returns schema.shape with guardrail comment
      - prose: MCP entrypoints register raw shapes only
      - prose: Autopilot documentation updated to reflect guardrail
      - prose: critic:build passes
      - prose: 'Guardrail: validation confirms schema handling does not weaken blue/green
          safety gates'
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T8.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T8.1.2
      title: 'Implement command allow-list in guardrails (SAFE: additive security)'
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: ALLOWED_COMMANDS constant defined
      - prose: isCommandAllowed() enforced before execution
      - prose: Deny-list kept as secondary check
      - prose: critic:tests passes with new test_command_allowlist.py
      - prose: critic:manager_self_check passes
      - prose: 'Guardrail: allow-list integration verified against blue/green upgrade
          scenarios'
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T8.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T8.1.3
      title: 'Thread correlation IDs through state transitions (SAFE: observability
        only)'
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: All tool handlers generate correlationId
      - prose: All state transitions include correlationId
      - prose: Events in SQLite include correlation_id column populated
      - prose: critic:manager_self_check passes
      - prose: End-to-end trace visible in state/orchestrator.db
      - prose: "Guardrail: correlation IDs trace compliance with Step 0\u201315 safety\
          \ checks"
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T8.1.3
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M8.2
    title: Context & Performance Optimization
    status: done
    tasks:
    - id: T8.2.1
      title: 'Implement compact evidence-pack prompt mode (SAFE: new function, backward
        compatible)'
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: formatForPromptCompact() returns JSON evidence pack
      - prose: unknown
      - prose: All coordinator calls use compact mode
      - prose: critic:build passes
      - prose: critic:manager_self_check passes
      - prose: unknown
      - prose: 'Guardrail: compact mode flip integrated with Step 15 staged flag process'
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T8.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T8.2.2
      title: "Finalize Claude\u2194Codex coordinator failover (SAFE: expose existing\
        \ functionality)"
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: orchestrator_status tool shows coordinator type and availability
      - prose: Telemetry includes coordinator field in execution logs
      - prose: Documentation updated in IMPLEMENTATION_STATUS.md
      - prose: critic:manager_self_check passes
      - prose: Failover behavior visible and logged
      - prose: 'Guardrail: failover reporting feeds SLO/error budget monitors for
          auto rollback'
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T8.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  description: "Critical production readiness tasks for MCP orchestrator. Complete\
    \ before WeatherVane v1 launch while maintaining the Step 0\u201315 run-safety\
    \ guardrails (docs/MCP_ORCHESTRATOR.md#1113-tight-integration-playbook-steps-0-15)."
- id: E9
  title: "PHASE-5-OPTIMIZATION \u2014 Performance & Observability"
  status: pending
  domain: mcp
  milestones:
  - id: M9.1
    title: Cost Optimization & Caching
    status: done
    tasks:
    - id: T9.1.1
      title: 'Stable prompt headers with provider caching (SAFE: additive optimization)'
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: standardPromptHeader() returns deterministic header
      - prose: All prompts include standard header
      - prose: Header enables provider caching (verified with API logs)
      - prose: critic:cost_perf shows token cache hit rate
      - prose: critic:manager_self_check passes
      - prose: "Guardrail: caching rollout assessed via Step 0\u201315 safety checks\
          \ before staying live"
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T9.1.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T9.1.2
      title: 'Batch queue for non-urgent prompts (SAFE: new queueing system)'
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Priority queue with 3 lanes operational
      - prose: Semaphore limits enforced per lane
      - prose: Interactive tasks always get priority
      - prose: critic:tests passes
      - prose: critic:manager_self_check passes
      - prose: 'Guardrail: queue respects worker concurrency caps from blue/green
          playbook'
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T9.1.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M9.2
    title: Reliability & Quality Improvements
    status: done
    tasks:
    - id: T9.2.1
      title: 'Strict output DSL validation (SAFE: validation layer only)'
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: validateDiff() rejects non-diff outputs
      - prose: validateJSON() rejects invalid JSON
      - prose: Retry rate reduction measured
      - prose: critic:tests passes
      - prose: 'Guardrail: validation enforced in canary shadow runs before live promotion'
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T9.2.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T9.2.2
      title: 'Idempotency keys for mutating tools (SAFE: caching layer)'
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Idempotency cache operational
      - prose: Duplicate operations return cached results
      - prose: 1-hour TTL enforced
      - prose: critic:tests passes
      - prose: 'Guardrail: cache respects DRY_RUN mode and avoids side effects during
          canary runs'
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T9.2.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M9.3
    title: Production Observability
    status: pending
    tasks:
    - id: T9.3.1
      title: 'OpenTelemetry spans for all operations (SAFE: tracing wrapper)'
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: All tool handlers instrumented
      - prose: Spans exported to tracing backend
      - prose: End-to-end traces visible
      - prose: Performance insights available
      - prose: critic:manager_self_check passes
      - prose: "Guardrail: telemetry alerts on Step 0\u201315 safety breaches"
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T9.3.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T9.3.2
      title: 'Sandbox pooling for test execution (SAFE: new executor)'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Sandbox pool with 3 pre-warmed containers
      - prose: Test execution uses pooled sandboxes
      - prose: 10x speedup measured
      - prose: Fallback to non-pooled works
      - prose: critic:tests passes
      - prose: 'Guardrail: pool enforces DRY_RUN read-only mode during canary validation'
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T9.3.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M9.4
    title: Advanced Context & Search
    status: pending
    tasks:
    - id: T9.4.1
      title: 'SQLite FTS5 index for code search (SAFE: new index)'
      status: done
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: code_fts virtual table created
      - prose: Index populated on repo sync
      - prose: Search performance <50ms
      - prose: critic:tests passes
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T9.4.1
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: T9.4.2
      title: 'LSP proxy tools for symbol-aware context (SAFE: new tools)'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: tsserver and pyright proxies running
      - prose: lsp.definition and lsp.references tools work
      - prose: Context assembler uses LSP for code slices
      - prose: Context relevance measured and improved
      - prose: critic:tests passes
      - prose: "Guardrail: LSP tools routed through worker proxy with Step 0\u2013\
          15 safety enforcement"
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      evidence_path: state/evidence/T9.4.2
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  description: Post-v1 performance improvements and production observability. High
    ROI optimizations that still honour the blue/green guardrail contract.
- id: E9.1
  title: "Research and design for PHASE-5-OPTIMIZATION \u2014 Performance & Observability"
  status: pending
  domain: product
  milestones: []
  description: "Research phase: Understand requirements and design approach for PHASE-5-OPTIMIZATION\
    \ \u2014 Performance & Observability"
- id: E9.2
  title: "Implement PHASE-5-OPTIMIZATION \u2014 Performance & Observability"
  status: pending
  domain: product
  milestones: []
  description: "Implementation phase: Execute the plan for PHASE-5-OPTIMIZATION \u2014\
    \ Performance & Observability"
- id: E9.3
  title: "Validate and test PHASE-5-OPTIMIZATION \u2014 Performance & Observability"
  status: pending
  domain: product
  milestones: []
  description: "Validation phase: Test and verify PHASE-5-OPTIMIZATION \u2014 Performance\
    \ & Observability"
- id: E-META-COMPLEXITY
  title: Task Complexity Governance
  status: proposed
  domain: governance
  description: "Establish a repeatable framework that maps task complexity, difficulty,\
    \ and blast radius to the level of Autopilot attention required (evidence depth,\
    \ reviewer lanes, verification gates). Ensures high-risk work cannot proceed with\
    \ baseline STRATEGIZE\u2192MONITOR evidence.\n"
  milestones:
  - id: M-META-COMPLEXITY-FOUNDATION
    title: Complexity rubric and enforcement plan
    status: proposed
    priority: high
    tasks:
    - id: META-COMPLEXITY-01
      title: Define complexity tiers and gating requirements
      status: pending
      priority: high
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Complexity rubric documented with tier definitions, triggers, and escalation
          rules
      - prose: Roadmap metadata (complexity_score, effort_hours, required_tools) mapped
          to rubric tiers
      - test: node tools/wvo_mcp/scripts/check_risk_oracle_coverage.ts
        expect: exit 0
      domain: governance
      description: 'Analyze historical incidents and current roadmap metadata to derive
        tier thresholds. Document required artifacts, reviewers, and verification
        suites per tier and outline automation hooks for WorkProcessEnforcer and roadmap
        validation.

        '
      complexity_score: 7
      effort_hours: 6
      required_tools:
      - docs
      - node
      - analysis
      evidence_path: state/evidence/META-COMPLEXITY-01
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
- id: E-QUALITY-FOLLOWUP
  title: Quality Assurance System Follow-Up (WORK-PROCESS-FAILURES deferred work)
  status: pending
  domain: product
  milestones:
  - id: M-QUALITY-INTEGRATION
    title: Critical Integrations
    status: pending
    tasks:
    - id: FIX-INTEGRATION-WorkProcessEnforcer
      title: Integrate quality checks into WorkProcessEnforcer (AC3, AC11)
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: work_process_quality_integration.ts wired into work_process_enforcer.ts
      - prose: Pre-flight checks run at IMPLEMENT phase start
      - prose: Quality gates checked before VERIFY phase
      - prose: Reasoning validation before marking task complete
      - prose: Fallback strategy implemented (timeout handling)
      - prose: Feature flags for gradual rollout
      - prose: End-to-end integration tests passing
      - prose: Autopilot success rate maintained >90%
      domain: mcp
      complexity_score: 8
      effort_hours: 3
      required_tools: []
      auto_created: true
      source_issue:
        type: quality
        severity: high
        gap: WorkProcessEnforcer integration incomplete (deferred from WORK-PROCESS-FAILURES)
      evidence_path: state/evidence/FIX-INTEGRATION-WorkProcessEnforcer
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: FIX-CI-Pipeline-Integration
      title: CI pipeline integration for quality checks (AC3, Task 6.4)
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: GitHub Actions workflow runs checks on every PR
      - prose: Quality gates block merge if critical failures
      - prose: PR comments with violation details
      - prose: CI metrics tracked (pass rate, execution time)
      - prose: Performance <10min total pipeline time
      domain: mcp
      complexity_score: 6
      effort_hours: 2
      required_tools: []
      auto_created: true
      source_issue:
        type: quality
        severity: high
        gap: CI pipeline integration deferred (Task 6.4)
      evidence_path: state/evidence/FIX-CI-Pipeline-Integration
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M-QUALITY-TESTING
    title: Comprehensive Testing
    status: pending
    tasks:
    - id: FIX-TEST-BATS-Suite
      title: BATS test suite for bash scripts (Task X.1)
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: BATS tests for all core check scripts
      - prose: Unit tests for bash functions in lib/
      - prose: Regression tests for known issues (bash bugs, security)
      - prose: Test coverage >80%
      - prose: CI runs BATS tests on every commit
      domain: mcp
      complexity_score: 5
      effort_hours: 3
      required_tools: []
      auto_created: true
      source_issue:
        type: quality
        severity: medium
        gap: No automated test suite for bash scripts
      evidence_path: state/evidence/FIX-TEST-BATS-Suite
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: FIX-TEST-TypeScript-Suite
      title: Unit tests for TypeScript quality tools (Task X.1)
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Unit tests for failure_parser.ts
      - prose: Unit tests for task_creator.ts
      - prose: Unit tests for detection_metrics.ts
      - prose: Unit tests for dashboard_generator.ts
      - prose: Test coverage >80%
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      auto_created: true
      source_issue:
        type: quality
        severity: medium
        gap: No unit tests for TypeScript files
      evidence_path: state/evidence/FIX-TEST-TypeScript-Suite
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M-QUALITY-LEARNING
    title: Learning System (Phase 7)
    status: pending
    tasks:
    - id: FIX-LEARNING-Capture-Script
      title: Learning capture script (Task 7.1)
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: scripts/capture_learning.sh exists and is executable
      - prose: 'Prompts for: what went wrong, root cause, prevention strategy'
      - prose: Creates learning entry in docs/learnings/
      - prose: Updates relevant documentation (CLAUDE.md, work process docs)
      - prose: Can suggest automated check to add
      domain: mcp
      complexity_score: 4
      effort_hours: 1
      required_tools: []
      auto_created: true
      source_issue:
        type: quality
        severity: medium
        gap: Learning capture not yet automated (Phase 7)
      evidence_path: state/evidence/FIX-LEARNING-Capture-Script
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: FIX-LEARNING-Review-Script
      title: Learning review script (Task 7.2)
      status: pending
      dependencies:
        depends_on:
        - FIX-LEARNING-Capture-Script
      exit_criteria:
      - prose: scripts/review_learnings.sh shows last 30 days
      - prose: Identifies patterns (same root cause multiple times)
      - prose: Suggests meta-learnings
      - prose: "Tracks learning\u2192prevention mapping"
      domain: mcp
      complexity_score: 4
      effort_hours: 1
      required_tools: []
      auto_created: true
      source_issue:
        type: quality
        severity: medium
        gap: Learning review not yet automated (Phase 7)
      evidence_path: state/evidence/FIX-LEARNING-Review-Script
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: FIX-LEARNING-Prevention-Workflow
      title: Prevention layer update workflow (Task 7.3, 0.9)
      status: pending
      dependencies:
        depends_on:
        - FIX-LEARNING-Capture-Script
      exit_criteria:
      - prose: Every learning MUST result in prevention update within 7 days
      - prose: Track learning count vs prevention count
      - prose: Alert if gap >10 learnings without prevention
      - prose: Tag each prevention with learning ID
      - prose: Track prevention effectiveness (recurrence rate)
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      auto_created: true
      source_issue:
        type: quality
        severity: medium
        gap: "Learning\u2192Prevention SLA not enforced (Task 0.9)"
      evidence_path: state/evidence/FIX-LEARNING-Prevention-Workflow
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M-QUALITY-PREMORTEM
    title: Pre-Mortem Mitigations (Phase 8)
    status: pending
    tasks:
    - id: FIX-PREMORTEM-Phased-Rollout
      title: Phased rollout plan (Task 0.1)
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Week 1 - Observe mode (log, don't block)
      - prose: Week 2 - Analyze false positive rate
      - prose: Week 3 - Tune thresholds based on data
      - prose: Week 4 - Enable blocking for critical only
      - prose: Month 2 - Gradually enable all checks
      - prose: Rollout documented in docs/
      domain: mcp
      complexity_score: 3
      effort_hours: 1
      required_tools: []
      auto_created: true
      source_issue:
        type: quality
        severity: medium
        gap: Phased rollout not planned (Pre-mortem Scenario 1)
      evidence_path: state/evidence/FIX-PREMORTEM-Phased-Rollout
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: FIX-PREMORTEM-FP-Tracking
      title: False positive tracking system (Task 0.2)
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Dashboard metric - FP rate by check type
      - prose: Weekly review - Top 5 FP generators
      - prose: Suppression analysis - Which checks get suppressed most
      - prose: Feedback mechanism - Mark issue as false positive
      - prose: Circuit breaker - Auto-disable if FP rate >20%
      domain: mcp
      complexity_score: 5
      effort_hours: 2
      required_tools: []
      auto_created: true
      source_issue:
        type: quality
        severity: medium
        gap: FP tracking not implemented (Pre-mortem Scenario 1)
      evidence_path: state/evidence/FIX-PREMORTEM-FP-Tracking
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: FIX-PREMORTEM-Performance-SLO
      title: Performance SLO monitoring (Task 0.3)
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Log execution time for every run
      - prose: Alert if p95 >80% of limit
      - prose: Dashboard shows performance trends
      - prose: Monthly performance review process
      - prose: SLOs - Preflight <30s, Hunt <5min, Dashboard <10s
      domain: mcp
      complexity_score: 4
      effort_hours: 1
      required_tools: []
      auto_created: true
      source_issue:
        type: quality
        severity: medium
        gap: Performance SLO monitoring not implemented (Pre-mortem Scenario 2)
      evidence_path: state/evidence/FIX-PREMORTEM-Performance-SLO
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: FIX-PREMORTEM-Self-Monitoring
      title: System self-monitoring (Task 0.6)
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Heartbeat - Cron jobs report success
      - prose: Health checks - Verify all components working
      - prose: Alert if any component silent >24h
      - prose: Dashboard shows system health
      domain: mcp
      complexity_score: 4
      effort_hours: 1
      required_tools: []
      auto_created: true
      source_issue:
        type: quality
        severity: medium
        gap: Self-monitoring not implemented (Pre-mortem Scenario 5)
      evidence_path: state/evidence/FIX-PREMORTEM-Self-Monitoring
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: FIX-PREMORTEM-Evidence-Quality-Scoring
      title: Evidence quality scoring (Task 0.8)
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Size heuristic - >500 words minimum
      - prose: Vocabulary diversity - Shannon entropy
      - prose: Template detection - Fuzzy match against common templates
      - prose: Substantiveness - Check for specific details
      - prose: Cross-linking - References to other evidence
      - prose: Score 0-1, require >0.8
      domain: mcp
      complexity_score: 6
      effort_hours: 2
      required_tools: []
      auto_created: true
      source_issue:
        type: quality
        severity: low
        gap: Evidence quality scoring not semantic (Pre-mortem Scenario 7)
      evidence_path: state/evidence/FIX-PREMORTEM-Evidence-Quality-Scoring
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  - id: M-QUALITY-OPTIMIZATION
    title: Performance & Documentation (Cross-Cutting)
    status: pending
    tasks:
    - id: FIX-PERF-Optimization
      title: Performance optimization (Task X.3)
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Incremental architecture - Only scan changed files
      - prose: Cache analysis results per file
      - prose: Parallel execution where possible
      - prose: Sampling for large codebases (random 10%)
      - prose: Load testing - Simulate 100K LOC codebase
      - prose: All scripts meet SLOs at 100K LOC scale
      domain: mcp
      complexity_score: 7
      effort_hours: 3
      required_tools: []
      auto_created: true
      source_issue:
        type: quality
        severity: medium
        gap: Performance not optimized for scale (Pre-mortem Scenario 2)
      evidence_path: state/evidence/FIX-PERF-Optimization
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
    - id: FIX-DOCS-Comprehensive
      title: Comprehensive documentation (Task X.2)
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Per-script READMEs
      - prose: Troubleshooting guide
      - prose: Integration call graph
      - prose: Examples of common workflows
      - prose: FAQ section
      domain: mcp
      complexity_score: 4
      effort_hours: 2
      required_tools: []
      auto_created: true
      source_issue:
        type: quality
        severity: low
        gap: Documentation gaps identified in review
      evidence_path: state/evidence/FIX-DOCS-Comprehensive
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
  description: 'Follow-up work deferred from WORK-PROCESS-FAILURES task: WorkProcessEnforcer
    integration, CI pipeline, learning system, pre-mortem mitigations, performance
    optimization, comprehensive testing.'
- id: E-EVIDENCE-HYBRID
  title: Hybrid Execution-Based + Document-Based Evidence (Exploratory)
  status: proposed
  domain: governance
  description: 'EXPLORATORY: Investigate augmenting (NOT replacing) the current rigorous
    9-phase document-based evidence system with executable checks (tests, scripts,
    metrics). Goal is ADDITIVE rigor - keep all existing document evidence AND add
    automated verification. This is NOT about reducing rigor or overhead, but about
    making evidence executable and continuous (runs in CI, catches regressions). Approach
    uncertain - may result in ROLLOUT, ITERATE, or ABANDON based on pilot data.

    '
  milestones:
  - id: M-EVIDENCE-PILOT
    title: Pilot Execution-Based Evidence (Exploratory)
    status: proposed
    priority: medium
    tasks:
    - id: META-EXEC-01
      title: Pilot hybrid execution-based + document-based evidence system
      status: pending
      priority: medium
      dependencies:
        depends_on: []
        blocks: []
      warning: "\u26A0\uFE0F  CRITICAL: User is HESITANT about this approach and prefers\
        \ HYBRID (additive).\n\nDO NOT proceed with \"execution-only\" evidence that\
        \ replaces documents.\nDO NOT reduce rigor or skip existing document-based\
        \ evidence.\nDO NOT treat this as \"simplification\" or \"overhead reduction\"\
        .\n\nThis is EXPLORATORY. User wants to ADD executable checks ON TOP OF existing\n\
        rigor, not replace it. If you're unsure, ASK THE USER before proceeding.\n\
        \nThe user's exact words: \"I'm not even sure that this is the approach and\n\
        I'd prefer a hybrid approach that still requires all the rigor we developed\n\
        but also adds more.\"\n\nAcceptable outcomes: ROLLOUT (works), ITERATE (needs\
        \ refinement), or\nABANDON (not worth it). All three are valid based on pilot\
        \ data.\n"
      exit_criteria:
      - prose: 'One pilot task completed using HYBRID approach: all existing document
          evidence PLUS executable checks (tests, scripts, metrics)'
      - test: bash state/evidence/META-EXEC-01/verify/pilot_task_check.sh
        expect: exit 0
      - prose: Pilot task's executable evidence runs in CI and catches real issues
      - test: gh run list --workflow=pilot-evidence.yml --status=success --limit=1
        expect: exit 0
      - prose: Reusable templates created for adding executable evidence to each phase
      - test: ls templates/evidence/{strategize,spec,plan,think,verify,review,pr,monitor}/*.{ts,sh}
          | wc -l
        expect: "\u2265 16 files (2 per phase)"
      - prose: 'Comparative analysis: hybrid vs document-only (time, effectiveness,
          ongoing value)'
      - test: test -f docs/process/execution_based_evidence_results.md && grep -E
          'Time to create|Issues caught|Runs in CI' docs/process/execution_based_evidence_results.md
        expect: exit 0
      - prose: 'Decision made: ROLLOUT, ITERATE, or ABANDON based on pilot results'
      - test: 'grep -E ''Decision: (ROLLOUT|ITERATE|ABANDON)'' state/evidence/META-EXEC-01/review/decision.md'
        expect: exit 0
      domain: governance
      complexity_score: 10
      effort_hours: 10
      required_tools:
      - bash
      - typescript
      - jest
      - github-actions
      description: 'IMPORTANT: This is NOT about replacing document-based evidence.
        This is about AUGMENTING it with executable checks. The hypothesis: adding
        tests/scripts/metrics ON TOP of existing rigor will catch more issues and
        provide continuous verification without reducing thoughtfulness.

        Approach: Pick one medium-complexity task and execute it with BOTH: (1) All
        existing document-based evidence (strategy.md, spec.md, etc.) (2) NEW executable
        evidence (examples.test.ts, smoke_test.sh, metrics.ts, etc.)

        The goal is MORE rigor, not less. More verification, not less documentation.
        More continuous checking, not less upfront thinking.

        After pilot, measure: - Time overhead (is hybrid 2x slower? Acceptable?) -
        Effectiveness (does executable catch issues docs miss?) - Ongoing value (does
        evidence run in CI? Catch regressions?)

        Then decide: - ROLLOUT: Hybrid approach works, expand to all tasks - ITERATE:
        Mixed results, refine and re-pilot - ABANDON: Not worth overhead, keep document-only

        Deliverables: 1. Pilot task evidence (document + executable) 2. Templates
        for adding executable to each phase 3. Comparative metrics (time, effectiveness,
        value) 4. Decision doc with clear rationale 5. If ROLLOUT: Updated CLAUDE.md
        and WorkProcessEnforcer

        '
      evidence_path: state/evidence/META-EXEC-01
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: warn
      notes: 'USER CAVEAT: "I''m not even sure that this is the approach and I''d
        prefer a hybrid approach that still requires all the rigor we developed but
        also adds more." This task respects that - it''s exploratory and additive,
        not a replacement. If pilot shows hybrid is too expensive for value gained,
        that''s a valid outcome.

        '
- id: E-AUTO-FOLLOWUPS
  title: Auto-generated Follow-Up Tasks
  status: pending
  domain: mcp
  description: Container for automatically generated follow-up work items.
  milestones:
  - id: M-AUTO-FOLLOWUPS
    title: Auto Follow-Up Backlog
    status: pending
    tasks:
    - id: AUTO-FU-C022BEC6C639
      title: '- Add automation to invoke precision checker in CI once corpus has >=50
        vectors.'
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Follow-up resolved or documented with evidence.
      - file: state/evidence/AUTO-FU-C022BEC6C639/verify/verification.md
        expect: exists
      domain: mcp
      description: 'Auto-generated follow-up from /Volumes/BigSSD4/nathanielschmiedehaus/Documents/WeatherVane/state/evidence/IMP-QG-01/monitor/plan.md:18


        - Add automation to invoke precision checker in CI once corpus has >=50 vectors.'
      complexity_score: 3
      effort_hours: 2
      required_tools: []
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_path: state/evidence/AUTO-FU-C022BEC6C639
      evidence_enforcement: warn
      notes: classification=scheduled_improvement; generated_at=2025-10-30T12:54:02.797Z
    - id: META-AUDIT-01
      title: Automate improvement reviews
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Automated review job validates autopilot/non-autopilot improvements
          with no high severity findings
      - test: node tools/wvo_mcp/scripts/run_review_automation.ts
        expect: exit 0
      domain: mcp
      description: Build tooling to automatically audit completed improvement tasks
        (inside/outside Autopilot) for integration, documentation, and testing gaps.
      complexity_score: 6
      effort_hours: 4
      required_tools: []
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_path: state/evidence/META-AUDIT-01
      evidence_enforcement: warn
    - id: META-AUDIT-02
      title: Define scope for comprehensive improvement audits
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Documented scope covering all completed improvement batch items and
          autopilot components with prioritised audit plan
      - file: state/evidence/META-AUDIT-02/strategize/scope.md
        expect: exists
      - test: node tools/wvo_mcp/scripts/run_review_scope_check.ts
        expect: exit 0
      domain: mcp
      description: Establish logical boundaries, sequencing, and automation requirements
        for auditing all completed improvements inside and outside Autopilot before
        execution begins.
      complexity_score: 5
      effort_hours: 3
      required_tools: []
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_path: state/evidence/META-AUDIT-02
      evidence_enforcement: warn
    - id: META-AUDIT-WP
      title: Define scope for work-process evidence audits
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: "Documented scope for auditing STRATEGIZE\u2192MONITOR artefacts across\
          \ completed tasks"
      - file: state/evidence/META-AUDIT-WP/strategize/scope.md
        expect: exists
      domain: mcp
      description: Establish audit boundaries and cadence for verifying work-process
        outputs (evidence presence, delta notes, follow-ups).
      complexity_score: 4
      effort_hours: 3
      required_tools: []
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_path: state/evidence/META-AUDIT-WP
      evidence_enforcement: warn
    - id: META-AUDIT-03
      title: Implement automated improvement audits
      status: pending
      dependencies:
        depends_on:
        - META-AUDIT-02
        - META-AUDIT-WP
      exit_criteria:
      - prose: Audit runner produces consolidated audit_report.json covering defined
          domains
      - test: node tools/wvo_mcp/scripts/run_review_audit.ts --workspace-root . --output
          state/automation/audit_report.json
        expect: exit 0
      - file: state/automation/audit_report.json
        expect: exists
      domain: mcp
      description: Build tooling to execute the scoped improvement/work-process audits,
        orchestrate checks, collect results, and surface failures.
      complexity_score: 7
      effort_hours: 6
      required_tools:
      - cmd_run
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_path: state/evidence/META-AUDIT-03
      evidence_enforcement: warn
- id: E-META-TESTING-STANDARDS-FOLLOW-UP
  title: META-TESTING-STANDARDS Follow-Up Tasks
  status: pending
  domain: mcp
  milestones:
  - id: M-META-TEST-ENFORCEMENT
    title: Verification Standards Enforcement
    status: pending
    tasks:
    - id: FIX-META-TEST-ENFORCEMENT
      title: Integrate verification levels into WorkProcessEnforcer
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: WorkProcessEnforcer can detect verification level from evidence documents
      - prose: Phase transitions blocked if verification level insufficient
      - prose: Clear error messages when blocking transitions
      - test: npm run test -- work_process_enforcer
        expect: exit 0
      - prose: Can be disabled via flag for emergencies
      domain: mcp
      description: |
        Integrate verification level checking into WorkProcessEnforcer to automatically block phase transitions when verification level insufficient.

        **Implementation**:
        - Add detectVerificationLevel() function to parse evidence documents
        - Check Level 1 before IMPLEMENT  VERIFY transition
        - Check Level 2 before VERIFY  REVIEW transition
        - Check Level 3 (or valid deferral) before REVIEW  PR transition
        - Start in observe mode (log mismatches), upgrade to enforce after validation

        **Source**: REVIEW adversarial_review.md (Gap 2, lines 229-234) from META-TESTING-STANDARDS
      complexity_score: 8
      effort_hours: 4
      required_tools:
      - fs_read
      - fs_write
      auto_created: true
      source_issue:
        type: quality
        severity: medium
        gap: "WorkProcessEnforcer integration deferred (AC7 from META-TESTING-STANDARDS)"
      evidence_path: state/evidence/FIX-META-TEST-ENFORCEMENT
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: enforce
    - id: FIX-META-TEST-GAMING
      title: Detect gaming patterns (trivial tests, mock abuse)
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: Script can detect tests without assertions
      - prose: Script can detect mock-heavy "integration" tests
      - prose: Script flags weak deferral justifications
      - test: bash scripts/detect_test_gaming.sh
        expect: exit 0
      - prose: Integrated into pre-commit or VERIFY phase checks
      domain: mcp
      description: |
        Add automated detection for common gaming patterns to prevent agents from claiming verification levels without proper testing.

        **Gaming scenarios addressed**:
        - Trivial tests (no assertions)
        - Mock-only integration tests (claiming Level 3 with all mocks)
        - Cherry-picked evidence (showing only passing tests)

        **Implementation**:
        - Static analysis: Check test files contain expect() / assert() statements
        - Integration detector: Parse test code for mock usage vs real dependencies
        - Deferral reasonableness check: Flag generic deferral reasons ("don't have time")
        - Add to VERIFY phase automated checks

        **Source**: REVIEW adversarial_review.md (Gap 4, lines 237-243) + Q4 (lines 80-115) from META-TESTING-STANDARDS
      complexity_score: 7
      effort_hours: 5
      required_tools:
      - fs_read
      - cmd_run
      auto_created: true
      source_issue:
        type: quality
        severity: low_medium
        gap: "Agents could game system with trivial tests or mock-only integration (from META-TESTING-STANDARDS)"
      evidence_path: state/evidence/FIX-META-TEST-GAMING
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: enforce
    - id: FIX-META-TEST-MANUAL-SESSIONS
      title: Apply verification standards to manual Claude sessions
      status: pending
      dependencies:
        depends_on: []
      exit_criteria:
      - prose: CLAUDE.md explicitly states standards apply to manual sessions
      - prose: Lightweight checklist available for quick tasks (don't need full evidence structure)
      - prose: Examples show verification level documentation for manual work
      - prose: VERIFICATION_LEVELS.md introduction updated to clarify scope
      domain: mcp
      description: |
        Ensure verification level standards apply consistently whether working within autopilot or manual Claude sessions.

        **Current gap**: Standards focus on autopilot work process, may not apply to manual sessions outside autopilot

        **Implementation**:
        - Add section to CLAUDE.md: "Verification Levels for Manual Sessions"
        - Create lightweight checklist for quick manual tasks
        - Update WORK_PROCESS.md note: "Applies to both autopilot and manual sessions"
        - Provide examples of manual session verification documentation
        - Update VERIFICATION_LEVELS.md introduction to clarify scope

        **Source**: User request (2025-10-30) + REVIEW adversarial_review.md (Recommendation 6, lines 323-326) from META-TESTING-STANDARDS
      complexity_score: 5
      effort_hours: 3
      required_tools:
      - fs_read
      - fs_write
      auto_created: true
      source_issue:
        type: quality
        severity: medium
        gap: "Standards currently focus on autopilot work process, should also apply to manual Claude sessions (from META-TESTING-STANDARDS)"
      evidence_path: state/evidence/FIX-META-TEST-MANUAL-SESSIONS
      work_process_phases:
      - strategize
      - spec
      - plan
      - think
      - implement
      - verify
      - review
      - pr
      - monitor
      evidence_enforcement: enforce
last_updated: '2025-10-30T14:07:38.3NZ'
updated_by: claude_code
