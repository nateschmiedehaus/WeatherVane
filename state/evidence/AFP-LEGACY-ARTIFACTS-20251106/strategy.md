# Strategy Analysis ‚Äî AFP-LEGACY-ARTIFACTS-20251106

**Template Version:** 1.0  
**Date:** 2025-11-06  
**Author:** Codex (WeatherVane)

---

## Purpose

Large sections of the repo (supervisor sources, schemas, historical AFP evidence, taskflow tooling) are currently untracked. That leaves us blind to prior work, makes reproducibility impossible, and breaks the policy that every artifact must land in Git/GitHub. We need to inventory the backlog, keep what belongs in version control, and intentionally ignore any generated noise. This closes the gap between documentation (‚Äúalways push to GitHub‚Äù) and reality.

---

## Problem Statement

Hundreds of files under `autopilot_mvp/`, `shared/schemas/`, `state/evidence/`, and `tools/taskflow/` are untracked even though they are part of the working toolchain or evidence history. Reviewers cannot diff them, ProcessCritic cannot enforce policies, and GitHub has no record. Agents (human or Autopilot) are told to stage/commit everything, yet the repo violates that rule.

**Stakeholders:** All agents (lack of history causes duplicate work), reviewers (cannot audit evidence), infrastructure (can‚Äôt restore state), leadership (compliance breach).

---

## Root Cause Analysis

- Historical automation imported artifacts without committing them.
- No guardrail verified Git cleanliness.
- Agents assumed evidence directories generated by other teams were already in history.

Evidence: `git status` shows dozens of AFP evidence folders, supervisor TypeScript/JS pairs, taskflow source, missing schema declarations.

---

## Current vs Desired State

Current: Git reports >150 untracked items (schemas, supervisor runtime, state evidence). Process/AXP docs claim everything‚Äôs tracked but reality disagrees.

Desired: Every intentional artifact versioned; only generated noise excluded via `.gitignore`. `git status` clean after tasks.

Gap: Lack of classification/inventory + no enforcement.

---

## Success Criteria

1. `git status` shows no untracked files after task completes.
2. Autopilot supervisor TypeScript sources (and only sources) are versioned; compiled artifacts ignored.
3. Historical AFP evidence directories committed with metadata intact.
4. `.gitignore` documents any intentionally ignored generated outputs (if needed).
5. Process docs already updated to require Git usage remain consistent (no contradictions).

---

## Impact Assessment

- **Quality:** Reintroduces traceability and audit trail (critical for autopilot compliance).
- **Risk:** Eliminates ambiguity on what‚Äôs considered canonical (no more local-only evidence).
- **Strategic:** Enables ProcessCritic/Git tooling to operate on clean tree; baseline for future automation.
- **Cost:** One-time organization effort; afterwards autop and docs teams can build on accurate repo state.

If ignored: future tasks reintroduce drift, compliance claims weakened, evidence lost.

---

## Alignment with AFP/SCAS

- **Via Negativa:** Deletes uncertainty; prevents silent drift.
- **Refactor vs Repair:** Addresses root cause (lack of tracking) not just symptoms.
- **AFP/SCAS:** Reinstates visibility + coherence required by ProcessCritic.

**Complexity Control:**
[Does this increase or decrease system complexity? Justify.]

**Examples:**
- "Increases code complexity: +900 LOC (critic + template + scripts). Decreases cognitive complexity: clear quality bar, automated enforcement. Net: justified trade-off."

**Force Multiplier:**
[Does this amplify future value delivery?]

**Examples:**
- "Proven pattern extends to THINK phase, SPEC phase. Enables better task selection ‚Üí less waste ‚Üí more value per token spent. Compounds over time as agents learn from feedback."

---

## Risks and Mitigations

**What could go wrong with this task?**

[List 3-5 risks with honest assessment]

**Risk 1: [Risk description]**
- **Likelihood:** [High/Medium/Low]
- **Impact:** [High/Medium/Low]
- **Mitigation:** [How will we address this?]

**Risk 2: [Risk description]**
- **Likelihood:** [High/Medium/Low]
- **Impact:** [High/Medium/Low]
- **Mitigation:** [How will we address this?]

**Examples:**
- ‚ùå GENERIC: "Risk: Implementation might fail"
- ‚úÖ SPECIFIC: "Risk: StrategyReviewer too strict ‚Üí false positives ‚Üí agent frustration ‚Üí gaming behavior. Likelihood: Medium (DesignReviewer has ~5% false positive rate). Impact: High (erodes trust). Mitigation: Human escalation path always available, analytics track false positive rate, tune thresholds based on data."

---

## Dependencies and Constraints

**What does this task depend on?**

[List prerequisites: tools, data, other tasks, approvals]

**Examples:**
- "Depends on: Critic base class (exists: tools/wvo_mcp/src/critics/base.ts), Research layer (exists), Analytics infrastructure (exists: state/analytics/)"

**What constraints must we respect?**

[List limitations: time, budget, technical, policy]

**Examples:**
- "Constraints: Micro-batching limits (‚â§5 files, ‚â§150 LOC - will need to split into sub-tasks), Token budget (must use intelligence layer sparingly), DesignReviewer pattern (must maintain consistency)"

---

## Open Questions

**What don't we know yet?**

[List uncertainties that might affect the approach. Be honest.]

**Examples:**
- ‚ùå PRETENDING TO KNOW: "This will definitely work"
- ‚úÖ HONEST: "Unknown: Will agents game the critic by writing longer but still superficial strategy docs? Mitigation: Start with semantic analysis, evolve based on analytics. Unknown: What's the right balance between strictness and flexibility? Mitigation: Monitor false positive/negative rates, tune thresholds."

**Questions:**

1. [Question 1]
2. [Question 2]
3. [Question 3]
4. [Question 4, optional]
5. [Question 5, optional]

---

## Recommendation

**Should we do this task?**

[Yes/No/Defer and why]

**Examples:**
- ‚ùå WEAK: "Yes, sounds good"
- ‚úÖ STRONG: "YES - proceed immediately. Strong evidence of problem (40% compliance, 20 hours waste per cycle). Clear impact (save 20 hours + 1M tokens per cycle). Proven pattern (DesignReviewer works). High strategic value (extends to other phases). Low risk (human escalation path, analytics feedback loop)."

**If YES:**
- **Priority:** [Critical/High/Medium/Low]
- **Urgency:** [Immediate/Soon/Can wait]
- **Effort:** [Small/Medium/Large - rough estimate]

**If NO or DEFER:**
- **Why not?** [Specific reasoning]
- **What would change your mind?** [What evidence/conditions would make this worthwhile?]

---

## Notes

[Any additional context, references, or decisions made during analysis]

**References:**
- [Link to related tasks, docs, code, discussions]

**Decisions:**
- [Key decisions made during strategy phase]

---

**Strategy Complete:** [YYYY-MM-DD]
**Next Phase:** SPEC (define requirements and acceptance criteria)

---

## Anti-Patterns to Avoid

**This template should help you avoid:**
- üö´ Jumping straight to solutions (focus on WHY and WHAT, not HOW)
- üö´ Vague problem statements ("improve quality" vs specific evidence)
- üö´ Shallow root cause analysis (stopping at symptoms)
- üö´ Unmeasurable success criteria ("better" vs quantified targets)
- üö´ Generic risk assessment (specific risks with likelihood/impact)
- üö´ Missing evidence (claims without supporting data)
- üö´ Solution bias (starting with "we need X tool" vs "we need to achieve Y outcome")

**Remember:** Strategy is about THINKING, not TYPING. If your strategy.md is < 30 lines, you probably haven't thought deeply enough.
