# Deep Thinking Analysis — AFP-PROCESS-TEST-SEQUENCE-20251106

**Template Version:** 1.0  
**Date:** 2025-11-06  
**Author:** Codex (WeatherVane)

---

## Edge Cases

1. **Docs-only tasks with no automated suite.**  
   - Scenario: Task touches only documentation; policy should not require fake unit tests.  
   - Impact: Low risk of confusion.  
   - Mitigation: PLAN must explicitly document "Tests not applicable" with justification; VERIFY simply acknowledges the exemption.

2. **Legacy areas lacking fixtures.**  
   - Scenario: We require tests during PLAN but the module has no harness.  
   - Impact: Medium; PLAN could stall.  
   - Mitigation: Allow skipped/failing tests with TODO markers and note required scaffolding so IMPLEMENT builds the harness next.

3. **Hotfix under extreme urgency.**  
   - Scenario: Sev-1 bug fix tempts bypassing PLAN tests.  
   - Impact: High if policy unclear.  
   - Mitigation: Document that PLAN must at least author failing regression tests immediately, even if they cannot yet run, and reviewers should block otherwise.

4. **Refactor removing redundant tests.**  
   - Scenario: Via negativa work deletes tests; requirement may seem to force additions anyway.  
   - Impact: Medium confusion.  
   - Mitigation: PLAN guidance will cover add/update/delete expectations so deletion decisions are recorded before IMPLEMENT.

5. **Cross-team coordination on shared tests.**  
   - Scenario: Multiple agents need to touch common suites; PLAN authoring could diverge.  
   - Impact: Medium due to merge conflicts.  
   - Mitigation: PLAN should allocate ownership and note dependencies; IMPLEMENT coordinates via roadmap tasks.

6. **Test files requiring generated data.**  
   - Scenario: Tests rely on large fixtures generated by scripts not yet available.  
   - Impact: Medium — PLAN authored tests might fail immediately.  
   - Mitigation: Allow lightweight stub data during PLAN; IMPLEMENT later replaces with full fixtures while keeping test intent intact.

7. **Changes that primarily adjust configuration.**  
   - Scenario: Work modifies config toggles where automated tests are heavy.  
   - Impact: Medium risk of over-scoping PLAN.  
   - Mitigation: PLAN will document targeted smoke/manual checks sufficient to validate behaviour without violating micro-batching limits.

8. **Autopilot-generated tasks with limited context.**  
   - Scenario: Autonomous agent may not have environment to author tests.  
   - Impact: High potential for policy drift.  
   - Mitigation: Documentation will instruct autopilot agents to log requested tests in PLAN and coordinate with human operators if environment constraints block execution.

---

## Failure Modes

1. **Failure Mode: Conflicting Documentation**  
   - Cause: Some docs still reference VERIFY for authoring.  
   - Symptom: Agents cite old instructions.  
   - Impact: High.  
   - Likelihood: Medium.  
   - Detection: Manual audit during PR; future reviewer feedback.  
   - Mitigation: Update all primary docs in this change and note follow-up if stragglers exist.

2. **Failure Mode: Overly Rigid Interpretation**  
   - Cause: Language implies tests must pass before implementation can start.  
   - Symptom: Agents delay coding.  
   - Impact: Medium.  
   - Likelihood: Medium.  
   - Detection: Feedback in roadmap or reviewer comments.  
   - Mitigation: Explicitly state PLAN-authored tests may fail or be skipped until implementation catches up.

3. **Failure Mode: Template Drift**  
   - Cause: Template update conflicts with DesignReviewer heuristics.  
   - Symptom: False failures during GATE.  
   - Impact: Low.  
   - Likelihood: Low.  
   - Detection: Monitor critic output after change.  
   - Mitigation: Keep addition concise and in existing structure.

4. **Failure Mode: Mis-scoped Test Expectations**  
   - Cause: Agents interpret rule as requiring full integration suites for small fixes.  
   - Symptom: Micro-batching guardrails exceeded.  
   - Impact: Medium.  
   - Likelihood: Medium.  
   - Detection: Review PLAN artefacts for test scope.  
   - Mitigation: Include guidance that PLAN should right-size tests to change scope.

5. **Failure Mode: Lack of Enforcement**  
   - Cause: Documentation changes without accountability.  
   - Symptom: PLAN artefacts continue omitting tests.  
   - Impact: Medium.  
   - Likelihood: Medium.  
   - Detection: Review future task evidence; track via reviewers.  
   - Mitigation: Mention expectation that reviewers reject implementations lacking PLAN-authored tests; consider future critic task if behaviour persists.

---

## Essential vs Accidental Complexity

- **Essential complexity:** Establishing a single authoritative phase for test authoring is inherent to solving the behavioural problem. Clear documentation across guidelines, checklists, and templates is unavoidable to shift habits.
- **Accidental complexity:** Navigating multiple docs risks wording divergence. Template edits could add noise if overly verbose.
- **Mitigation:** Keep wording concise, reuse phrases across files, and reference PLAN explicitly to minimise accidental discrepancies.

---

## Mitigation Strategies

### Prevention
- Align all touched docs in a single change to avoid stale instructions.
- Emphasise allowances for skipped/failing tests so agents do not bypass the rule out of fear.
- Tie PLAN guidance to micro-batching limits to prevent scope creep.
- Clarify exceptions (docs-only tasks) to prevent unnecessary work.
- Document reviewer expectations so enforcement happens before regressions.

### Detection
- Monitor future PLAN artefacts for absence of test entries; reviewers can flag violations.
- Use DesignReviewer feedback—template addition should surface missing test plans.
- Watch roadmap/PR discussions for confusion that signals unclear wording.
- Track autopilot task logs for mentions of missing environments when tests cannot be authored.
- Review `state/context.md` updates for mention of PLAN test compliance issues.

### Recovery
- If conflicting docs are discovered later, schedule a remedial task to align them immediately.
- When PLAN lacks tests, require remediation tasks before implementation proceeds.
- Provide fallback guidance on writing skipped tests quickly (e.g., skeleton tests with TODO) so agents can recover without blocking.
- Encourage reviewers to annotate deviations, creating a feedback loop to update docs if policies prove impractical.
- If template change causes critic false positives, quickly iterate on the template wording in a follow-up micro-task.

---

## Testing Strategy

- **Documentation consistency check:** Manually review the modified docs to confirm every reference points to PLAN for test authoring and VERIFY for execution.
- **Checklist validation:** Simulate creating a new task artefact following the updated checklist to ensure the PLAN checkbox prompts recording authored tests.
- **Template dry run:** Instantiate `design.md` from the template and confirm the Implementation Plan section now requires enumerating PLAN-authored tests.
- **Process rehearsal:** Walk through a hypothetical change (e.g., adding API endpoint) to verify the timeline: PLAN writes failing pytest case, IMPLEMENT makes it pass, VERIFY runs it. This ensures instructions are unambiguous.
- **Reviewer alignment:** Share the updated wording with reviewers (via PR description) and solicit feedback to ensure expectations are clear before adoption.

---

## Paranoid Thinking

1. **Policy ignored despite update.**  
   - Prevention: Highlight change in summary and context updates.  
   - Recovery: Schedule enforcement critic task if non-compliance observed.

2. **Agents misinterpret requirement as blocking partial testing.**  
   - Prevention: Wording explicitly allows failing/skipped tests.  
   - Recovery: Add FAQ or clarifying note if confusion persists.

3. **Autopilot lacks permissions to write tests in PLAN.**  
   - Prevention: Document expectation that autopilot logs blockers.  
   - Recovery: Provide escalation path to human operators to author tests.

4. **Template change causes GATE noise.**  
   - Prevention: Minimal addition referencing existing bullets.  
   - Recovery: Rapidly adjust template if DesignReviewer flags false positives.

5. **Process change increases cycle time unacceptably.**  
   - Prevention: Set expectation that tests can be skeletal initially.  
   - Recovery: Collect data on cycle time; if impact high, iterate on guidance (e.g., allow pairing with THINK for simple fixes).

6. **Documentation drift returns later.**  
   - Prevention: Log change in `state/context.md` so future agents know policy origin.  
   - Recovery: Establish periodic audit tasks if drift detected.

---

## Assumptions

1. **Agents can create or stub tests in PLAN.**  
   - Impact if false: Policy ignored.  
   - Likelihood: Medium.  
   - Mitigation: Allow skipped/failing tests with TODOs.

2. **Plan artefacts are reviewed prior to IMPLEMENT.**  
   - Impact if false: Tests still added late.  
   - Likelihood: Medium.  
   - Mitigation: Reinforce reviewer obligations.

3. **Documentation updates propagate to new agents.**  
   - Impact if false: Old behaviour persists.  
   - Likelihood: Medium.  
   - Mitigation: Update `state/context.md` and onboarding materials.

4. **Design template addition will be read before PLAN is finalised.**  
   - Impact if false: Template guidance unused.  
   - Likelihood: Low.  
   - Mitigation: Keep instruction concise so it stands out.

5. **Micro-batching limits remain unchanged.**  
   - Impact if false: Test authoring may exceed allowed LOC.  
   - Likelihood: Low.  
   - Mitigation: Note that tests must respect current limits.

6. **Autopilot has file-system permissions to save PLAN tests.**  
   - Impact if false: Policy technically impossible.  
   - Likelihood: Low.  
   - Mitigation: Provide escalation step when permission denied.

7. **Reviewers will enforce the new rule.**  
   - Impact if false: No behaviour change.  
   - Likelihood: Medium.  
   - Mitigation: Document expectation explicitly in AGENTS.md.

8. **Agents understand difference between authoring and running tests.**  
   - Impact if false: VERIFY instructions misapplied.  
   - Likelihood: Medium.  
   - Mitigation: Provide examples describing timeline.

9. **Plan to update only four files is sufficient.**  
   - Impact if false: Additional docs may need change.  
   - Likelihood: Medium.  
   - Mitigation: Note potential follow-up tasks if other docs discovered.

10. **Templates remain ASCII-compatible after edits.**  
    - Impact if false: Tooling errors.  
    - Likelihood: Low.  
    - Mitigation: Verify diff for encoding issues.

11. **DesignReviewer will tolerate added bullet text.**  
    - Impact if false: Blocks tasks.  
    - Likelihood: Low.  
    - Mitigation: Respond quickly with revision if issues appear.

12. **Wording about exceptions (docs-only) will not be abused.**  
    - Impact if false: Agents skip tests incorrectly.  
    - Likelihood: Medium.  
    - Mitigation: Require explicit justification in PLAN for any exemption.

---

**Thinking Complete:** 2025-11-06
