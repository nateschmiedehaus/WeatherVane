# Design: AFP-W0-M1-WORKTREE-STABILIZE-SEMANTIC

> **Purpose:** Document design thinking for semantic merge (Batch 2) that enhances Batch 1 with structure-aware TypeScript/JSON merging

---

## Context

**What problem are you solving and WHY?**

**Problem**: Batch 1 (just completed) uses **text-based** auto-merge (`git merge-file`), which fails when both agents modify the same line number - even if changes are semantically independent.

**Example failure**:
```typescript
// Base: export function foo() { return 1; }
// Agent A adds bar() at line 2
// Agent B adds baz() at line 2
// Auto-merge FAILS (both modified line 2)
// Falls to union merge (manual review)
```

**But semantically**: No conflict! Different function names ‚Üí both should be kept.

**Current Batch 1 success rate**: 50-70% auto-merge + 30-50% union (manual)

**Gap**: 20-30% of conflicts are structure-overlapping but semantically independent

**WHY Batch 2 needed**:
- Batch 1 is text-blind (only sees line numbers)
- Batch 2 is structure-aware (understands imports, functions, JSON keys)
- Expected improvement: 70-90% automated (vs 50-70% in Batch 1)

---

## Five Forces Check

### COHERENCE - Match the terrain

- [x] I searched for similar patterns in the codebase
- Modules checked:
  1. `tools/wvo_mcp/scripts/merge_helpers.sh` - Batch 1 merge functions
  2. `tools/wvo_mcp/scripts/git_error_recovery.sh` - Existing merge workflow
  3. Various grep-based extraction patterns in shell scripts

- Pattern I'm reusing: **Structure extraction with validation**
  - Existing: git hygiene scripts use grep for pattern extraction
  - Batch 2: Uses grep for TypeScript structure + jq for JSON
  - Validation ensures correctness (tsc, jq)

**Similar patterns found**:
1. `tools/wvo_mcp/scripts/git_error_recovery.sh:52-86` - grep-based file detection
2. Batch 1 merge_helpers.sh - Validation pattern (check then stage)
3. Standard Unix pattern: grep/sed for text processing

**Pattern selected**: Structure extraction + validation + fallback chain

**Why this pattern**: Proven approach (grep is standard), fast, good enough for MVP

---

### ECONOMY - Achieve more with less

- [x] I explored deletion/simplification (via negativa)
- Code I can delete: NONE (Batch 1 functions all still needed)
- Why I must add: Text-based merge cannot understand structure
- LOC estimate: +70 -0 = net +70 LOC (under 150 limit ‚úÖ)

**Why net addition is justified**:
- Batch 1: Baseline (text-based, 50-70% success)
- Batch 2: Enhancement (structure-aware, +20-30% success)
- ROI: Additional ~10 hours/day saved (5 semantic merges √ó 2 hours rework)
- 70 LOC for 10 hours/day = 8.5 minutes per saved hour (good return)

**What we're NOT building** (economy via negativa):
- ‚ùå AST parser (200+ LOC vs 70 LOC grep-based)
- ‚ùå Class extraction (functions are 80% of conflicts)
- ‚ùå Semantic conflict detection (validation catches errors)

---

### LOCALITY - Related near, unrelated far

- [x] Related changes are in same module
- Files changing:
  1. `tools/wvo_mcp/scripts/merge_helpers.sh` (add 2 functions + helper)
  2. `tools/wvo_mcp/scripts/git_error_recovery.sh` (insert semantic merge into chain)

All files in same area: ‚úÖ git hygiene scripts

- Dependencies:
  - Local: Reuses Batch 1 functions (validate_merge, log_merge_decision)
  - External: grep, sed, jq, tsc (CLI tools, widely available)
  - No scattered dependencies ‚úÖ

---

### VISIBILITY - Important obvious, unimportant hidden

- [x] Errors are observable, interfaces are clear

**Error handling**:
- Semantic merge failures ‚Üí fallback to union (visible via echo)
- Validation failures ‚Üí logged + fallback
- Telemetry logs all semantic merge attempts

**Public API** (2 new functions):
```bash
attempt_semantic_merge_typescript()  # Returns: 0 if success, 1 if failed
attempt_semantic_merge_json()        # Returns: 0 if success, 1 if failed
```

**Observability**:
- Telemetry tracks `semantic_merge_typescript` and `semantic_merge_json`
- Metrics: success rate = count(semantic) / count(total)
- Dashboard query: `jq 'select(.resolution_strategy | startswith("semantic"))' ...`

---

### EVOLUTION - Patterns prove fitness

- [x] I'm using proven patterns

**Pattern fitness**:
1. **Grep-based extraction**:
   - Fitness: HIGH (used for decades in Unix scripts)
   - Usage: grep is universal (every shell script)
   - Bug rate: LOW (mature, well-tested)

2. **jq for JSON merge**:
   - Fitness: HIGH (standard JSON processor)
   - Usage: jq's `*` operator is documented, reliable
   - Bug rate: LOW (stable tool)

3. **Validation pipeline** (from Batch 1):
   - Fitness: MEDIUM (reusing Batch 1 pattern)
   - Usage: Already proven in Batch 1
   - Bug rate: LOW (catches merge errors)

**How we'll measure success**:
- `semantic_merge_success_rate`: target ‚â•20%
- `combined_automation_rate`: target 70-90% (vs 50-70% Batch 1)
- `union_merge_rate`: target <20% (vs 30-50% Batch 1)

**Upgrade path**:
- If grep limitations problematic ‚Üí upgrade to AST parser
- If jq array handling problematic ‚Üí custom JSON merge
- If success rate <10% ‚Üí consider reverting Batch 2

**Pattern Decision**:

**Similar patterns found**:
- Pattern 1: `tools/wvo_mcp/scripts/git_error_recovery.sh` - Grep for file detection
- Pattern 2: Batch 1 merge_helpers.sh - Fallback chain (auto ‚Üí union)
- Pattern 3: Shell scripts throughout codebase - sed/grep for text processing

**Pattern selected**: Grep-based structure extraction + fallback chain + validation

**Why this pattern**: Matches existing codebase patterns (grep/sed), proven approach, simple MVP

**Leverage Classification**:

**Code leverage level:** MEDIUM

**My code is:** MEDIUM **because**:
- Git hygiene affects all agents (merge failures block progress)
- Incorrect merge can break build (but CI catches it)
- Fallback to union is safe (manual review path)

**Assurance strategy**:
- Unit tests: 10 tests (TypeScript + JSON extraction, merge logic)
- Integration tests: 3 tests (end-to-end with real git conflicts)
- Validation pipeline: Always validate before staging
- Fallback: Union merge is safety net

**Commit message will include**:
```
Pattern: structure_extraction_with_validation
Leverage: Medium (git hygiene, comprehensive tests planned)
Batch: 2 of 2 (extends Batch 1 with semantic merge)
```

---

## Via Negativa Analysis

**Can you DELETE or SIMPLIFY existing code instead of adding?**

### Batch 1 Code Examined:

1. **`attempt_auto_merge` (Batch 1)**:
   - Examined: YES
   - Could be deleted: NO
   - Why: Still handles non-overlapping text changes (50-70% success)
   - Semantic merge is **supplement** (not replacement)

2. **`attempt_union_merge` (Batch 1)**:
   - Examined: YES
   - Could be deleted: NO
   - Why: Still needed as final fallback (semantic won't handle 100%)
   - **Decision**: Keep

3. **`validate_merge` (Batch 1)**:
   - Examined: YES
   - Could be simplified: NO
   - Why: Already minimal (case statement + tool calls)
   - Batch 2 **reuses** this (no duplication)

4. **`log_merge_decision` (Batch 1)**:
   - Examined: YES
   - Could be simplified: NO
   - Why: Already minimal (jq + append)
   - Batch 2 **reuses** this (no duplication)

**Via Negativa Conclusion**:
- ‚ùå Cannot delete Batch 1 code (all still needed)
- ‚úÖ CAN reuse validate + telemetry (no duplication)
- üîÑ MUST add semantic layer (new capability, not replacement)

**If you must add code, why is deletion/simplification insufficient?**

**Batch 1 is algorithmic

ally insufficient**:
- Text-based merge cannot understand code structure
- git merge-file only sees: "both modified line N"
- Cannot determine: "different function names ‚Üí no semantic conflict"

**Semantic merge requires NEW capability**:
- Extract structure (imports, functions, keys)
- Compare structure (same names? different names?)
- Rebuild file preserving both structures
- This is **addition**, not just better text processing

**Net LOC (+70) is justified because**:
- Addresses fundamental limitation (text vs structure)
- Progressive enhancement (Batch 1 ‚Üí Batch 2)
- No simpler solution exists (grep is already simplest)

---

## Refactor vs Repair Analysis

**Are you patching a symptom or refactoring the root cause?**

- Is this a PATCH/WORKAROUND or a PROPER FIX? **ENHANCEMENT (proper extension)**

**Why this is an enhancement**:
1. Not fixing a bug (Batch 1 works as designed)
2. Adding new capability (structure-aware vs text-based)
3. Progressive enhancement pattern (MVP ‚Üí Enhanced)
4. Increases success rate (50-70% ‚Üí 70-90%)

**NOT a patch because**:
- Batch 1 is not broken (works correctly for its design)
- User wants better success rate ("keep all changes")
- Requires new algorithm (structure extraction, not better text merging)

- If modifying file >200 LOC: Did you consider refactoring the WHOLE module? **N/A (not modifying large files)**

**Analysis**:
- merge_helpers.sh: 157 lines (not >200)
- git_error_recovery.sh: ~260 lines (>200, but only inserting ~15 LOC)

**Decision**: Surgical addition (not full rewrite)
- Batch 1 functions work well (no rewrite needed)
- Batch 2 adds semantic layer (insert into existing chain)
- Risk: Full rewrite could break working code

- What technical debt does this create (if any)? **MINIMAL**

**Debt NOT created**:
- Clean interfaces (functions testable, swappable)
- MVP limitations documented (grep vs AST)
- Clear upgrade path (AST parser for future)

**Debt paid down**:
- Reduces manual review (10-20% vs 30-50%)
- Preserves more work (70-90% vs 50-70% automated)

---

## Alternatives Considered

### Alternative 1: Grep-Based Semantic Merge (MVP) - ‚úÖ SELECTED

- What: Use grep/sed to extract TypeScript structure, jq for JSON
- Pros:
  - ‚úÖ Simple (70 LOC vs 200+ for AST)
  - ‚úÖ Fast (grep is instant, tsc validation ~5 sec)
  - ‚úÖ Good enough for MVP (handles 80% of structure conflicts)
  - ‚úÖ Clear limitations (documented, understood)
- Cons:
  - ‚ùå Misses edge cases (multi-line imports, nested functions)
  - ‚ùå No semantic conflict detection (renamed functions)
- Why selected:
  - ‚úÖ MVP philosophy (simple, fast, iterate based on data)
  - ‚úÖ Validation catches errors (safety net)
  - ‚úÖ Upgrade path clear (AST parser later if needed)

---

### Alternative 2: AST-Based Semantic Merge - DEFERRED

- What: Use TypeScript compiler API to parse AST, merge structures
- Pros:
  - ‚úÖ Robust (handles all TypeScript syntax)
  - ‚úÖ Can detect semantic conflicts (renamed functions, circular deps)
- Cons:
  - ‚ùå Complex (200+ LOC for parser + merge logic)
  - ‚ùå Slow (AST parsing + traversal ~10-15 sec per file)
  - ‚ùå High maintenance (TypeScript API changes over time)
- Why deferred:
  - ‚ùå Overkill for MVP (grep handles most cases)
  - ‚ùå Exceeds LOC limit (200 LOC > 150 limit)
  - Future enhancement if grep proves insufficient

---

### Alternative 3: LLM-Based Semantic Merge - REJECTED

- What: Use LLM to understand code intent and merge intelligently
- Pros:
  - Could understand semantic conflicts
  - Could handle any language
- Cons:
  - ‚ùå Unpredictable (LLM output varies)
  - ‚ùå Slow (LLM inference ~30-60 sec)
  - ‚ùå Expensive (token costs for every conflict)
  - ‚ùå Complex (prompt engineering, error handling)
- Why rejected:
  - Too slow for autopilot (60 sec per conflict)
  - Unpredictable results (hard to validate)
  - Rule-based merge is deterministic

---

### Selected Approach: Grep-Based Semantic Merge (Batch 2)

**Why this is best**:
- ‚úÖ Balances simplicity vs capability (70 LOC, 20-30% improvement)
- ‚úÖ Fast enough (<60 sec for 5 files)
- ‚úÖ Validation ensures safety (tsc, jq catch errors)
- ‚úÖ Clear upgrade path (AST later if needed)
- ‚úÖ Data-driven decision (measure success rate, iterate)

**How it aligns with AFP/SCAS**:
- **ECONOMY**: 70 LOC vs 200+ LOC (AST), saves 10 hours/day
- **COHERENCE**: Uses grep/sed (proven Unix patterns)
- **LOCALITY**: All changes in git scripts directory
- **VISIBILITY**: Telemetry tracks semantic merge success rate
- **EVOLUTION**: Measure, iterate (upgrade to AST if <10% success)

**Trade-offs accepted**:
- Grep limitations (multi-line, nested) vs complexity
- MVP now, iterate later vs perfect solution upfront
- 70-90% success vs 95%+ success (AST could achieve)

---

## Complexity Analysis

**How does this change affect complexity?**

- **Complexity increases:**
  - Cyclomatic: +13 paths (Batch 2 adds 3 functions)
  - Total: 25 paths (Batch 1: 12, Batch 2: 13)
  - Cognitive: MEDIUM (requires TypeScript/JSON knowledge)
  - Testing: MEDIUM-HIGH (needs git repo fixtures)

  - Is this increase JUSTIFIED? **YES**
    - Improves success rate 20-30 percentage points
    - Saves ~10 hours/day (5 conflicts √ó 2 hours rework)
    - ROI: 8.5 min implementation per hour saved
    - User explicitly requested ("keep all changes")

  - How will you MITIGATE this complexity?
    1. **Modular functions**: Each strategy is separate function (testable)
    2. **Validation pipeline**: Catches errors (safety net)
    3. **Fallback chain**: Always makes progress (never stuck)
    4. **Clear comments**: Explain grep patterns, jq behavior
    5. **Unit tests**: 10 tests for semantic merge logic
    6. **Integration tests**: 3 end-to-end scenarios
    7. **Measure success**: Data-driven decision (revert if <10%)

- **Complexity decreases**: NONE (pure addition)

- **Trade-offs**:
  - Necessary complexity: Structure extraction, validation (required for correctness)
  - Unnecessary complexity: NONE (grep is simplest approach)
  - Future optimization: Upgrade to AST if grep proves insufficient

**Verdict**: Complexity increase is justified (20-30% improvement, user-requested, data-driven)

---

## Implementation Plan

**Scope:**
- Files to change: 2 files
  1. `tools/wvo_mcp/scripts/merge_helpers.sh` (add 3 functions: ~55 LOC)
  2. `tools/wvo_mcp/scripts/git_error_recovery.sh` (insert semantic merge: ~15 LOC)

- Estimated LOC: +70 -0 = net +70 LOC

- Micro-batching compliance:
  - Files: 2 files (under 5 limit ‚úÖ)
  - LOC: 70 LOC (under 150 limit ‚úÖ)

**Risk Analysis**:

**Edge cases** (8 total, from THINK phase):
1. Multi-line TypeScript import (grep misses, validation catches)
2. Multi-line function signature (name still extracted)
3. JSON array merge (right side wins - documented)
4. TypeScript class (not extracted - union fallback)
5. JSON key conflict (right side wins - documented)
6. Function name collision (detected by helper)
7. Empty files (handled correctly)
8. Duplicate imports from same module (validation catches)

**Failure modes** (6 total, from THINK phase):
1. Grep extraction fails ‚Üí union fallback
2. jq merge produces invalid JSON ‚Üí validation catches
3. TypeScript validation fails ‚Üí union fallback
4. Function name collision not detected ‚Üí validation catches
5. Success rate <10% ‚Üí measure, consider reverting
6. Semantic merge slower than expected ‚Üí acceptable (under 60 sec)

**Testing strategy**:
1. Unit tests (10 tests): TypeScript extraction, JSON merge, function merging
2. Integration tests (3 tests): End-to-end with real git conflicts
3. Regression tests: Ensure Batch 1 still works
4. Performance tests: Verify <60 sec for 5 files

**Assumptions**:
1. Grep extracts most TypeScript structure (validated with real files)
2. jq's `*` operator sufficient for JSON (validated with nested JSON)
3. ‚â•20% conflicts structure-mergeable (will measure actual rate)
4. Validation catches semantic errors (standard practice)
5. Performance acceptable <60 sec (calculated: ~18 sec typical)

---

## Review Checklist (Self-Check)

Before implementing, verify:

- [x] I explored deletion/simplification (via negativa)
- [x] If adding code, I explained why deletion won't work
- [x] If modifying large files/functions, I considered full refactoring
- [x] I documented 2-3 alternative approaches
- [x] Any complexity increases are justified and mitigated
- [x] I estimated scope (files, LOC) and it's within limits
- [x] I thought through edge cases and failure modes
- [x] I have a testing strategy

**All boxes checked**: ‚úÖ Ready for DesignReviewer evaluation

---

## Notes

**Relationship to Batch 1**:
- Batch 1: Foundation (text-based merge, 50-70% success)
- Batch 2: Enhancement (structure-aware, +20-30% success)
- Combined: 70-90% automated (vs 30-50% manual in conservative approach)

**Data-Driven Approach**:
- Measure actual semantic merge success rate
- If <10%: Revert (not worth complexity)
- If 10-20%: Acceptable (marginal improvement)
- If >20%: Success (as planned)

**Upgrade Path**:
- MVP: Grep-based (simple, fast, 70 LOC)
- Future: AST-based (robust, 200+ LOC) - if grep proves insufficient
- Decision: Based on telemetry data, not guesswork

---

**Design Date:** 2025-11-05
**Author:** Claude Council

---

## GATE Review Tracking

### Review 1: 2025-11-05
- **DesignReviewer Result:** pending
- **Concerns Raised:** (awaiting DesignReviewer evaluation)
- **Remediation Task:** (will create if concerns raised)
- **Time Spent:** (will track if remediation needed)

---

**Status**: Ready for DesignReviewer evaluation
**Command**: `cd tools/wvo_mcp && npm run gate:review AFP-W0-M1-WORKTREE-STABILIZE-SEMANTIC && cd ../..`
